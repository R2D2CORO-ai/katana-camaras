{"version":3,"file":"tf-core.js","sources":["../../../../node_modules/tslib/tslib.es6.js","../../../../tfjs-core/src/backends/backend.ts","../../../../tfjs-core/src/util_base.ts","../../../../tfjs-core/src/environment.ts","../../../../tfjs-core/src/global_util.ts","../../../../tfjs-core/src/kernel_names.ts","../../../../tfjs-core/src/log.ts","../../../../tfjs-core/src/kernel_registry.ts","../../../../node_modules/long/src/long.js","../../../../tfjs-core/src/hash_util.ts","../../../../tfjs-core/src/util.ts","../../../../tfjs-core/src/profiler.ts","../../../../tfjs-core/src/tape.ts","../../../../tfjs-core/src/tensor_format.ts","../../../../tfjs-core/src/tensor.ts","../../../../tfjs-core/src/types.ts","../../../../tfjs-core/src/tensor_util.ts","../../../../tfjs-core/src/engine.ts","../../../../tfjs-core/src/device_util.ts","../../../../tfjs-core/src/flags.ts","../../../../tfjs-core/src/tensor_util_env.ts","../../../../tfjs-core/src/ops/operation.ts","../../../../tfjs-core/src/ops/complex.ts","../../../../tfjs-core/src/ops/tensor_ops_util.ts","../../../../tfjs-core/src/ops/tensor.ts","../../../../tfjs-core/src/io/types.ts","../../../../tfjs-core/src/io/io_utils.ts","../../../../tfjs-core/src/io/router_registry.ts","../../../../tfjs-core/src/io/indexed_db.ts","../../../../tfjs-core/src/io/local_storage.ts","../../../../tfjs-core/src/io/model_management.ts","../../../../tfjs-core/src/platforms/platform_browser.ts","../../../../tfjs-core/src/platforms/platform_node.ts","../../../../tfjs-core/src/ops/buffer.ts","../../../../tfjs-core/src/ops/cast.ts","../../../../tfjs-core/src/ops/clone.ts","../../../../tfjs-core/src/ops/print.ts","../../../../tfjs-core/src/base_side_effects.ts","../../../../tfjs-core/src/io/browser_files.ts","../../../../tfjs-core/src/io/progress.ts","../../../../tfjs-core/src/io/weights_loader.ts","../../../../tfjs-core/src/io/http.ts","../../../../tfjs-core/src/io/passthrough.ts","../../../../tfjs-core/src/io/io.ts","../../../../tfjs-core/src/ops/mat_mul.ts","../../../../tfjs-core/src/ops/one_hot.ts","../../../../tfjs-core/src/globals.ts","../../../../tfjs-core/src/ops/imag.ts","../../../../tfjs-core/src/ops/neg.ts","../../../../tfjs-core/src/ops/real.ts","../../../../tfjs-core/src/ops/transpose.ts","../../../../tfjs-core/src/ops/confusion_matrix.ts","../../../../tfjs-core/src/math.ts","../../../../tfjs-core/src/ops/broadcast_util.ts","../../../../tfjs-core/src/ops/tensor3d.ts","../../../../tfjs-core/src/ops/browser.ts","../../../../tfjs-core/src/ops/gather_nd_util.ts","../../../../tfjs-core/src/ops/scatter_nd_util.ts","../../../../tfjs-core/src/ops/slice_util.ts","../../../../tfjs-core/src/serialization.ts","../../../../tfjs-core/src/test_util.ts","../../../../tfjs-core/src/version.ts","../../../../tfjs-core/src/ops/add.ts","../../../../tfjs-core/src/ops/floorDiv.ts","../../../../tfjs-core/src/ops/div.ts","../../../../tfjs-core/src/ops/mul.ts","../../../../tfjs-core/src/ops/abs.ts","../../../../tfjs-core/src/ops/acos.ts","../../../../tfjs-core/src/ops/acosh.ts","../../../../tfjs-core/src/ops/add_n.ts","../../../../tfjs-core/src/ops/all.ts","../../../../tfjs-core/src/ops/any.ts","../../../../tfjs-core/src/ops/arg_max.ts","../../../../tfjs-core/src/ops/arg_min.ts","../../../../tfjs-core/src/ops/asin.ts","../../../../tfjs-core/src/ops/asinh.ts","../../../../tfjs-core/src/ops/atan.ts","../../../../tfjs-core/src/ops/atan2.ts","../../../../tfjs-core/src/ops/atanh.ts","../../../../tfjs-core/src/ops/conv_util.ts","../../../../tfjs-core/src/ops/reshape.ts","../../../../tfjs-core/src/ops/avg_pool.ts","../../../../tfjs-core/src/ops/avg_pool_3d.ts","../../../../tfjs-core/src/ops/concat.ts","../../../../tfjs-core/src/ops/sigmoid.ts","../../../../tfjs-core/src/ops/slice.ts","../../../../tfjs-core/src/ops/tanh.ts","../../../../tfjs-core/src/ops/basic_lstm_cell.ts","../../../../tfjs-core/src/ops/batch_to_space_nd.ts","../../../../tfjs-core/src/ops/batchnorm_util.ts","../../../../tfjs-core/src/ops/batchnorm.ts","../../../../tfjs-core/src/ops/batchnorm2d.ts","../../../../tfjs-core/src/ops/batchnorm3d.ts","../../../../tfjs-core/src/ops/batchnorm4d.ts","../../../../tfjs-core/src/ops/bincount.ts","../../../../tfjs-core/src/ops/broadcast_args.ts","../../../../tfjs-core/src/ops/broadcast_to.ts","../../../../tfjs-core/src/ops/ceil.ts","../../../../tfjs-core/src/ops/fill.ts","../../../../tfjs-core/src/ops/clip_by_value.ts","../../../../tfjs-core/src/ops/concat_1d.ts","../../../../tfjs-core/src/ops/concat_2d.ts","../../../../tfjs-core/src/ops/concat_3d.ts","../../../../tfjs-core/src/ops/concat_4d.ts","../../../../tfjs-core/src/ops/conv2d.ts","../../../../tfjs-core/src/ops/conv1d.ts","../../../../tfjs-core/src/ops/conv2d_backprop_input.ts","../../../../tfjs-core/src/ops/conv2d_transpose.ts","../../../../tfjs-core/src/ops/conv3d.ts","../../../../tfjs-core/src/ops/conv3d_backprop_input.ts","../../../../tfjs-core/src/ops/conv3d_transpose.ts","../../../../tfjs-core/src/ops/cos.ts","../../../../tfjs-core/src/ops/cosh.ts","../../../../tfjs-core/src/ops/cumprod.ts","../../../../tfjs-core/src/ops/cumsum.ts","../../../../tfjs-core/src/ops/dense_bincount.ts","../../../../tfjs-core/src/ops/depth_to_space.ts","../../../../tfjs-core/src/ops/depthwise_conv2d.ts","../../../../tfjs-core/src/ops/diag.ts","../../../../tfjs-core/src/ops/dilation2d.ts","../../../../tfjs-core/src/ops/equal.ts","../../../../tfjs-core/src/ops/where.ts","../../../../tfjs-core/src/ops/zeros_like.ts","../../../../tfjs-core/src/ops/div_no_nan.ts","../../../../tfjs-core/src/ops/dot.ts","../../../../tfjs-core/src/ops/einsum.ts","../../../../tfjs-core/src/ops/elu.ts","../../../../tfjs-core/src/ops/erf.ts","../../../../tfjs-core/src/ops/axis_util.ts","../../../../tfjs-core/src/ops/max.ts","../../../../tfjs-core/src/ops/min.ts","../../../../tfjs-core/src/ops/pow.ts","../../../../tfjs-core/src/ops/scalar.ts","../../../../tfjs-core/src/ops/sqrt.ts","../../../../tfjs-core/src/ops/square.ts","../../../../tfjs-core/src/ops/sum.ts","../../../../tfjs-core/src/ops/norm.ts","../../../../tfjs-core/src/ops/euclidean_norm.ts","../../../../tfjs-core/src/ops/exp.ts","../../../../tfjs-core/src/ops/expand_dims.ts","../../../../tfjs-core/src/ops/expm1.ts","../../../../tfjs-core/src/ops/tile.ts","../../../../tfjs-core/src/ops/eye.ts","../../../../tfjs-core/src/ops/floor.ts","../../../../tfjs-core/src/ops/gather.ts","../../../../tfjs-core/src/ops/greater.ts","../../../../tfjs-core/src/ops/greater_equal.ts","../../../../tfjs-core/src/ops/is_finite.ts","../../../../tfjs-core/src/ops/is_inf.ts","../../../../tfjs-core/src/ops/is_nan.ts","../../../../tfjs-core/src/ops/leaky_relu.ts","../../../../tfjs-core/src/ops/less.ts","../../../../tfjs-core/src/ops/less_equal.ts","../../../../tfjs-core/src/ops/linspace.ts","../../../../tfjs-core/src/ops/local_response_normalization.ts","../../../../tfjs-core/src/ops/log.ts","../../../../tfjs-core/src/ops/log1p.ts","../../../../tfjs-core/src/gradients.ts","../../../../tfjs-core/src/ops/softplus.ts","../../../../tfjs-core/src/ops/log_sigmoid.ts","../../../../tfjs-core/src/ops/sub.ts","../../../../tfjs-core/src/ops/log_softmax.ts","../../../../tfjs-core/src/ops/log_sum_exp.ts","../../../../tfjs-core/src/ops/logical_and.ts","../../../../tfjs-core/src/ops/logical_not.ts","../../../../tfjs-core/src/ops/logical_or.ts","../../../../tfjs-core/src/ops/logical_xor.ts","../../../../tfjs-core/src/ops/search_sorted.ts","../../../../tfjs-core/src/ops/lower_bound.ts","../../../../tfjs-core/src/ops/max_pool.ts","../../../../tfjs-core/src/ops/max_pool_3d.ts","../../../../tfjs-core/src/ops/max_pool_with_argmax.ts","../../../../tfjs-core/src/ops/maximum.ts","../../../../tfjs-core/src/ops/mean.ts","../../../../tfjs-core/src/ops/zeros.ts","../../../../tfjs-core/src/ops/ones.ts","../../../../tfjs-core/src/ops/meshgrid.ts","../../../../tfjs-core/src/ops/minimum.ts","../../../../tfjs-core/src/ops/mirror_pad.ts","../../../../tfjs-core/src/ops/mod.ts","../../../../tfjs-core/src/ops/moments.ts","../../../../tfjs-core/src/ops/multi_rnn_cell.ts","../../../../tfjs-core/src/ops/multinomial.ts","../../../../tfjs-core/src/ops/not_equal.ts","../../../../tfjs-core/src/ops/ones_like.ts","../../../../tfjs-core/src/ops/outer_product.ts","../../../../tfjs-core/src/ops/pad.ts","../../../../tfjs-core/src/ops/pad1d.ts","../../../../tfjs-core/src/ops/pad2d.ts","../../../../tfjs-core/src/ops/pad3d.ts","../../../../tfjs-core/src/ops/pad4d.ts","../../../../tfjs-core/src/ops/space_to_batch_nd.ts","../../../../tfjs-core/src/ops/pool.ts","../../../../tfjs-core/src/ops/prelu.ts","../../../../tfjs-core/src/ops/prod.ts","../../../../tfjs-core/src/ops/ragged_gather.ts","../../../../tfjs-core/src/ops/ragged_tensor_to_tensor.ts","../../../../tfjs-core/src/ops/rand.ts","../../../../node_modules/seedrandom/lib/alea.js","../../../../node_modules/seedrandom/lib/xor128.js","../../../../node_modules/seedrandom/lib/xorwow.js","../../../../node_modules/seedrandom/lib/xorshift7.js","../../../../node_modules/seedrandom/lib/xor4096.js","../../../../node_modules/seedrandom/lib/tychei.js","../../../../node_modules/seedrandom/seedrandom.js","../../../../node_modules/seedrandom/index.js","../../../../tfjs-core/src/ops/rand_util.ts","../../../../tfjs-core/src/ops/random_gamma.ts","../../../../tfjs-core/src/ops/random_normal.ts","../../../../tfjs-core/src/ops/random_standard_normal.ts","../../../../tfjs-core/src/ops/random_uniform.ts","../../../../tfjs-core/src/ops/range.ts","../../../../tfjs-core/src/ops/reciprocal.ts","../../../../tfjs-core/src/ops/relu.ts","../../../../tfjs-core/src/ops/relu6.ts","../../../../tfjs-core/src/ops/reverse.ts","../../../../tfjs-core/src/ops/reverse_1d.ts","../../../../tfjs-core/src/ops/reverse_2d.ts","../../../../tfjs-core/src/ops/reverse_3d.ts","../../../../tfjs-core/src/ops/reverse_4d.ts","../../../../tfjs-core/src/ops/round.ts","../../../../tfjs-core/src/ops/rsqrt.ts","../../../../tfjs-core/src/ops/selu.ts","../../../../tfjs-core/src/ops/separable_conv2d.ts","../../../../tfjs-core/src/ops/setdiff1d_async.ts","../../../../tfjs-core/src/ops/sign.ts","../../../../tfjs-core/src/ops/sin.ts","../../../../tfjs-core/src/ops/sinh.ts","../../../../tfjs-core/src/ops/slice1d.ts","../../../../tfjs-core/src/ops/slice2d.ts","../../../../tfjs-core/src/ops/slice3d.ts","../../../../tfjs-core/src/ops/slice4d.ts","../../../../tfjs-core/src/ops/softmax.ts","../../../../tfjs-core/src/ops/spectral/fft.ts","../../../../tfjs-core/src/ops/spectral/ifft.ts","../../../../tfjs-core/src/ops/spectral/irfft.ts","../../../../tfjs-core/src/ops/split.ts","../../../../tfjs-core/src/ops/spectral/rfft.ts","../../../../tfjs-core/src/ops/squared_difference.ts","../../../../tfjs-core/src/ops/squeeze.ts","../../../../tfjs-core/src/ops/stack.ts","../../../../tfjs-core/src/ops/step.ts","../../../../tfjs-core/src/ops/strided_slice.ts","../../../../tfjs-core/src/ops/tan.ts","../../../../tfjs-core/src/ops/tensor1d.ts","../../../../tfjs-core/src/ops/tensor2d.ts","../../../../tfjs-core/src/ops/tensor4d.ts","../../../../tfjs-core/src/ops/tensor5d.ts","../../../../tfjs-core/src/ops/tensor6d.ts","../../../../tfjs-core/src/ops/topk.ts","../../../../tfjs-core/src/ops/truncated_normal.ts","../../../../tfjs-core/src/ops/unique.ts","../../../../tfjs-core/src/ops/unsorted_segment_sum.ts","../../../../tfjs-core/src/ops/unstack.ts","../../../../tfjs-core/src/ops/upper_bound.ts","../../../../tfjs-core/src/ops/variable.ts","../../../../tfjs-core/src/backends/where_impl.ts","../../../../tfjs-core/src/ops/where_async.ts","../../../../tfjs-core/src/ops/boolean_mask.ts","../../../../tfjs-core/src/ops/moving_average.ts","../../../../tfjs-core/src/ops/scatter_nd.ts","../../../../tfjs-core/src/ops/sparse_to_dense_util.ts","../../../../tfjs-core/src/ops/sparse_to_dense.ts","../../../../tfjs-core/src/ops/gather_nd.ts","../../../../tfjs-core/src/ops/dropout_util.ts","../../../../tfjs-core/src/ops/dropout.ts","../../../../tfjs-core/src/ops/signal_ops_util.ts","../../../../tfjs-core/src/ops/in_top_k.ts","../../../../tfjs-core/src/ops/conv2d_backprop_filter.ts","../../../../tfjs-core/src/ops/fused_util.ts","../../../../tfjs-core/src/ops/fused/conv2d.ts","../../../../tfjs-core/src/ops/depthwise_conv2d_native_backprop_filter.ts","../../../../tfjs-core/src/ops/depthwise_conv2d_native_backprop_input.ts","../../../../tfjs-core/src/ops/fused/depthwise_conv2d.ts","../../../../tfjs-core/src/ops/fused/mat_mul.ts","../../../../tfjs-core/src/ops/fused_ops.ts","../../../../tfjs-core/src/ops/signal/hamming_window.ts","../../../../tfjs-core/src/ops/signal/hann_window.ts","../../../../tfjs-core/src/ops/signal/frame.ts","../../../../tfjs-core/src/ops/signal/stft.ts","../../../../tfjs-core/src/ops/image/crop_and_resize.ts","../../../../tfjs-core/src/ops/image/flip_left_right.ts","../../../../tfjs-core/src/ops/image/grayscale_to_rgb.ts","../../../../tfjs-core/src/ops/image/rotate_with_offset.ts","../../../../tfjs-core/src/ops/nonmax_util.ts","../../../../tfjs-core/src/ops/image/non_max_suppression.ts","../../../../tfjs-core/src/backends/non_max_suppression_util.ts","../../../../tfjs-core/src/backends/non_max_suppression_impl.ts","../../../../tfjs-core/src/ops/image/non_max_suppression_async.ts","../../../../tfjs-core/src/ops/image/non_max_suppression_with_score.ts","../../../../tfjs-core/src/ops/image/non_max_suppression_with_score_async.ts","../../../../tfjs-core/src/ops/image/non_max_suppression_padded.ts","../../../../tfjs-core/src/ops/image/non_max_suppression_padded_async.ts","../../../../tfjs-core/src/ops/image/resize_bilinear.ts","../../../../tfjs-core/src/ops/image/resize_nearest_neighbor.ts","../../../../tfjs-core/src/ops/image/threshold.ts","../../../../tfjs-core/src/ops/image/transform.ts","../../../../tfjs-core/src/ops/linalg/band_part.ts","../../../../tfjs-core/src/ops/linalg/gram_schmidt.ts","../../../../tfjs-core/src/ops/linalg/qr.ts","../../../../tfjs-core/src/ops/loss_ops_utils.ts","../../../../tfjs-core/src/ops/losses/compute_weighted_loss.ts","../../../../tfjs-core/src/ops/losses/absolute_difference.ts","../../../../tfjs-core/src/ops/losses/cosine_distance.ts","../../../../tfjs-core/src/ops/losses/hinge_loss.ts","../../../../tfjs-core/src/ops/losses/huber_loss.ts","../../../../tfjs-core/src/ops/losses/log_loss.ts","../../../../tfjs-core/src/ops/losses/mean_squared_error.ts","../../../../tfjs-core/src/ops/losses/sigmoid_cross_entropy.ts","../../../../tfjs-core/src/ops/losses/softmax_cross_entropy.ts","../../../../tfjs-core/src/ops/sparse/sparse_fill_empty_rows.ts","../../../../tfjs-core/src/ops/sparse/sparse_reshape.ts","../../../../tfjs-core/src/ops/sparse/sparse_segment_mean.ts","../../../../tfjs-core/src/ops/sparse/sparse_segment_sum.ts","../../../../tfjs-core/src/ops/string/string_n_grams.ts","../../../../tfjs-core/src/ops/string/string_split.ts","../../../../tfjs-core/src/ops/string/string_to_hash_bucket_fast.ts","../../../../tfjs-core/src/ops/ops.ts","../../../../tfjs-core/src/optimizers/optimizer.ts","../../../../tfjs-core/src/optimizers/adadelta_optimizer.ts","../../../../tfjs-core/src/optimizers/adagrad_optimizer.ts","../../../../tfjs-core/src/optimizers/adam_optimizer.ts","../../../../tfjs-core/src/optimizers/adamax_optimizer.ts","../../../../tfjs-core/src/optimizers/sgd_optimizer.ts","../../../../tfjs-core/src/optimizers/momentum_optimizer.ts","../../../../tfjs-core/src/optimizers/rmsprop_optimizer.ts","../../../../tfjs-core/src/optimizers/optimizer_constructors.ts","../../../../tfjs-core/src/train.ts","../../../../tfjs-core/src/browser_util.ts","../../../../tfjs-core/src/ops/concat_util.ts","../../../../tfjs-core/src/ops/ragged_to_dense_util.ts","../../../../tfjs-core/src/ops/reduce_util.ts","../../../../tfjs-core/src/ops/rotate_util.ts","../../../../tfjs-core/src/ops/array_ops_util.ts","../../../../tfjs-core/src/ops/selu_util.ts","../../../../tfjs-core/src/ops/erf_util.ts","../../../../tfjs-core/src/backends/complex_util.ts","../../../../tfjs-core/src/backends/einsum_util.ts","../../../../tfjs-core/src/ops/split_util.ts","../../../../tfjs-core/src/ops/sparse/sparse_fill_empty_rows_util.ts","../../../../tfjs-core/src/ops/sparse/sparse_reshape_util.ts","../../../../tfjs-core/src/ops/sparse/sparse_segment_reduction_util.ts","../../../../tfjs-core/src/ops/segment_util.ts","../../../../tfjs-core/src/backends/backend_util.ts","../../../../tfjs-core/src/backends/kernel_impls.ts"],"sourcesContent":["/******************************************************************************\r\nCopyright (c) Microsoft Corporation.\r\n\r\nPermission to use, copy, modify, and/or distribute this software for any\r\npurpose with or without fee is hereby granted.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH\r\nREGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY\r\nAND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,\r\nINDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM\r\nLOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR\r\nOTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR\r\nPERFORMANCE OF THIS SOFTWARE.\r\n***************************************************************************** */\r\n/* global Reflect, Promise */\r\n\r\nvar extendStatics = function(d, b) {\r\n    extendStatics = Object.setPrototypeOf ||\r\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\r\n        function (d, b) { for (var p in b) if (Object.prototype.hasOwnProperty.call(b, p)) d[p] = b[p]; };\r\n    return extendStatics(d, b);\r\n};\r\n\r\nexport function __extends(d, b) {\r\n    if (typeof b !== \"function\" && b !== null)\r\n        throw new TypeError(\"Class extends value \" + String(b) + \" is not a constructor or null\");\r\n    extendStatics(d, b);\r\n    function __() { this.constructor = d; }\r\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\r\n}\r\n\r\nexport var __assign = function() {\r\n    __assign = Object.assign || function __assign(t) {\r\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\r\n            s = arguments[i];\r\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p)) t[p] = s[p];\r\n        }\r\n        return t;\r\n    }\r\n    return __assign.apply(this, arguments);\r\n}\r\n\r\nexport function __rest(s, e) {\r\n    var t = {};\r\n    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)\r\n        t[p] = s[p];\r\n    if (s != null && typeof Object.getOwnPropertySymbols === \"function\")\r\n        for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {\r\n            if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i]))\r\n                t[p[i]] = s[p[i]];\r\n        }\r\n    return t;\r\n}\r\n\r\nexport function __decorate(decorators, target, key, desc) {\r\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\r\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\r\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\r\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\r\n}\r\n\r\nexport function __param(paramIndex, decorator) {\r\n    return function (target, key) { decorator(target, key, paramIndex); }\r\n}\r\n\r\nexport function __metadata(metadataKey, metadataValue) {\r\n    if (typeof Reflect === \"object\" && typeof Reflect.metadata === \"function\") return Reflect.metadata(metadataKey, metadataValue);\r\n}\r\n\r\nexport function __awaiter(thisArg, _arguments, P, generator) {\r\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\r\n    return new (P || (P = Promise))(function (resolve, reject) {\r\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\r\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\r\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\r\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\r\n    });\r\n}\r\n\r\nexport function __generator(thisArg, body) {\r\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\r\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\r\n    function verb(n) { return function (v) { return step([n, v]); }; }\r\n    function step(op) {\r\n        if (f) throw new TypeError(\"Generator is already executing.\");\r\n        while (_) try {\r\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\r\n            if (y = 0, t) op = [op[0] & 2, t.value];\r\n            switch (op[0]) {\r\n                case 0: case 1: t = op; break;\r\n                case 4: _.label++; return { value: op[1], done: false };\r\n                case 5: _.label++; y = op[1]; op = [0]; continue;\r\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\r\n                default:\r\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\r\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\r\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\r\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\r\n                    if (t[2]) _.ops.pop();\r\n                    _.trys.pop(); continue;\r\n            }\r\n            op = body.call(thisArg, _);\r\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\r\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\r\n    }\r\n}\r\n\r\nexport var __createBinding = Object.create ? (function(o, m, k, k2) {\r\n    if (k2 === undefined) k2 = k;\r\n    var desc = Object.getOwnPropertyDescriptor(m, k);\r\n    if (!desc || (\"get\" in desc ? !m.__esModule : desc.writable || desc.configurable)) {\r\n        desc = { enumerable: true, get: function() { return m[k]; } };\r\n    }\r\n    Object.defineProperty(o, k2, desc);\r\n}) : (function(o, m, k, k2) {\r\n    if (k2 === undefined) k2 = k;\r\n    o[k2] = m[k];\r\n});\r\n\r\nexport function __exportStar(m, o) {\r\n    for (var p in m) if (p !== \"default\" && !Object.prototype.hasOwnProperty.call(o, p)) __createBinding(o, m, p);\r\n}\r\n\r\nexport function __values(o) {\r\n    var s = typeof Symbol === \"function\" && Symbol.iterator, m = s && o[s], i = 0;\r\n    if (m) return m.call(o);\r\n    if (o && typeof o.length === \"number\") return {\r\n        next: function () {\r\n            if (o && i >= o.length) o = void 0;\r\n            return { value: o && o[i++], done: !o };\r\n        }\r\n    };\r\n    throw new TypeError(s ? \"Object is not iterable.\" : \"Symbol.iterator is not defined.\");\r\n}\r\n\r\nexport function __read(o, n) {\r\n    var m = typeof Symbol === \"function\" && o[Symbol.iterator];\r\n    if (!m) return o;\r\n    var i = m.call(o), r, ar = [], e;\r\n    try {\r\n        while ((n === void 0 || n-- > 0) && !(r = i.next()).done) ar.push(r.value);\r\n    }\r\n    catch (error) { e = { error: error }; }\r\n    finally {\r\n        try {\r\n            if (r && !r.done && (m = i[\"return\"])) m.call(i);\r\n        }\r\n        finally { if (e) throw e.error; }\r\n    }\r\n    return ar;\r\n}\r\n\r\n/** @deprecated */\r\nexport function __spread() {\r\n    for (var ar = [], i = 0; i < arguments.length; i++)\r\n        ar = ar.concat(__read(arguments[i]));\r\n    return ar;\r\n}\r\n\r\n/** @deprecated */\r\nexport function __spreadArrays() {\r\n    for (var s = 0, i = 0, il = arguments.length; i < il; i++) s += arguments[i].length;\r\n    for (var r = Array(s), k = 0, i = 0; i < il; i++)\r\n        for (var a = arguments[i], j = 0, jl = a.length; j < jl; j++, k++)\r\n            r[k] = a[j];\r\n    return r;\r\n}\r\n\r\nexport function __spreadArray(to, from, pack) {\r\n    if (pack || arguments.length === 2) for (var i = 0, l = from.length, ar; i < l; i++) {\r\n        if (ar || !(i in from)) {\r\n            if (!ar) ar = Array.prototype.slice.call(from, 0, i);\r\n            ar[i] = from[i];\r\n        }\r\n    }\r\n    return to.concat(ar || Array.prototype.slice.call(from));\r\n}\r\n\r\nexport function __await(v) {\r\n    return this instanceof __await ? (this.v = v, this) : new __await(v);\r\n}\r\n\r\nexport function __asyncGenerator(thisArg, _arguments, generator) {\r\n    if (!Symbol.asyncIterator) throw new TypeError(\"Symbol.asyncIterator is not defined.\");\r\n    var g = generator.apply(thisArg, _arguments || []), i, q = [];\r\n    return i = {}, verb(\"next\"), verb(\"throw\"), verb(\"return\"), i[Symbol.asyncIterator] = function () { return this; }, i;\r\n    function verb(n) { if (g[n]) i[n] = function (v) { return new Promise(function (a, b) { q.push([n, v, a, b]) > 1 || resume(n, v); }); }; }\r\n    function resume(n, v) { try { step(g[n](v)); } catch (e) { settle(q[0][3], e); } }\r\n    function step(r) { r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r); }\r\n    function fulfill(value) { resume(\"next\", value); }\r\n    function reject(value) { resume(\"throw\", value); }\r\n    function settle(f, v) { if (f(v), q.shift(), q.length) resume(q[0][0], q[0][1]); }\r\n}\r\n\r\nexport function __asyncDelegator(o) {\r\n    var i, p;\r\n    return i = {}, verb(\"next\"), verb(\"throw\", function (e) { throw e; }), verb(\"return\"), i[Symbol.iterator] = function () { return this; }, i;\r\n    function verb(n, f) { i[n] = o[n] ? function (v) { return (p = !p) ? { value: __await(o[n](v)), done: n === \"return\" } : f ? f(v) : v; } : f; }\r\n}\r\n\r\nexport function __asyncValues(o) {\r\n    if (!Symbol.asyncIterator) throw new TypeError(\"Symbol.asyncIterator is not defined.\");\r\n    var m = o[Symbol.asyncIterator], i;\r\n    return m ? m.call(o) : (o = typeof __values === \"function\" ? __values(o) : o[Symbol.iterator](), i = {}, verb(\"next\"), verb(\"throw\"), verb(\"return\"), i[Symbol.asyncIterator] = function () { return this; }, i);\r\n    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }\r\n    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }\r\n}\r\n\r\nexport function __makeTemplateObject(cooked, raw) {\r\n    if (Object.defineProperty) { Object.defineProperty(cooked, \"raw\", { value: raw }); } else { cooked.raw = raw; }\r\n    return cooked;\r\n};\r\n\r\nvar __setModuleDefault = Object.create ? (function(o, v) {\r\n    Object.defineProperty(o, \"default\", { enumerable: true, value: v });\r\n}) : function(o, v) {\r\n    o[\"default\"] = v;\r\n};\r\n\r\nexport function __importStar(mod) {\r\n    if (mod && mod.__esModule) return mod;\r\n    var result = {};\r\n    if (mod != null) for (var k in mod) if (k !== \"default\" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);\r\n    __setModuleDefault(result, mod);\r\n    return result;\r\n}\r\n\r\nexport function __importDefault(mod) {\r\n    return (mod && mod.__esModule) ? mod : { default: mod };\r\n}\r\n\r\nexport function __classPrivateFieldGet(receiver, state, kind, f) {\r\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\r\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\r\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\r\n}\r\n\r\nexport function __classPrivateFieldSet(receiver, state, value, kind, f) {\r\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\r\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\r\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\r\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\r\n}\r\n\r\nexport function __classPrivateFieldIn(state, receiver) {\r\n    if (receiver === null || (typeof receiver !== \"object\" && typeof receiver !== \"function\")) throw new TypeError(\"Cannot use 'in' operator on non-object\");\r\n    return typeof state === \"function\" ? receiver === state : state.has(receiver);\r\n}\r\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Backend, DataId, DataToGPUOptions, GPUData} from '../tensor';\nimport {BackendValues, DataType} from '../types';\n\nexport const EPSILON_FLOAT32 = 1e-7;\nexport const EPSILON_FLOAT16 = 1e-4;\n\n// Required information for all backends.\nexport interface BackendTimingInfo {\n  kernelMs: number|{error: string};\n  getExtraProfileInfo?(): string;  // a field for additional timing information\n                                   // e.g. packing / unpacking for WebGL backend\n}\n\nexport interface TensorStorage {\n  read(dataId: DataId): Promise<BackendValues>;\n  readSync(dataId: DataId): BackendValues;\n  disposeData(dataId: DataId, force?: boolean): boolean;\n  write(values: BackendValues, shape: number[], dtype: DataType): DataId;\n  move(\n      dataId: DataId, values: BackendValues, shape: number[], dtype: DataType,\n      refCount: number): void;\n  memory(): {unreliable: boolean;};  // Backend-specific information.\n  /** Returns number of data ids currently in the storage. */\n  numDataIds(): number;\n  refCount(dataId: DataId): number;\n}\n\n/** Convenient class for storing tensor-related data. */\nexport class DataStorage<T> {\n  private data = new WeakMap<DataId, T>();\n  private dataIdsCount = 0;\n\n  constructor(private backend: KernelBackend, private dataMover: DataMover) {}\n\n  get(dataId: DataId) {\n    if (!this.data.has(dataId)) {\n      this.dataMover.moveData(this.backend, dataId);\n    }\n    return this.data.get(dataId);\n  }\n\n  set(dataId: DataId, value: T): void {\n    this.dataIdsCount++;\n    this.data.set(dataId, value);\n  }\n\n  has(dataId: DataId): boolean {\n    return this.data.has(dataId);\n  }\n\n  delete(dataId: DataId): boolean {\n    this.dataIdsCount--;\n    return this.data.delete(dataId);\n  }\n\n  numDataIds(): number {\n    return this.dataIdsCount;\n  }\n}\n\nexport interface DataMover {\n  /**\n   * To be called by backends whenever they see a dataId that they don't own.\n   * Upon calling this method, the mover will fetch the tensor from another\n   * backend and register it with the current active backend.\n   */\n  moveData(backend: KernelBackend, dataId: DataId): void;\n}\n\nexport interface BackendTimer {\n  // check if backend timer is available\n  timerAvailable(): boolean;\n  time(f: () => void): Promise<BackendTimingInfo>;\n}\n\n/**\n * The interface that defines the kernels that should be implemented when\n * adding a new backend. New backends don't need to implement every one of the\n * methods, this can be done gradually (throw an error for unimplemented\n * methods).\n */\nexport class KernelBackend implements TensorStorage, Backend, BackendTimer {\n  refCount(dataId: DataId): number {\n    return notYetImplemented('refCount');\n  }\n  incRef(dataId: DataId): void {\n    return notYetImplemented('incRef');\n  }\n  timerAvailable(): boolean {\n    return true;\n  }\n  time(f: () => void): Promise<BackendTimingInfo> {\n    return notYetImplemented('time');\n  }\n  read(dataId: object): Promise<BackendValues> {\n    return notYetImplemented('read');\n  }\n  readSync(dataId: object): BackendValues {\n    return notYetImplemented('readSync');\n  }\n  readToGPU(dataId: object, options?: DataToGPUOptions): GPUData {\n    return notYetImplemented('readToGPU');\n  }\n  numDataIds(): number {\n    return notYetImplemented('numDataIds');\n  }\n  disposeData(dataId: object, force?: boolean): boolean {\n    return notYetImplemented('disposeData');\n  }\n  write(values: BackendValues, shape: number[], dtype: DataType): DataId {\n    return notYetImplemented('write');\n  }\n  move(\n      dataId: DataId, values: BackendValues, shape: number[], dtype: DataType,\n      refCount: number): void {\n    return notYetImplemented('move');\n  }\n  memory(): {unreliable: boolean; reasons?: string[]} {\n    return notYetImplemented('memory');\n  }\n  /** Returns the highest precision for floats in bits (e.g. 16 or 32) */\n  floatPrecision(): 16|32 {\n    return notYetImplemented('floatPrecision');\n  }\n  /** Returns the smallest representable number.  */\n  epsilon(): number {\n    return this.floatPrecision() === 32 ? EPSILON_FLOAT32 : EPSILON_FLOAT16;\n  }\n  dispose(): void {\n    return notYetImplemented('dispose');\n  }\n}\n\nfunction notYetImplemented(kernelName: string): never {\n  throw new Error(\n      `'${kernelName}' not yet implemented or not found in the registry. ` +\n      `This kernel may not be supported by the tfjs backend you have chosen`);\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType, DataTypeMap, FlatVector, NumericDataType, RecursiveArray, TensorLike, TypedArray} from './types';\n\n/**\n * Shuffles the array in-place using Fisher-Yates algorithm.\n *\n * ```js\n * const a = [1, 2, 3, 4, 5];\n * tf.util.shuffle(a);\n * console.log(a);\n * ```\n *\n * @param array The array to shuffle in-place.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\n// tslint:disable-next-line:no-any\nexport function shuffle(array: any[]|Uint32Array|Int32Array|\n                        Float32Array): void {\n  let counter = array.length;\n  let index = 0;\n  // While there are elements in the array\n  while (counter > 0) {\n    // Pick a random index\n    index = (Math.random() * counter) | 0;\n    // Decrease counter by 1\n    counter--;\n    // And swap the last element with it\n    swap(array, counter, index);\n  }\n}\n\n/**\n * Shuffles two arrays in-place the same way using Fisher-Yates algorithm.\n *\n * ```js\n * const a = [1,2,3,4,5];\n * const b = [11,22,33,44,55];\n * tf.util.shuffleCombo(a, b);\n * console.log(a, b);\n * ```\n *\n * @param array The first array to shuffle in-place.\n * @param array2 The second array to shuffle in-place with the same permutation\n *     as the first array.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function shuffleCombo(\n    // tslint:disable-next-line:no-any\n    array: any[]|Uint32Array|Int32Array|Float32Array,\n    // tslint:disable-next-line:no-any\n    array2: any[]|Uint32Array|Int32Array|Float32Array): void {\n  if (array.length !== array2.length) {\n    throw new Error(\n        `Array sizes must match to be shuffled together ` +\n        `First array length was ${array.length}` +\n        `Second array length was ${array2.length}`);\n  }\n  let counter = array.length;\n  let index = 0;\n  // While there are elements in the array\n  while (counter > 0) {\n    // Pick a random index\n    index = (Math.random() * counter) | 0;\n    // Decrease counter by 1\n    counter--;\n    // And swap the last element of each array with it\n    swap(array, counter, index);\n    swap(array2, counter, index);\n  }\n}\n\n/** Clamps a value to a specified range. */\nexport function clamp(min: number, x: number, max: number): number {\n  return Math.max(min, Math.min(x, max));\n}\n\nexport function nearestLargerEven(val: number): number {\n  return val % 2 === 0 ? val : val + 1;\n}\n\nexport function swap<T>(\n    object: {[index: number]: T}, left: number, right: number) {\n  const temp = object[left];\n  object[left] = object[right];\n  object[right] = temp;\n}\n\nexport function sum(arr: number[]): number {\n  let sum = 0;\n  for (let i = 0; i < arr.length; i++) {\n    sum += arr[i];\n  }\n  return sum;\n}\n\n/**\n * Returns a sample from a uniform [a, b) distribution.\n *\n * @param a The minimum support (inclusive).\n * @param b The maximum support (exclusive).\n * @return A pseudorandom number on the half-open interval [a,b).\n */\nexport function randUniform(a: number, b: number) {\n  const r = Math.random();\n  return (b * r) + (1 - r) * a;\n}\n\n/** Returns the squared Euclidean distance between two vectors. */\nexport function distSquared(a: FlatVector, b: FlatVector): number {\n  let result = 0;\n  for (let i = 0; i < a.length; i++) {\n    const diff = Number(a[i]) - Number(b[i]);\n    result += diff * diff;\n  }\n  return result;\n}\n\n/**\n * Asserts that the expression is true. Otherwise throws an error with the\n * provided message.\n *\n * ```js\n * const x = 2;\n * tf.util.assert(x === 2, 'x is not 2');\n * ```\n *\n * @param expr The expression to assert (as a boolean).\n * @param msg A function that returns the message to report when throwing an\n *     error. We use a function for performance reasons.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function assert(expr: boolean, msg: () => string) {\n  if (!expr) {\n    throw new Error(typeof msg === 'string' ? msg : msg());\n  }\n}\n\nexport function assertShapesMatch(\n    shapeA: number[], shapeB: number[], errorMessagePrefix = ''): void {\n  assert(\n      arraysEqual(shapeA, shapeB),\n      () => errorMessagePrefix + ` Shapes ${shapeA} and ${shapeB} must match`);\n}\n\nexport function assertNonNull(a: TensorLike): void {\n  assert(\n      a != null,\n      () => `The input to the tensor constructor must be a non-null value.`);\n}\n\n// NOTE: We explicitly type out what T extends instead of any so that\n// util.flatten on a nested array of number doesn't try to infer T as a\n// number[][], causing us to explicitly type util.flatten<number>().\n/**\n *  Flattens an arbitrarily nested array.\n *\n * ```js\n * const a = [[1, 2], [3, 4], [5, [6, [7]]]];\n * const flat = tf.util.flatten(a);\n * console.log(flat);\n * ```\n *\n *  @param arr The nested array to flatten.\n *  @param result The destination array which holds the elements.\n *  @param skipTypedArray If true, avoids flattening the typed arrays. Defaults\n *      to false.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function\nflatten<T extends number|boolean|string|Promise<number>|TypedArray>(\n    arr: T|RecursiveArray<T>, result: T[] = [], skipTypedArray = false): T[] {\n  if (result == null) {\n    result = [];\n  }\n  if (Array.isArray(arr) || isTypedArray(arr) && !skipTypedArray) {\n    for (let i = 0; i < arr.length; ++i) {\n      flatten(arr[i], result, skipTypedArray);\n    }\n  } else {\n    result.push(arr as T);\n  }\n  return result;\n}\n\n/**\n * Returns the size (number of elements) of the tensor given its shape.\n *\n * ```js\n * const shape = [3, 4, 2];\n * const size = tf.util.sizeFromShape(shape);\n * console.log(size);\n * ```\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function sizeFromShape(shape: number[]): number {\n  if (shape.length === 0) {\n    // Scalar.\n    return 1;\n  }\n  let size = shape[0];\n  for (let i = 1; i < shape.length; i++) {\n    size *= shape[i];\n  }\n  return size;\n}\n\nexport function isScalarShape(shape: number[]): boolean {\n  return shape.length === 0;\n}\n\nexport function arraysEqual(n1: FlatVector, n2: FlatVector) {\n  if (n1 === n2) {\n    return true;\n  }\n  if (n1 == null || n2 == null) {\n    return false;\n  }\n\n  if (n1.length !== n2.length) {\n    return false;\n  }\n  for (let i = 0; i < n1.length; i++) {\n    if (n1[i] !== n2[i]) {\n      return false;\n    }\n  }\n  return true;\n}\n\nexport function isInt(a: number): boolean {\n  return a % 1 === 0;\n}\n\nexport function tanh(x: number): number {\n  // tslint:disable-next-line:no-any\n  if ((Math as any).tanh != null) {\n    // tslint:disable-next-line:no-any\n    return (Math as any).tanh(x);\n  }\n  if (x === Infinity) {\n    return 1;\n  } else if (x === -Infinity) {\n    return -1;\n  } else {\n    const e2x = Math.exp(2 * x);\n    return (e2x - 1) / (e2x + 1);\n  }\n}\n\nexport function sizeToSquarishShape(size: number): [number, number] {\n  const width = Math.ceil(Math.sqrt(size));\n  return [width, Math.ceil(size / width)];\n}\n\n/**\n * Creates a new array with randomized indices to a given quantity.\n *\n * ```js\n * const randomTen = tf.util.createShuffledIndices(10);\n * console.log(randomTen);\n * ```\n *\n * @param number Quantity of how many shuffled indices to create.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function createShuffledIndices(n: number): Uint32Array {\n  const shuffledIndices = new Uint32Array(n);\n  for (let i = 0; i < n; ++i) {\n    shuffledIndices[i] = i;\n  }\n  shuffle(shuffledIndices);\n  return shuffledIndices;\n}\n\nexport function rightPad(a: string, size: number): string {\n  if (size <= a.length) {\n    return a;\n  }\n  return a + ' '.repeat(size - a.length);\n}\n\nexport function repeatedTry(\n    checkFn: () => boolean, delayFn = (counter: number) => 0,\n    maxCounter?: number,\n    scheduleFn: (functionRef: Function, delay: number) => void =\n        setTimeout): Promise<void> {\n  return new Promise<void>((resolve, reject) => {\n    let tryCount = 0;\n\n    const tryFn = () => {\n      if (checkFn()) {\n        resolve();\n        return;\n      }\n\n      tryCount++;\n\n      const nextBackoff = delayFn(tryCount);\n\n      if (maxCounter != null && tryCount >= maxCounter) {\n        reject();\n        return;\n      }\n      scheduleFn(tryFn, nextBackoff);\n    };\n\n    tryFn();\n  });\n}\n\n/**\n * Given the full size of the array and a shape that may contain -1 as the\n * implicit dimension, returns the inferred shape where -1 is replaced.\n * E.g. For shape=[2, -1, 3] and size=24, it will return [2, 4, 3].\n *\n * @param shape The shape, which may contain -1 in some dimension.\n * @param size The full size (number of elements) of the array.\n * @return The inferred shape where -1 is replaced with the inferred size.\n */\nexport function inferFromImplicitShape(\n    shape: number[], size: number): number[] {\n  let shapeProd = 1;\n  let implicitIdx = -1;\n\n  for (let i = 0; i < shape.length; ++i) {\n    if (shape[i] >= 0) {\n      shapeProd *= shape[i];\n    } else if (shape[i] === -1) {\n      if (implicitIdx !== -1) {\n        throw Error(\n            `Shapes can only have 1 implicit size. ` +\n            `Found -1 at dim ${implicitIdx} and dim ${i}`);\n      }\n      implicitIdx = i;\n    } else if (shape[i] < 0) {\n      throw Error(`Shapes can not be < 0. Found ${shape[i]} at dim ${i}`);\n    }\n  }\n\n  if (implicitIdx === -1) {\n    if (size > 0 && size !== shapeProd) {\n      throw Error(`Size(${size}) must match the product of shape ${shape}`);\n    }\n    return shape;\n  }\n\n  if (shapeProd === 0) {\n    throw Error(\n        `Cannot infer the missing size in [${shape}] when ` +\n        `there are 0 elements`);\n  }\n  if (size % shapeProd !== 0) {\n    throw Error(\n        `The implicit shape can't be a fractional number. ` +\n        `Got ${size} / ${shapeProd}`);\n  }\n\n  const newShape = shape.slice();\n  newShape[implicitIdx] = size / shapeProd;\n  return newShape;\n}\n\nexport function parseAxisParam(\n    axis: number|number[], shape: number[]): number[] {\n  const rank = shape.length;\n\n  // Normalize input\n  axis = axis == null ? shape.map((s, i) => i) : [].concat(axis);\n\n  // Check for valid range\n  assert(\n      axis.every(ax => ax >= -rank && ax < rank),\n      () =>\n          `All values in axis param must be in range [-${rank}, ${rank}) but ` +\n          `got axis ${axis}`);\n\n  // Check for only integers\n  assert(\n      axis.every(ax => isInt(ax)),\n      () => `All values in axis param must be integers but ` +\n          `got axis ${axis}`);\n\n  // Handle negative axis.\n  return axis.map(a => a < 0 ? rank + a : a);\n}\n\n/** Reduces the shape by removing all dimensions of shape 1. */\nexport function squeezeShape(shape: number[], axis?: number[]):\n    {newShape: number[], keptDims: number[]} {\n  const newShape: number[] = [];\n  const keptDims: number[] = [];\n  const isEmptyArray = axis != null && Array.isArray(axis) && axis.length === 0;\n  const axes = (axis == null || isEmptyArray) ?\n      null :\n      parseAxisParam(axis, shape).sort();\n  let j = 0;\n  for (let i = 0; i < shape.length; ++i) {\n    if (axes != null) {\n      if (axes[j] === i && shape[i] !== 1) {\n        throw new Error(\n            `Can't squeeze axis ${i} since its dim '${shape[i]}' is not 1`);\n      }\n      if ((axes[j] == null || axes[j] > i) && shape[i] === 1) {\n        newShape.push(shape[i]);\n        keptDims.push(i);\n      }\n      if (axes[j] <= i) {\n        j++;\n      }\n    }\n    if (shape[i] !== 1) {\n      newShape.push(shape[i]);\n      keptDims.push(i);\n    }\n  }\n  return {newShape, keptDims};\n}\n\nexport function getTypedArrayFromDType<D extends NumericDataType>(\n    dtype: D, size: number): DataTypeMap[D] {\n  let values = null;\n  if (dtype == null || dtype === 'float32') {\n    values = new Float32Array(size);\n  } else if (dtype === 'int32') {\n    values = new Int32Array(size);\n  } else if (dtype === 'bool') {\n    values = new Uint8Array(size);\n  } else {\n    throw new Error(`Unknown data type ${dtype}`);\n  }\n  return values as DataTypeMap[D];\n}\n\nexport function getArrayFromDType<D extends DataType>(\n    dtype: D, size: number): DataTypeMap[D] {\n  let values = null;\n  if (dtype == null || dtype === 'float32') {\n    values = new Float32Array(size);\n  } else if (dtype === 'int32') {\n    values = new Int32Array(size);\n  } else if (dtype === 'bool') {\n    values = new Uint8Array(size);\n  } else if (dtype === 'string') {\n    values = new Array<'string'>(size);\n  } else {\n    throw new Error(`Unknown data type ${dtype}`);\n  }\n  return values as DataTypeMap[D];\n}\n\nexport function checkConversionForErrors<D extends DataType>(\n    vals: DataTypeMap[D]|number[], dtype: D): void {\n  for (let i = 0; i < vals.length; i++) {\n    const num = vals[i] as number;\n    if (isNaN(num) || !isFinite(num)) {\n      throw Error(`A tensor of type ${dtype} being uploaded contains ${num}.`);\n    }\n  }\n}\n\n/** Returns true if the dtype is valid. */\nexport function isValidDtype(dtype: DataType): boolean {\n  return dtype === 'bool' || dtype === 'complex64' || dtype === 'float32' ||\n      dtype === 'int32' || dtype === 'string';\n}\n\n/**\n * Returns true if the new type can't encode the old type without loss of\n * precision.\n */\nexport function hasEncodingLoss(oldType: DataType, newType: DataType): boolean {\n  if (newType === 'complex64') {\n    return false;\n  }\n  if (newType === 'float32' && oldType !== 'complex64') {\n    return false;\n  }\n  if (newType === 'int32' && oldType !== 'float32' && oldType !== 'complex64') {\n    return false;\n  }\n  if (newType === 'bool' && oldType === 'bool') {\n    return false;\n  }\n  return true;\n}\n\nexport function isTypedArray(a: {}): a is Float32Array|Int32Array|Uint8Array|\n    Uint8ClampedArray {\n  return a instanceof Float32Array || a instanceof Int32Array ||\n      a instanceof Uint8Array || a instanceof Uint8ClampedArray;\n}\n\nexport function bytesPerElement(dtype: DataType): number {\n  if (dtype === 'float32' || dtype === 'int32') {\n    return 4;\n  } else if (dtype === 'complex64') {\n    return 8;\n  } else if (dtype === 'bool') {\n    return 1;\n  } else {\n    throw new Error(`Unknown dtype ${dtype}`);\n  }\n}\n\n/**\n * Returns the approximate number of bytes allocated in the string array - 2\n * bytes per character. Computing the exact bytes for a native string in JS\n * is not possible since it depends on the encoding of the html page that\n * serves the website.\n */\nexport function bytesFromStringArray(arr: Uint8Array[]): number {\n  if (arr == null) {\n    return 0;\n  }\n  let bytes = 0;\n  arr.forEach(x => bytes += x.length);\n  return bytes;\n}\n\n/** Returns true if the value is a string. */\nexport function isString(value: {}): value is string {\n  return typeof value === 'string' || value instanceof String;\n}\n\nexport function isBoolean(value: {}): boolean {\n  return typeof value === 'boolean';\n}\n\nexport function isNumber(value: {}): boolean {\n  return typeof value === 'number';\n}\n\nexport function inferDtype(values: TensorLike): DataType {\n  if (Array.isArray(values)) {\n    return inferDtype(values[0]);\n  }\n  if (values instanceof Float32Array) {\n    return 'float32';\n  } else if (\n      values instanceof Int32Array || values instanceof Uint8Array ||\n      values instanceof Uint8ClampedArray) {\n    return 'int32';\n  } else if (isNumber(values)) {\n    return 'float32';\n  } else if (isString(values)) {\n    return 'string';\n  } else if (isBoolean(values)) {\n    return 'bool';\n  }\n  return 'float32';\n}\n\nexport function isFunction(f: Function) {\n  return !!(f && f.constructor && f.call && f.apply);\n}\n\nexport function nearestDivisor(size: number, start: number): number {\n  for (let i = start; i < size; ++i) {\n    if (size % i === 0) {\n      return i;\n    }\n  }\n  return size;\n}\n\nexport function computeStrides(shape: number[]): number[] {\n  const rank = shape.length;\n  if (rank < 2) {\n    return [];\n  }\n\n  // Last dimension has implicit stride of 1, thus having D-1 (instead of D)\n  // strides.\n  const strides = new Array(rank - 1);\n  strides[rank - 2] = shape[rank - 1];\n  for (let i = rank - 3; i >= 0; --i) {\n    strides[i] = strides[i + 1] * shape[i + 1];\n  }\n  return strides;\n}\n\nfunction createNestedArray(\n    offset: number, shape: number[], a: TypedArray, isComplex = false) {\n  const ret = new Array();\n  if (shape.length === 1) {\n    const d = shape[0] * (isComplex ? 2 : 1);\n    for (let i = 0; i < d; i++) {\n      ret[i] = a[offset + i];\n    }\n  } else {\n    const d = shape[0];\n    const rest = shape.slice(1);\n    const len = rest.reduce((acc, c) => acc * c) * (isComplex ? 2 : 1);\n    for (let i = 0; i < d; i++) {\n      ret[i] = createNestedArray(offset + i * len, rest, a, isComplex);\n    }\n  }\n  return ret;\n}\n\n// Provide a nested array of TypedArray in given shape.\nexport function toNestedArray(\n    shape: number[], a: TypedArray, isComplex = false) {\n  if (shape.length === 0) {\n    // Scalar type should return a single number.\n    return a[0];\n  }\n  const size = shape.reduce((acc, c) => acc * c) * (isComplex ? 2 : 1);\n  if (size === 0) {\n    // A tensor with shape zero should be turned into empty list.\n    return [];\n  }\n  if (size !== a.length) {\n    throw new Error(`[${shape}] does not match the input size ${a.length}${\n        isComplex ? ' for a complex tensor' : ''}.`);\n  }\n\n  return createNestedArray(0, shape, a, isComplex);\n}\n\nexport function makeOnesTypedArray<D extends DataType>(\n    size: number, dtype: D): DataTypeMap[D] {\n  const array = makeZerosTypedArray(size, dtype);\n  for (let i = 0; i < array.length; i++) {\n    array[i] = 1;\n  }\n  return array;\n}\n\nexport function makeZerosTypedArray<D extends DataType>(\n    size: number, dtype: D): DataTypeMap[D] {\n  if (dtype == null || dtype === 'float32' || dtype === 'complex64') {\n    return new Float32Array(size) as DataTypeMap[D];\n  } else if (dtype === 'int32') {\n    return new Int32Array(size) as DataTypeMap[D];\n  } else if (dtype === 'bool') {\n    return new Uint8Array(size) as DataTypeMap[D];\n  } else {\n    throw new Error(`Unknown data type ${dtype}`);\n  }\n}\n\n/**\n * Make nested `TypedArray` filled with zeros.\n * @param shape The shape information for the nested array.\n * @param dtype dtype of the array element.\n */\nexport function makeZerosNestedTypedArray<D extends DataType>(\n    shape: number[], dtype: D) {\n  const size = shape.reduce((prev, curr) => prev * curr, 1);\n  if (dtype == null || dtype === 'float32') {\n    return toNestedArray(shape, new Float32Array(size));\n  } else if (dtype === 'int32') {\n    return toNestedArray(shape, new Int32Array(size));\n  } else if (dtype === 'bool') {\n    return toNestedArray(shape, new Uint8Array(size));\n  } else {\n    throw new Error(`Unknown data type ${dtype}`);\n  }\n}\n\nexport function assertNonNegativeIntegerDimensions(shape: number[]) {\n  shape.forEach(dimSize => {\n    assert(\n        Number.isInteger(dimSize) && dimSize >= 0,\n        () =>\n            `Tensor must have a shape comprised of positive integers but got ` +\n            `shape [${shape}].`);\n  });\n}\n\n/**\n * Computes flat index for a given location (multidimentionsal index) in a\n * Tensor/multidimensional array.\n *\n * @param locs Location in the tensor.\n * @param rank Rank of the tensor.\n * @param strides Tensor strides.\n */\nexport function locToIndex(\n    locs: number[], rank: number, strides: number[]): number {\n  if (rank === 0) {\n    return 0;\n  } else if (rank === 1) {\n    return locs[0];\n  }\n  let index = locs[locs.length - 1];\n  for (let i = 0; i < locs.length - 1; ++i) {\n    index += strides[i] * locs[i];\n  }\n  return index;\n}\n\n/**\n * Computes the location (multidimensional index) in a\n * tensor/multidimentional array for a given flat index.\n *\n * @param index Index in flat array.\n * @param rank Rank of tensor.\n * @param strides Strides of tensor.\n */\nexport function indexToLoc(\n    index: number, rank: number, strides: number[]): number[] {\n  if (rank === 0) {\n    return [];\n  } else if (rank === 1) {\n    return [index];\n  }\n  const locs: number[] = new Array(rank);\n  for (let i = 0; i < locs.length - 1; ++i) {\n    locs[i] = Math.floor(index / strides[i]);\n    index -= locs[i] * strides[i];\n  }\n  locs[locs.length - 1] = index;\n  return locs;\n}\n\n/**\n * This method asserts whether an object is a Promise instance.\n * @param object\n */\n// tslint:disable-next-line: no-any\nexport function isPromise(object: any): object is Promise<unknown> {\n  //  We chose to not use 'obj instanceOf Promise' for two reasons:\n  //  1. It only reliably works for es6 Promise, not other Promise\n  //  implementations.\n  //  2. It doesn't work with framework that uses zone.js. zone.js monkey\n  //  patch the async calls, so it is possible the obj (patched) is\n  //  comparing to a pre-patched Promise.\n  return object && object.then && typeof object.then === 'function';\n}\n","/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Platform} from './platforms/platform';\nimport {isPromise} from './util_base';\n\n// Expects flags from URL in the format ?tfjsflags=FLAG1:1,FLAG2:true.\nconst TENSORFLOWJS_FLAGS_PREFIX = 'tfjsflags';\n\ntype FlagValue = number|boolean;\ntype FlagEvaluationFn = (() => FlagValue)|(() => Promise<FlagValue>);\nexport type Flags = {\n  [featureName: string]: FlagValue\n};\nexport type FlagRegistryEntry = {\n  evaluationFn: FlagEvaluationFn;\n  setHook?: (value: FlagValue) => void;\n};\n\n/**\n * The environment contains evaluated flags as well as the registered platform.\n * This is always used as a global singleton and can be retrieved with\n * `tf.env()`.\n *\n * @doc {heading: 'Environment'}\n */\nexport class Environment {\n  private flags: Flags = {};\n  private flagRegistry: {[flagName: string]: FlagRegistryEntry} = {};\n\n  private urlFlags: Flags = {};\n\n  platformName: string;\n  platform: Platform;\n\n  // Jasmine spies on this in 'environment_test.ts'\n  getQueryParams = getQueryParams;\n\n  // tslint:disable-next-line: no-any\n  constructor(public global: any) {\n    this.populateURLFlags();\n  }\n\n  setPlatform(platformName: string, platform: Platform) {\n    if (this.platform != null) {\n      if (!(env().getBool('IS_TEST') || env().getBool('PROD'))) {\n        console.warn(\n            `Platform ${this.platformName} has already been set. ` +\n            `Overwriting the platform with ${platformName}.`);\n      }\n    }\n    this.platformName = platformName;\n    this.platform = platform;\n  }\n\n  registerFlag(\n      flagName: string, evaluationFn: FlagEvaluationFn,\n      setHook?: (value: FlagValue) => void) {\n    this.flagRegistry[flagName] = {evaluationFn, setHook};\n\n    // Override the flag value from the URL. This has to happen here because\n    // the environment is initialized before flags get registered.\n    if (this.urlFlags[flagName] != null) {\n      const flagValue = this.urlFlags[flagName];\n      if (!(env().getBool('IS_TEST') || env().getBool('PROD'))) {\n        console.warn(\n            `Setting feature override from URL ${flagName}: ${flagValue}.`);\n      }\n      this.set(flagName, flagValue);\n    }\n  }\n\n  async getAsync(flagName: string): Promise<FlagValue> {\n    if (flagName in this.flags) {\n      return this.flags[flagName];\n    }\n\n    this.flags[flagName] = await this.evaluateFlag(flagName);\n    return this.flags[flagName];\n  }\n\n  get(flagName: string): FlagValue {\n    if (flagName in this.flags) {\n      return this.flags[flagName];\n    }\n\n    const flagValue = this.evaluateFlag(flagName);\n    if (isPromise(flagValue)) {\n      throw new Error(\n          `Flag ${flagName} cannot be synchronously evaluated. ` +\n          `Please use getAsync() instead.`);\n    }\n\n    this.flags[flagName] = flagValue;\n    return this.flags[flagName];\n  }\n\n  getNumber(flagName: string): number {\n    return this.get(flagName) as number;\n  }\n\n  getBool(flagName: string): boolean {\n    return this.get(flagName) as boolean;\n  }\n\n  getFlags(): Flags {\n    return this.flags;\n  }\n  // For backwards compatibility.\n  get features(): Flags {\n    return this.flags;\n  }\n\n  set(flagName: string, value: FlagValue): void {\n    if (this.flagRegistry[flagName] == null) {\n      throw new Error(\n          `Cannot set flag ${flagName} as it has not been registered.`);\n    }\n    this.flags[flagName] = value;\n    if (this.flagRegistry[flagName].setHook != null) {\n      this.flagRegistry[flagName].setHook(value);\n    }\n  }\n\n  private evaluateFlag(flagName: string): FlagValue|Promise<FlagValue> {\n    if (this.flagRegistry[flagName] == null) {\n      throw new Error(\n          `Cannot evaluate flag '${flagName}': no evaluation function found.`);\n    }\n    return this.flagRegistry[flagName].evaluationFn();\n  }\n\n  setFlags(flags: Flags) {\n    this.flags = Object.assign({}, flags);\n  }\n\n  reset() {\n    this.flags = {};\n    this.urlFlags = {};\n    this.populateURLFlags();\n  }\n\n  private populateURLFlags(): void {\n    if (typeof this.global === 'undefined' ||\n        typeof this.global.location === 'undefined' ||\n        typeof this.global.location.search === 'undefined') {\n      return;\n    }\n\n    const urlParams = this.getQueryParams(this.global.location.search);\n    if (TENSORFLOWJS_FLAGS_PREFIX in urlParams) {\n      const keyValues = urlParams[TENSORFLOWJS_FLAGS_PREFIX].split(',');\n      keyValues.forEach(keyValue => {\n        const [key, value] = keyValue.split(':') as [string, string];\n        this.urlFlags[key] = parseValue(key, value);\n      });\n    }\n  }\n}\n\nexport function getQueryParams(queryString: string): {[key: string]: string} {\n  const params = {};\n  queryString.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g, (s, ...t) => {\n    decodeParam(params, t[0], t[1]);\n    return t.join('=');\n  });\n  return params;\n}\n\nfunction decodeParam(\n    params: {[key: string]: string}, name: string, value?: string) {\n  params[decodeURIComponent(name)] = decodeURIComponent(value || '');\n}\n\nfunction parseValue(flagName: string, value: string): FlagValue {\n  value = value.toLowerCase();\n  if (value === 'true' || value === 'false') {\n    return value === 'true';\n  } else if (`${+ value}` === value) {\n    return +value;\n  }\n  throw new Error(\n      `Could not parse value flag value ${value} for flag ${flagName}.`);\n}\n\n/**\n * Returns the current environment (a global singleton).\n *\n * The environment object contains the evaluated feature values as well as the\n * active platform.\n *\n * @doc {heading: 'Environment'}\n */\nexport function env() {\n  return ENV;\n}\n\nexport let ENV: Environment = null;\nexport function setEnvironmentGlobal(environment: Environment) {\n  ENV = environment;\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// Note that the identifier globalNameSpace is scoped to this module, but will\n// always resolve to the same global object regardless of how the module is\n// resolved.\n// tslint:disable-next-line:no-any\nlet globalNameSpace: {_tfGlobals: Map<string, any>};\n// tslint:disable-next-line:no-any\nexport function getGlobalNamespace(): {_tfGlobals: Map<string, any>} {\n  if (globalNameSpace == null) {\n    // tslint:disable-next-line:no-any\n    let ns: any;\n    if (typeof (window) !== 'undefined') {\n      ns = window;\n    } else if (typeof (global) !== 'undefined') {\n      ns = global;\n    } else if (typeof (process) !== 'undefined') {\n      ns = process;\n    } else if (typeof (self) !== 'undefined') {\n      ns = self;\n    } else {\n      throw new Error('Could not find a global object');\n    }\n    globalNameSpace = ns;\n  }\n  return globalNameSpace;\n}\n\n// tslint:disable-next-line:no-any\nfunction getGlobalMap(): Map<string, any> {\n  const ns = getGlobalNamespace();\n  if (ns._tfGlobals == null) {\n    ns._tfGlobals = new Map();\n  }\n  return ns._tfGlobals;\n}\n\n/**\n * Returns a globally accessible 'singleton' object.\n *\n * @param key the name of the object\n * @param init a function to initialize to initialize this object\n *             the first time it is fetched.\n */\nexport function getGlobal<T>(key: string, init: () => T): T {\n  const globalMap = getGlobalMap();\n  if (globalMap.has(key)) {\n    return globalMap.get(key);\n  } else {\n    const singleton = init();\n    globalMap.set(key, singleton);\n    return globalMap.get(key);\n  }\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// Allow UpperCamelCase variable names\n// tslint:disable: variable-name\n// Unfortunately just enabling PascalCase per file (tslint:enable:\n// allow-pascal-case) doesn't work.\nimport {NamedTensorInfoMap, TensorInfo} from './kernel_registry';\nimport {ExplicitPadding} from './ops/conv_util';\nimport {Activation} from './ops/fused_types';\nimport {DataType, PixelData} from './types';\n\nexport const Abs = 'Abs';\nexport type AbsInputs = UnaryInputs;\n\nexport const Acos = 'Acos';\nexport type AcosInputs = UnaryInputs;\n\nexport const Acosh = 'Acosh';\nexport type AcoshInputs = UnaryInputs;\n\nexport const Add = 'Add';\nexport type AddInputs = BinaryInputs;\n\nexport const AddN = 'AddN';\nexport type AddNInputs = TensorInfo[];\n\nexport const All = 'All';\nexport type AllInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface AllAttrs {\n  axis: number|number[];\n  keepDims: boolean;\n}\n\nexport const Any = 'Any';\nexport type AnyInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface AnyAttrs {\n  axis: number|number[];\n  keepDims: boolean;\n}\n\nexport const ArgMax = 'ArgMax';\nexport type ArgMaxInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface ArgMaxAttrs {\n  axis: number;\n}\n\nexport const ArgMin = 'ArgMin';\nexport type ArgMinInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface ArgMinAttrs {\n  axis: number;\n}\n\nexport const Asin = 'Asin';\nexport type AsinInputs = UnaryInputs;\n\nexport const Asinh = 'Asinh';\nexport type AsinhInputs = UnaryInputs;\n\nexport const Atan = 'Atan';\nexport type AtanInputs = UnaryInputs;\n\nexport const Atanh = 'Atanh';\nexport type AtanhInputs = UnaryInputs;\n\nexport const Atan2 = 'Atan2';\nexport type Atan2Inputs = BinaryInputs;\n\nexport const AvgPool = 'AvgPool';\nexport type AvgPoolInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface AvgPoolAttrs {\n  filterSize: [number, number]|number;\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n}\n\nexport const AvgPoolGrad = 'AvgPoolGrad';\nexport type AvgPoolGradInputs = Pick<NamedTensorInfoMap, 'dy'|'input'>;\nexport interface AvgPoolGradAttrs {\n  filterSize: [number, number]|number;\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n}\n\nexport const AvgPool3D = 'AvgPool3D';\nexport type AvgPool3DInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface AvgPool3DAttrs {\n  filterSize: [number, number, number]|number;\n  strides: [number, number, number]|number;\n  pad: 'valid'|'same'|number;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n  dataFormat: 'NDHWC'|'NCDHW';\n}\n\nexport const AvgPool3DGrad = 'AvgPool3DGrad';\nexport type AvgPool3DGradInputs = Pick<NamedTensorInfoMap, 'dy'|'input'>;\nexport interface AvgPool3DGradAttrs {\n  filterSize: [number, number, number]|number;\n  strides: [number, number, number]|number;\n  pad: 'valid'|'same'|number;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n}\n\nexport const BatchMatMul = 'BatchMatMul';\nexport type BatchMatMulInputs = Pick<NamedTensorInfoMap, 'a'|'b'>;\nexport interface BatchMatMulAttrs {\n  transposeA: boolean;\n  transposeB: boolean;\n}\n\nexport const BatchToSpaceND = 'BatchToSpaceND';\nexport type BatchToSpaceNDInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface BatchToSpaceNDAttrs {\n  blockShape: number[];\n  crops: number[][];\n}\n\nexport type BinaryInputs = Pick<NamedTensorInfoMap, 'a'|'b'>;\n\nexport const Bincount = 'Bincount';\nexport type BincountInputs = Pick<NamedTensorInfoMap, 'x'|'weights'>;\nexport interface BincountAttrs {\n  size: number;\n}\n\nexport const BroadcastTo = 'BroadcastTo';\nexport type BroadcastToInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface BroadCastToAttrs {\n  shape: number[];\n  inputShape: number[];  // for gradient\n}\n\nexport const BroadcastArgs = 'BroadcastArgs';\nexport type BroadcastArgsInputs = Pick<NamedTensorInfoMap, 's0'|'s1'>;\n\nexport const Cast = 'Cast';\nexport type CastInputs = UnaryInputs;\nexport interface CastAttrs {\n  dtype: DataType;\n}\n\nexport const Ceil = 'Ceil';\nexport type CeilInputs = UnaryInputs;\n\nexport const ClipByValue = 'ClipByValue';\nexport type ClipByValueInputs = UnaryInputs;\nexport interface ClipByValueAttrs {\n  clipValueMin: number;\n  clipValueMax: number;\n}\n\nexport const Complex = 'Complex';\nexport type ComplexInputs = Pick<NamedTensorInfoMap, 'real'|'imag'>;\n\nexport const ComplexAbs = 'ComplexAbs';\nexport type ComplexAbsInputs = UnaryInputs;\n\nexport const Concat = 'Concat';\nexport type ConcatInputs = TensorInfo[];\nexport interface ConcatAttrs {\n  axis: number;\n}\n\nexport const Conv2D = 'Conv2D';\nexport type Conv2DInputs = Pick<NamedTensorInfoMap, 'x'|'filter'>;\nexport interface Conv2DAttrs {\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dataFormat: 'NHWC'|'NCHW';\n  dilations: [number, number]|number;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n}\n\nexport const Conv2DBackpropFilter = 'Conv2DBackpropFilter';\nexport type Conv2DBackpropFilterInputs = Pick<NamedTensorInfoMap, 'x'|'dy'>;\nexport interface Conv2DBackpropFilterAttrs {\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dataFormat: 'NHWC'|'NCHW';\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n  filterShape: [number, number, number, number];\n}\n\nexport const Conv2DBackpropInput = 'Conv2DBackpropInput';\nexport type Conv2DBackpropInputInputs = Pick<NamedTensorInfoMap, 'dy'|'filter'>;\nexport interface Conv2DBackpropInputAttrs {\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dataFormat: 'NHWC'|'NCHW';\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n  inputShape: [number, number, number, number];\n}\n\nexport const Conv3D = 'Conv3D';\nexport type Conv3DInputs = Pick<NamedTensorInfoMap, 'x'|'filter'>;\nexport interface Conv3DAttrs {\n  strides: [number, number, number]|number;\n  pad: 'valid'|'same';\n  dataFormat: 'NDHWC'|'NCDHW';\n  dilations: [number, number, number]|number;\n}\n\nexport const Conv3DBackpropFilterV2 = 'Conv3DBackpropFilterV2';\nexport type Conv3DBackpropFilterV2Inputs = Pick<NamedTensorInfoMap, 'x'|'dy'>;\n\nexport interface Conv3DBackpropFilterV2Attrs {\n  strides: [number, number, number]|number;\n  pad: 'valid'|'same';\n  filterShape: [number, number, number, number, number];\n}\n\nexport const Conv3DBackpropInputV2 = 'Conv3DBackpropInputV2';\nexport type Conv3DBackpropInputV2Inputs =\n    Pick<NamedTensorInfoMap, 'dy'|'filter'>;\nexport interface Conv3DBackpropInputV2Attrs {\n  strides: [number, number, number]|number;\n  pad: 'valid'|'same';\n  inputShape: [number, number, number, number, number];\n}\n\nexport const Cos = 'Cos';\nexport type CosInputs = UnaryInputs;\n\nexport const Cosh = 'Cosh';\nexport type CoshInputs = UnaryInputs;\n\nexport const Cumprod = 'Cumprod';\nexport type CumprodInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface CumprodAttrs {\n  axis: number;\n  exclusive: boolean;\n  reverse: boolean;\n}\n\nexport const Cumsum = 'Cumsum';\nexport type CumsumInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface CumsumAttrs {\n  axis: number;\n  exclusive: boolean;\n  reverse: boolean;\n}\n\nexport const CropAndResize = 'CropAndResize';\nexport type CropAndResizeInputs =\n    Pick<NamedTensorInfoMap, 'image'|'boxes'|'boxInd'>;\nexport interface CropAndResizeAttrs {\n  cropSize: [number, number];\n  method: 'bilinear'|'nearest';\n  extrapolationValue: number;\n}\n\nexport const DenseBincount = 'DenseBincount';\nexport type DenseBincountInputs = Pick<NamedTensorInfoMap, 'x'|'weights'>;\nexport interface DenseBincountAttrs {\n  size: number;\n  binaryOutput?: boolean;\n}\n\nexport const DepthToSpace = 'DepthToSpace';\nexport type DepthToSpaceInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface DepthToSpaceAttrs {\n  blockSize: number;\n  dataFormat: 'NHWC'|'NCHW';\n}\n\nexport const DepthwiseConv2dNative = 'DepthwiseConv2dNative';\nexport type DepthwiseConv2dNativeInputs =\n    Pick<NamedTensorInfoMap, 'x'|'filter'>;\nexport interface DepthwiseConv2dNativeAttrs {\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dataFormat: 'NHWC'|'NCHW';\n  dilations: [number, number]|number;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n}\n\nexport const DepthwiseConv2dNativeBackpropFilter =\n    'DepthwiseConv2dNativeBackpropFilter';\nexport type DepthwiseConv2dNativeBackpropFilterInputs =\n    Pick<NamedTensorInfoMap, 'x'|'dy'>;\nexport interface DepthwiseConv2dNativeBackpropFilterAttrs {\n  strides: [number, number]|number;\n  dilations: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n  filterShape: [number, number, number, number];\n}\n\nexport const DepthwiseConv2dNativeBackpropInput =\n    'DepthwiseConv2dNativeBackpropInput';\nexport type DepthwiseConv2dNativeBackpropInputInputs =\n    Pick<NamedTensorInfoMap, 'dy'|'filter'>;\nexport interface DepthwiseConv2dNativeBackpropInputAttrs {\n  strides: [number, number]|number;\n  dilations: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n  inputShape: [number, number, number, number];\n}\n\nexport const Diag = 'Diag';\nexport type DiagInputs = Pick<NamedTensorInfoMap, 'x'>;\n\nexport const Dilation2D = 'Dilation2D';\nexport type Dilation2DInputs = Pick<NamedTensorInfoMap, 'x'|'filter'>;\nexport interface Dilation2DAttrs {\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number;\n  dilations: [number, number]|number;\n}\n\nexport const Dilation2DBackpropInput = 'Dilation2DBackpropInput';\nexport type Dilation2DBackpropInputInputs =\n    Pick<NamedTensorInfoMap, 'x'|'filter'|'dy'>;\n\nexport const Dilation2DBackpropFilter = 'Dilation2DBackpropFilter';\nexport type Dilation2DBackpropFilterInputs =\n    Pick<NamedTensorInfoMap, 'x'|'filter'|'dy'>;\n\nexport const RealDiv = 'RealDiv';\nexport type RealDivInputs = BinaryInputs;\n\nexport const Einsum = 'Einsum';\nexport type EinsumInputs = TensorInfo[];\nexport interface EinsumAttrs {\n  equation: string;\n}\n\nexport const Elu = 'Elu';\nexport type EluInputs = Pick<NamedTensorInfoMap, 'x'>;\n\nexport const EluGrad = 'EluGrad';\nexport type EluGradInputs = Pick<NamedTensorInfoMap, 'dy'|'y'>;\n\nexport const Erf = 'Erf';\nexport type ErfInputs = UnaryInputs;\n\nexport const Equal = 'Equal';\nexport type EqualInputs = BinaryInputs;\n\nexport const Exp = 'Exp';\nexport type ExpInputs = UnaryInputs;\n\nexport const ExpandDims = 'ExpandDims';\nexport type ExpandDimsInputs = Pick<NamedTensorInfoMap, 'input'>;\nexport interface ExpandDimsAttrs {\n  dim: number;\n}\n\nexport const Expm1 = 'Expm1';\nexport type Expm1Inputs = UnaryInputs;\n\nexport const FFT = 'FFT';\nexport type FFTInputs = Pick<NamedTensorInfoMap, 'input'>;\n\nexport const Fill = 'Fill';\nexport interface FillAttrs {\n  shape: number[];\n  value: number|string;\n  dtype: DataType;\n}\n\nexport const FlipLeftRight = 'FlipLeftRight';\nexport type FlipLeftRightInputs = Pick<NamedTensorInfoMap, 'image'>;\n\nexport const Floor = 'Floor';\nexport type FloorInputs = UnaryInputs;\n\nexport const FloorDiv = 'FloorDiv';\nexport type FloorDivInputs = BinaryInputs;\n\nexport const FusedBatchNorm = 'FusedBatchNorm';\nexport type FusedBatchNormInputs =\n    Pick<NamedTensorInfoMap, 'x'|'scale'|'offset'|'mean'|'variance'>;\nexport interface FusedBatchNormAttrs {\n  varianceEpsilon: number;\n}\n\nexport const GatherV2 = 'GatherV2';\nexport type GatherV2Inputs = Pick<NamedTensorInfoMap, 'x'|'indices'>;\nexport interface GatherV2Attrs {\n  axis: number;\n  batchDims: number;\n}\n\nexport const GatherNd = 'GatherNd';\nexport type GatherNdInputs = Pick<NamedTensorInfoMap, 'params'|'indices'>;\n\nexport const Greater = 'Greater';\nexport type GreaterInputs = BinaryInputs;\n\nexport const GreaterEqual = 'GreaterEqual';\nexport type GreaterEqualInputs = BinaryInputs;\n\nexport const Identity = 'Identity';\nexport type IdentityInputs = Pick<NamedTensorInfoMap, 'x'>;\n\nexport const IFFT = 'IFFT';\nexport type IFFTInputs = Pick<NamedTensorInfoMap, 'input'>;\n\nexport const Imag = 'Imag';\nexport type ImagInputs = Pick<NamedTensorInfoMap, 'input'>;\n\nexport const IsFinite = 'IsFinite';\nexport type IsFiniteInputs = UnaryInputs;\n\nexport const IsInf = 'IsInf';\nexport type IsInfInputs = UnaryInputs;\n\nexport const IsNan = 'IsNan';\nexport type IsNanInputs = UnaryInputs;\n\nexport const LeakyRelu = 'LeakyRelu';\nexport type LeakyReluInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface LeakyReluAttrs {\n  alpha: number;\n}\n\nexport const Less = 'Less';\nexport type LessInputs = BinaryInputs;\n\nexport const LessEqual = 'LessEqual';\nexport type LessEqualInputs = BinaryInputs;\n\nexport const LinSpace = 'LinSpace';\nexport interface LinSpaceAttrs {\n  start: number;\n  stop: number;\n  num: number;\n}\nexport const Log = 'Log';\nexport type LogInputs = UnaryInputs;\n\nexport const Log1p = 'Log1p';\nexport type Log1pInputs = UnaryInputs;\n\nexport const LogicalAnd = 'LogicalAnd';\nexport type LogicalAndInputs = BinaryInputs;\n\nexport const LogicalNot = 'LogicalNot';\nexport type LogicalNotInputs = Pick<NamedTensorInfoMap, 'x'>;\n\nexport const LogicalOr = 'LogicalOr';\nexport type LogicalOrInputs = BinaryInputs;\n\nexport const LogicalXor = 'LogicalXor';\nexport type LogicalXorInputs = BinaryInputs;\n\nexport const LogSoftmax = 'LogSoftmax';\nexport type LogSoftmaxInputs = Pick<NamedTensorInfoMap, 'logits'>;\nexport interface LogSoftmaxAttrs {\n  axis: number;\n}\n\nexport const LowerBound = 'LowerBound';\nexport type LowerBoundInputs =\n    Pick<NamedTensorInfoMap, 'sortedSequence'|'values'>;\n\nexport const LRN = 'LRN';\nexport type LRNInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface LRNAttrs {\n  depthRadius: number;\n  bias: number;\n  alpha: number;\n  beta: number;\n}\n\nexport const LRNGrad = 'LRNGrad';\nexport type LRNGradInputs = Pick<NamedTensorInfoMap, 'x'|'y'|'dy'>;\nexport interface LRNGradAttrs {\n  depthRadius: number;\n  bias: number;\n  alpha: number;\n  beta: number;\n}\n\nexport const Max = 'Max';\nexport type MaxInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface MaxAttrs {\n  reductionIndices: number|number[];\n  keepDims: boolean;\n}\n\nexport const Maximum = 'Maximum';\nexport type MaximumInputs = BinaryInputs;\n\nexport const MaxPool = 'MaxPool';\nexport type MaxPoolInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface MaxPoolAttrs {\n  filterSize: [number, number]|number;\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n}\n\nexport const MaxPoolGrad = 'MaxPoolGrad';\nexport type MaxPoolGradInputs = Pick<NamedTensorInfoMap, 'dy'|'input'|'output'>;\nexport interface MaxPoolGradAttrs {\n  filterSize: [number, number]|number;\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n}\n\nexport const MaxPool3D = 'MaxPool3D';\nexport type MaxPool3DInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface MaxPool3DAttrs {\n  filterSize: [number, number, number]|number;\n  strides: [number, number, number]|number;\n  pad: 'valid'|'same'|number;\n  dataFormat: 'NDHWC'|'NCDHW';\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n}\n\nexport const MaxPool3DGrad = 'MaxPool3DGrad';\nexport type MaxPool3DGradInputs =\n    Pick<NamedTensorInfoMap, 'dy'|'input'|'output'>;\nexport interface MaxPool3DGradAttrs {\n  filterSize: [number, number, number]|number;\n  strides: [number, number, number]|number;\n  pad: 'valid'|'same'|number;\n  dimRoundingMode?: 'floor'|'round'|'ceil';\n}\n\nexport const MaxPoolWithArgmax = 'MaxPoolWithArgmax';\nexport type MaxPoolWithArgmaxInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface MaxPoolWithArgmaxAttrs {\n  filterSize: [number, number]|number;\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number;\n  includeBatchInIndex: boolean;\n}\n\nexport const Mean = 'Mean';\nexport type MeanInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface MeanAttrs {\n  axis: number|number[];\n  keepDims: boolean;\n}\n\nexport const Min = 'Min';\nexport type MinInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface MinAttrs {\n  axis: number|number[];\n  keepDims: boolean;\n}\n\nexport const Minimum = 'Minimum';\nexport type MinimumInputs = BinaryInputs;\n\nexport const MirrorPad = 'MirrorPad';\nexport type MirrorPadInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface MirrorPadAttrs {\n  paddings: Array<[number, number]>;\n  mode: 'reflect'|'symmetric';\n}\n\nexport const Mod = 'Mod';\nexport type ModInputs = BinaryInputs;\n\nexport const Multinomial = 'Multinomial';\nexport type MultinomialInputs = Pick<NamedTensorInfoMap, 'logits'>;\nexport interface MultinomialAttrs {\n  numSamples: number;\n  seed: number;\n  normalized: boolean;\n}\n\nexport const Multiply = 'Multiply';\nexport type MultiplyInputs = BinaryInputs;\n\nexport const Neg = 'Neg';\nexport type NegInputs = UnaryInputs;\n\nexport const NotEqual = 'NotEqual';\nexport type NotEqualInputs = BinaryInputs;\n\nexport const NonMaxSuppressionV3 = 'NonMaxSuppressionV3';\nexport type NonMaxSuppressionV3Inputs =\n    Pick<NamedTensorInfoMap, 'boxes'|'scores'>;\nexport interface NonMaxSuppressionV3Attrs {\n  maxOutputSize: number;\n  iouThreshold: number;\n  scoreThreshold: number;\n}\n\nexport const NonMaxSuppressionV4 = 'NonMaxSuppressionV4';\nexport type NonMaxSuppressionV4Inputs =\n    Pick<NamedTensorInfoMap, 'boxes'|'scores'>;\nexport interface NonMaxSuppressionV4Attrs {\n  maxOutputSize: number;\n  iouThreshold: number;\n  scoreThreshold: number;\n  padToMaxOutputSize: boolean;\n}\n\nexport const NonMaxSuppressionV5 = 'NonMaxSuppressionV5';\nexport type NonMaxSuppressionV5Inputs =\n    Pick<NamedTensorInfoMap, 'boxes'|'scores'>;\nexport interface NonMaxSuppressionV5Attrs {\n  maxOutputSize: number;\n  iouThreshold: number;\n  scoreThreshold: number;\n  softNmsSigma: number;\n}\n\nexport const OnesLike = 'OnesLike';\nexport type OnesLikeInputs = UnaryInputs;\n\nexport const OneHot = 'OneHot';\nexport type OneHotInputs = Pick<NamedTensorInfoMap, 'indices'>;\nexport interface OneHotAttrs {\n  depth: number;\n  onValue: number;\n  offValue: number;\n  dtype: DataType;\n}\n\nexport const Pack = 'Pack';\nexport type PackInputs = TensorInfo[];\nexport interface PackAttrs {\n  axis: number;\n}\n\nexport const PadV2 = 'PadV2';\nexport type PadV2Inputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface PadV2Attrs {\n  paddings: Array<[number, number]>;\n  constantValue: number;\n}\n\nexport const Pool = 'Pool';\nexport type PoolInputs = Pick<NamedTensorInfoMap, 'input'>;\n\nexport const Pow = 'Pow';\nexport type PowInputs = BinaryInputs;\n\nexport const Prelu = 'Prelu';\nexport type PreluInputs = Pick<NamedTensorInfoMap, 'x'|'alpha'>;\n\nexport const Prod = 'Prod';\nexport type ProdInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface ProdAttrs {\n  axis: number|number[];\n  keepDims: boolean;\n}\n\nexport const RaggedGather = 'RaggedGather';\nexport type RaggedGatherInputs = {\n  paramsNestedSplits: TensorInfo[]\n}&Pick<NamedTensorInfoMap, 'paramsDenseValues'|'indices'>;\nexport interface RaggedGatherAttrs {\n  outputRaggedRank: number;\n}\n\nexport const RaggedTensorToTensor = 'RaggedTensorToTensor';\nexport type RaggedTensorToTensorInputs =\n    Pick<NamedTensorInfoMap, 'shape'|'values'|'defaultValue'>&\n    {rowPartitionTensors: TensorInfo[]};\nexport interface RaggedTensorToTensorAttrs {\n  rowPartitionTypes: string[];\n}\n\nexport const Range = 'Range';\nexport interface RangeAttrs {\n  start: number;\n  stop: number;\n  step: number;\n  dtype: 'float32'|'int32';\n}\n\nexport const Real = 'Real';\nexport type RealInputs = Pick<NamedTensorInfoMap, 'input'>;\n\nexport const Reciprocal = 'Reciprocal';\nexport type ReciprocalInputs = UnaryInputs;\n\nexport const Relu = 'Relu';\nexport type ReluInputs = Pick<NamedTensorInfoMap, 'x'>;\n\nexport const Reshape = 'Reshape';\nexport type ReshapeInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface ReshapeAttrs {\n  shape: number[];\n}\n\nexport const ResizeNearestNeighbor = 'ResizeNearestNeighbor';\nexport type ResizeNearestNeighborInputs = Pick<NamedTensorInfoMap, 'images'>;\nexport interface ResizeNearestNeighborAttrs {\n  alignCorners: boolean;\n  halfPixelCenters: boolean;\n  size: [number, number];\n}\n\nexport const ResizeNearestNeighborGrad = 'ResizeNearestNeighborGrad';\nexport type ResizeNearestNeighborGradInputs =\n    Pick<NamedTensorInfoMap, 'images'|'dy'>;\nexport type ResizeNearestNeighborGradAttrs = ResizeNearestNeighborAttrs;\n\nexport const ResizeBilinear = 'ResizeBilinear';\nexport type ResizeBilinearInputs = Pick<NamedTensorInfoMap, 'images'>;\nexport interface ResizeBilinearAttrs {\n  alignCorners: boolean;\n  halfPixelCenters: boolean;\n  size: [number, number];\n}\n\nexport const ResizeBilinearGrad = 'ResizeBilinearGrad';\nexport type ResizeBilinearGradInputs = Pick<NamedTensorInfoMap, 'images'|'dy'>;\nexport type ResizeBilinearGradAttrs = ResizeBilinearAttrs;\n\nexport const Relu6 = 'Relu6';\nexport type Relu6Inputs = Pick<NamedTensorInfoMap, 'x'>;\n\nexport const Reverse = 'Reverse';\nexport type ReverseInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface ReverseAttrs {\n  dims: number|number[];\n}\n\nexport const Round = 'Round';\nexport type RoundInputs = UnaryInputs;\n\nexport const Rsqrt = 'Rsqrt';\nexport type RsqrtInputs = UnaryInputs;\n\nexport const ScatterNd = 'ScatterNd';\nexport type ScatterNdInputs = Pick<NamedTensorInfoMap, 'indices'|'updates'>;\nexport interface ScatterNdAttrs {\n  shape: number[];\n}\n\nexport const SearchSorted = 'SearchSorted';\nexport type SearchSortedInputs =\n    Pick<NamedTensorInfoMap, 'sortedSequence'|'values'>;\nexport interface SearchSortedAttrs {\n  side: 'left'|'right';\n}\n\nexport const Select = 'Select';\nexport type SelectInputs = Pick<NamedTensorInfoMap, 'condition'|'t'|'e'>;\n\nexport const Selu = 'Selu';\nexport type SeluInputs = Pick<NamedTensorInfoMap, 'x'>;\n\nexport const Slice = 'Slice';\nexport type SliceInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface SliceAttrs {\n  begin: number|number[];\n  size: number|number[];\n}\nexport const Sin = 'Sin';\nexport type SinInputs = UnaryInputs;\n\nexport const Sinh = 'Sinh';\nexport type SinhInputs = UnaryInputs;\n\nexport const Sign = 'Sign';\nexport type SignInputs = UnaryInputs;\n\nexport const Sigmoid = 'Sigmoid';\nexport type SigmoidInputs = UnaryInputs;\n\nexport const Softplus = 'Softplus';\nexport type SoftplusInputs = UnaryInputs;\n\nexport const Sqrt = 'Sqrt';\nexport type SqrtInputs = UnaryInputs;\n\nexport const Sum = 'Sum';\nexport type SumInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface SumAttrs {\n  axis: number|number[];\n  keepDims: boolean;\n}\n\nexport const SpaceToBatchND = 'SpaceToBatchND';\nexport type SpaceToBatchNDInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface SpaceToBatchNDAttrs {\n  blockShape: number[];\n  paddings: number[][];\n}\n\nexport const SplitV = 'SplitV';\nexport type SplitVInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface SplitVAttrs {\n  numOrSizeSplits: number[]|number;\n  axis: number;\n}\n\nexport const Softmax = 'Softmax';\nexport type SoftmaxInputs = Pick<NamedTensorInfoMap, 'logits'>;\nexport interface SoftmaxAttrs {\n  dim: number;\n}\n\nexport const SparseFillEmptyRows = 'SparseFillEmptyRows';\nexport type SparseFillEmptyRowsInputs =\n    Pick<NamedTensorInfoMap, 'indices'|'values'|'denseShape'|'defaultValue'>;\n\nexport const SparseReshape = 'SparseReshape';\nexport type SparseReshapeInputs =\n    Pick<NamedTensorInfoMap, 'inputIndices'|'inputShape'|'newShape'>;\n\nexport const SparseSegmentMean = 'SparseSegmentMean';\nexport type SparseSegmentMeanInputs =\n    Pick<NamedTensorInfoMap, 'data'|'indices'|'segmentIds'>;\n\nexport const SparseSegmentSum = 'SparseSegmentSum';\nexport type SparseSegmentSumInputs =\n    Pick<NamedTensorInfoMap, 'data'|'indices'|'segmentIds'>;\n\nexport const SparseToDense = 'SparseToDense';\nexport type SparseToDenseInputs =\n    Pick<NamedTensorInfoMap, 'sparseIndices'|'sparseValues'|'defaultValue'>;\nexport interface SparseToDenseAttrs {\n  outputShape: number[];\n}\n\nexport const SquaredDifference = 'SquaredDifference';\nexport type SquaredDifferenceInputs = BinaryInputs;\n\nexport const Square = 'Square';\nexport type SquareInputs = Pick<NamedTensorInfoMap, 'x'>;\n\nexport const StridedSlice = 'StridedSlice';\nexport type StridedSliceInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface StridedSliceAttrs {\n  begin: number[];\n  end: number[];\n  strides: number[];\n  beginMask: number;\n  endMask: number;\n  ellipsisMask: number;\n  newAxisMask: number;\n  shrinkAxisMask: number;\n}\n\nexport const StringNGrams = 'StringNGrams';\nexport type StringNGramsInputs = Pick<NamedTensorInfoMap, 'data'|'dataSplits'>;\nexport interface StringNGramsAttrs {\n  separator: string;\n  nGramWidths: number[];\n  leftPad: string;\n  rightPad: string;\n  padWidth: number;\n  preserveShortSequences: boolean;\n}\n\nexport const StringSplit = 'StringSplit';\nexport type StringSplitInputs = Pick<NamedTensorInfoMap, 'input'|'delimiter'>;\nexport interface StringSplitAttrs {\n  skipEmpty: boolean;\n}\n\nexport const StringToHashBucketFast = 'StringToHashBucketFast';\nexport type StringToHashBucketFastInputs = Pick<NamedTensorInfoMap, 'input'>;\nexport interface StringToHashBucketFastAttrs {\n  numBuckets: number;\n}\n\nexport const Sub = 'Sub';\nexport type SubInputs = BinaryInputs;\n\nexport const Tan = 'Tan';\nexport type TanInputs = UnaryInputs;\n\nexport const Tanh = 'Tanh';\nexport type TanhInputs = UnaryInputs;\n\nexport const Tile = 'Tile';\nexport type TileInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface TileAttrs {\n  reps: number[];\n}\n\nexport const TopK = 'TopK';\nexport type TopKInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface TopKAttrs {\n  k: number;\n  sorted: boolean;\n}\n\nexport const Transform = 'Transform';\nexport type TransformInputs = Pick<NamedTensorInfoMap, 'image'|'transforms'>;\nexport interface TransformAttrs {\n  interpolation: 'nearest'|'bilinear';\n  fillMode: 'constant'|'reflect'|'wrap'|'nearest';\n  fillValue: number;\n  outputShape?: [number, number];\n}\n\nexport const Transpose = 'Transpose';\nexport type TransposeInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface TransposeAttrs {\n  perm: number[];\n}\n\nexport const Unique = 'Unique';\nexport type UniqueInputs = Pick<NamedTensorInfoMap, 'x'>;\nexport interface UniqueAttrs {\n  axis: number;\n}\n\nexport type UnaryInputs = Pick<NamedTensorInfoMap, 'x'>;\n\nexport const Unpack = 'Unpack';\nexport type UnpackInputs = Pick<NamedTensorInfoMap, 'value'>;\nexport interface UnpackAttrs {\n  axis: number;\n}\n\nexport const UnsortedSegmentSum = 'UnsortedSegmentSum';\nexport type UnsortedSegmentSumInputs =\n    Pick<NamedTensorInfoMap, 'x'|'segmentIds'>;\nexport interface UnsortedSegmentSumAttrs {\n  numSegments: number;\n}\n\nexport const UpperBound = 'UpperBound';\nexport type UpperBoundInputs =\n    Pick<NamedTensorInfoMap, 'sortedSequence'|'values'>;\n\nexport const ZerosLike = 'ZerosLike';\nexport type ZerosLikeInputs = UnaryInputs;\n\n/**\n * TensorFlow.js-only kernels\n */\nexport const Step = 'Step';\nexport type StepInputs = UnaryInputs;\nexport interface StepAttrs {\n  alpha: number;\n}\n\nexport const FromPixels = 'FromPixels';\nexport interface FromPixelsInputs {\n  pixels: PixelData|ImageData|HTMLImageElement|HTMLCanvasElement|\n      HTMLVideoElement|ImageBitmap;\n}\nexport interface FromPixelsAttrs {\n  numChannels: number;\n}\n\nexport const RotateWithOffset = 'RotateWithOffset';\nexport type RotateWithOffsetInputs = Pick<NamedTensorInfoMap, 'image'>;\nexport interface RotateWithOffsetAttrs {\n  radians: number;\n  fillValue: number|[number, number, number];\n  center: number|[number, number];\n}\n\nexport const _FusedMatMul = '_FusedMatMul';\n// tslint:disable-next-line: class-name\nexport interface _FusedMatMulInputs extends NamedTensorInfoMap {\n  a: TensorInfo;\n  b: TensorInfo;\n  bias?: TensorInfo;\n  preluActivationWeights?: TensorInfo;\n}\n// tslint:disable-next-line: class-name\nexport interface _FusedMatMulAttrs {\n  transposeA: boolean;\n  transposeB: boolean;\n  activation: Activation;\n  leakyreluAlpha?: number;\n}\n\nexport const FusedConv2D = 'FusedConv2D';\nexport interface FusedConv2DInputs extends NamedTensorInfoMap {\n  x: TensorInfo;\n  filter: TensorInfo;\n  bias?: TensorInfo;\n  preluActivationWeights?: TensorInfo;\n}\nexport interface FusedConv2DAttrs {\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dataFormat: 'NHWC'|'NCHW';\n  dilations: [number, number]|number;\n  dimRoundingMode: 'floor'|'round'|'ceil';\n  activation: Activation;\n  leakyreluAlpha?: number;\n}\n\nexport const FusedDepthwiseConv2D = 'FusedDepthwiseConv2D';\nexport interface FusedDepthwiseConv2DInputs extends NamedTensorInfoMap {\n  x: TensorInfo;\n  filter: TensorInfo;\n  bias?: TensorInfo;\n  preluActivationWeights?: TensorInfo;\n}\nexport interface FusedDepthwiseConv2DAttrs {\n  strides: [number, number]|number;\n  pad: 'valid'|'same'|number|ExplicitPadding;\n  dataFormat: 'NHWC'|'NCHW';\n  dilations: [number, number]|number;\n  dimRoundingMode: 'floor'|'round'|'ceil';\n  activation: Activation;\n  leakyreluAlpha?: number;\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {env} from './environment';\n\nexport function warn(...msg: Array<{}>): void {\n  if (!(env().getBool('IS_TEST') || env().getBool('PROD'))) {\n    console.warn(...msg);\n  }\n}\n\nexport function log(...msg: Array<{}>): void {\n  if (!(env().getBool('IS_TEST') || env().getBool('PROD'))) {\n    console.log(...msg);\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {env} from './environment';\nimport {getGlobal} from './global_util';\nimport * as log from './log';\nimport {NamedGradientMap} from './tape';\nimport {Tensor} from './tensor';\nimport {DataType, RecursiveArray} from './types';\n\nconst kernelRegistry =\n    getGlobal('kernelRegistry', () => new Map<string, KernelConfig>());\nconst gradRegistry =\n    getGlobal('gradRegistry', () => new Map<string, GradConfig>());\n\nexport type DataId = object;\n\ntype AttributeValue =\n    number|number[]|boolean|boolean[]|string|string[]|NamedAttrMap;\n\n/** These are extra non-tensor/primitive params passed to kernel functions. */\nexport type Attribute = AttributeValue|RecursiveArray<AttributeValue>;\n\n/** Specifies the code to run when executing a kernel. */\nexport type KernelFunc = (params: {\n  inputs: NamedTensorInfoMap,\n  backend: {},\n  attrs?: NamedAttrMap,\n}) => TensorInfo|TensorInfo[];\n\n/** The function to run when computing a gradient during backprop. */\nexport type GradFunc =\n    (dy: Tensor|Tensor[], saved: Tensor[], attrs: NamedAttrMap) =>\n        NamedGradientMap;\n\n/** Function that gets called after the backend initializes. */\nexport type KernelSetupFunc = (backend: {}) => void;\n/** Function that gets called right before the backend is disposed. */\nexport type KernelDisposeFunc = KernelSetupFunc;\n\n/** Config object for registering a kernel in the global registry. */\nexport interface KernelConfig {\n  kernelName: string;\n  backendName: string;\n  kernelFunc: KernelFunc;\n  setupFunc?: KernelSetupFunc;\n  disposeFunc?: KernelDisposeFunc;\n}\n\n/** Config object for registering a gradient in the global registry. */\nexport interface GradConfig {\n  kernelName: string;\n  inputsToSave?: string[];\n  // When saveAllInputs is true, all inputs will be saved. Only use this flag\n  // if inputs is an array of Tensors.\n  saveAllInputs?: boolean;\n  outputsToSave?: boolean[];\n  gradFunc: GradFunc;\n}\n\n/** Holds metadata for a given tensor. */\nexport interface TensorInfo {\n  dataId: DataId;\n  shape: number[];\n  dtype: DataType;\n}\n\nexport interface NamedTensorInfoMap {\n  [name: string]: TensorInfo|undefined;\n}\n\nexport interface NamedAttrMap {\n  [name: string]: Attribute;\n}\n\n/**\n * Returns the kernel function (code) associated with the provided names.\n *\n * @param kernelName The official name of the kernel.\n * @param backendName The official name of the backend.\n */\nexport function getKernel(\n    kernelName: string, backendName: string): KernelConfig {\n  const key = makeKey(kernelName, backendName);\n  return kernelRegistry.get(key);\n}\n\n/**\n * Returns the registered gradient info associated with the provided kernel.\n * @param kernelName The official TF kernel name.\n */\nexport function getGradient(kernelName: string): GradConfig {\n  return gradRegistry.get(kernelName);\n}\n\nexport function getKernelsForBackend(backendName: string): KernelConfig[] {\n  const it = kernelRegistry.entries();\n  const result: KernelConfig[] = [];\n\n  while (true) {\n    const {done, value} = it.next();\n    if (done) {\n      break;\n    }\n    const [key, config] = value;\n    const [backend, ] = key.split('_');\n    if (backend === backendName) {\n      result.push(config);\n    }\n  }\n  return result;\n}\n\n/**\n * Registers the function (forward pass) for the kernel in a global registry.\n *\n * @param config A config object with the following properties:\n * - `kernelName` The official name of the kernel.\n * - `backendName` The official name of the backend.\n * - `kernelFunc` The function to run during the forward pass of the kernel.\n * - `setupFunc` Optional. Gets called once, after the backend initializes.\n * - `disposeFunc` Optional. Gets called once, right before the backend is\n * disposed.\n */\nexport function registerKernel(config: KernelConfig) {\n  const {kernelName, backendName} = config;\n  const key = makeKey(kernelName, backendName);\n  if (kernelRegistry.has(key)) {\n    log.warn(\n        `The kernel '${kernelName}' for backend ` +\n        `'${backendName}' is already registered`);\n  }\n  kernelRegistry.set(key, config);\n}\n\n/**\n * Registers a gradient function for a given kernel in the global registry,\n * to be used during the back-propagation of that kernel.\n *\n * @param config An object with the following properties:\n * - `kernelName` The name of the kernel that the gradient function is for.\n * - `gradFunc` The function to run during back-propagation.\n */\nexport function registerGradient(config: GradConfig) {\n  const {kernelName} = config;\n\n  if (gradRegistry.has(kernelName)) {\n    // TODO (yassogba) after 3.0 assess whether we need to keep this gated\n    // to debug mode.\n    if (env().getBool('DEBUG')) {\n      log.warn(`Overriding the gradient for '${kernelName}'`);\n    }\n  }\n  gradRegistry.set(kernelName, config);\n}\n\n/**\n * Removes the kernel function from the registry.\n *\n * @param kernelName The official name of the kernel.\n * @param backendName The official name of the backend.\n *\n */\nexport function unregisterKernel(\n    kernelName: string, backendName: string): void {\n  const key = makeKey(kernelName, backendName);\n  if (!kernelRegistry.has(key)) {\n    throw new Error(\n        `The kernel '${kernelName}' for backend ` +\n        `'${backendName}' is not registered`);\n  }\n  kernelRegistry.delete(key);\n}\n\n/** Removes the registered gradient from the global registry. */\nexport function unregisterGradient(kernelName: string): void {\n  if (!gradRegistry.has(kernelName)) {\n    throw new Error(\n        `The gradient '${kernelName}' for backend is not registered`);\n  }\n  gradRegistry.delete(kernelName);\n}\n\n/**\n * Finds kernels that have already been registered to a backend and re-registers\n * them for a new backend. Useful for registering custom backends.\n * @param registeredBackendName Already registered backend.\n * @param newBackendName New backend.\n */\nexport function copyRegisteredKernels(\n    registeredBackendName: string, newBackendName: string): void {\n  const kernels = getKernelsForBackend(registeredBackendName);\n  kernels.forEach(kernelConfig => {\n    const newKernelConfig =\n        Object.assign({}, kernelConfig, {backendName: newBackendName});\n    registerKernel(newKernelConfig);\n  });\n}\n\nfunction makeKey(kernelName: string, backendName: string) {\n  return `${backendName}_${kernelName}`;\n}\n","module.exports = Long;\r\n\r\n/**\r\n * wasm optimizations, to do native i64 multiplication and divide\r\n */\r\nvar wasm = null;\r\n\r\ntry {\r\n  wasm = new WebAssembly.Instance(new WebAssembly.Module(new Uint8Array([\r\n    0, 97, 115, 109, 1, 0, 0, 0, 1, 13, 2, 96, 0, 1, 127, 96, 4, 127, 127, 127, 127, 1, 127, 3, 7, 6, 0, 1, 1, 1, 1, 1, 6, 6, 1, 127, 1, 65, 0, 11, 7, 50, 6, 3, 109, 117, 108, 0, 1, 5, 100, 105, 118, 95, 115, 0, 2, 5, 100, 105, 118, 95, 117, 0, 3, 5, 114, 101, 109, 95, 115, 0, 4, 5, 114, 101, 109, 95, 117, 0, 5, 8, 103, 101, 116, 95, 104, 105, 103, 104, 0, 0, 10, 191, 1, 6, 4, 0, 35, 0, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 126, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 127, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 128, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 129, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 130, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11\r\n  ])), {}).exports;\r\n} catch (e) {\r\n  // no wasm support :(\r\n}\r\n\r\n/**\r\n * Constructs a 64 bit two's-complement integer, given its low and high 32 bit values as *signed* integers.\r\n *  See the from* functions below for more convenient ways of constructing Longs.\r\n * @exports Long\r\n * @class A Long class for representing a 64 bit two's-complement integer value.\r\n * @param {number} low The low (signed) 32 bits of the long\r\n * @param {number} high The high (signed) 32 bits of the long\r\n * @param {boolean=} unsigned Whether unsigned or not, defaults to signed\r\n * @constructor\r\n */\r\nfunction Long(low, high, unsigned) {\r\n\r\n    /**\r\n     * The low 32 bits as a signed value.\r\n     * @type {number}\r\n     */\r\n    this.low = low | 0;\r\n\r\n    /**\r\n     * The high 32 bits as a signed value.\r\n     * @type {number}\r\n     */\r\n    this.high = high | 0;\r\n\r\n    /**\r\n     * Whether unsigned or not.\r\n     * @type {boolean}\r\n     */\r\n    this.unsigned = !!unsigned;\r\n}\r\n\r\n// The internal representation of a long is the two given signed, 32-bit values.\r\n// We use 32-bit pieces because these are the size of integers on which\r\n// Javascript performs bit-operations.  For operations like addition and\r\n// multiplication, we split each number into 16 bit pieces, which can easily be\r\n// multiplied within Javascript's floating-point representation without overflow\r\n// or change in sign.\r\n//\r\n// In the algorithms below, we frequently reduce the negative case to the\r\n// positive case by negating the input(s) and then post-processing the result.\r\n// Note that we must ALWAYS check specially whether those values are MIN_VALUE\r\n// (-2^63) because -MIN_VALUE == MIN_VALUE (since 2^63 cannot be represented as\r\n// a positive number, it overflows back into a negative).  Not handling this\r\n// case would often result in infinite recursion.\r\n//\r\n// Common constant values ZERO, ONE, NEG_ONE, etc. are defined below the from*\r\n// methods on which they depend.\r\n\r\n/**\r\n * An indicator used to reliably determine if an object is a Long or not.\r\n * @type {boolean}\r\n * @const\r\n * @private\r\n */\r\nLong.prototype.__isLong__;\r\n\r\nObject.defineProperty(Long.prototype, \"__isLong__\", { value: true });\r\n\r\n/**\r\n * @function\r\n * @param {*} obj Object\r\n * @returns {boolean}\r\n * @inner\r\n */\r\nfunction isLong(obj) {\r\n    return (obj && obj[\"__isLong__\"]) === true;\r\n}\r\n\r\n/**\r\n * Tests if the specified object is a Long.\r\n * @function\r\n * @param {*} obj Object\r\n * @returns {boolean}\r\n */\r\nLong.isLong = isLong;\r\n\r\n/**\r\n * A cache of the Long representations of small integer values.\r\n * @type {!Object}\r\n * @inner\r\n */\r\nvar INT_CACHE = {};\r\n\r\n/**\r\n * A cache of the Long representations of small unsigned integer values.\r\n * @type {!Object}\r\n * @inner\r\n */\r\nvar UINT_CACHE = {};\r\n\r\n/**\r\n * @param {number} value\r\n * @param {boolean=} unsigned\r\n * @returns {!Long}\r\n * @inner\r\n */\r\nfunction fromInt(value, unsigned) {\r\n    var obj, cachedObj, cache;\r\n    if (unsigned) {\r\n        value >>>= 0;\r\n        if (cache = (0 <= value && value < 256)) {\r\n            cachedObj = UINT_CACHE[value];\r\n            if (cachedObj)\r\n                return cachedObj;\r\n        }\r\n        obj = fromBits(value, (value | 0) < 0 ? -1 : 0, true);\r\n        if (cache)\r\n            UINT_CACHE[value] = obj;\r\n        return obj;\r\n    } else {\r\n        value |= 0;\r\n        if (cache = (-128 <= value && value < 128)) {\r\n            cachedObj = INT_CACHE[value];\r\n            if (cachedObj)\r\n                return cachedObj;\r\n        }\r\n        obj = fromBits(value, value < 0 ? -1 : 0, false);\r\n        if (cache)\r\n            INT_CACHE[value] = obj;\r\n        return obj;\r\n    }\r\n}\r\n\r\n/**\r\n * Returns a Long representing the given 32 bit integer value.\r\n * @function\r\n * @param {number} value The 32 bit integer in question\r\n * @param {boolean=} unsigned Whether unsigned or not, defaults to signed\r\n * @returns {!Long} The corresponding Long value\r\n */\r\nLong.fromInt = fromInt;\r\n\r\n/**\r\n * @param {number} value\r\n * @param {boolean=} unsigned\r\n * @returns {!Long}\r\n * @inner\r\n */\r\nfunction fromNumber(value, unsigned) {\r\n    if (isNaN(value))\r\n        return unsigned ? UZERO : ZERO;\r\n    if (unsigned) {\r\n        if (value < 0)\r\n            return UZERO;\r\n        if (value >= TWO_PWR_64_DBL)\r\n            return MAX_UNSIGNED_VALUE;\r\n    } else {\r\n        if (value <= -TWO_PWR_63_DBL)\r\n            return MIN_VALUE;\r\n        if (value + 1 >= TWO_PWR_63_DBL)\r\n            return MAX_VALUE;\r\n    }\r\n    if (value < 0)\r\n        return fromNumber(-value, unsigned).neg();\r\n    return fromBits((value % TWO_PWR_32_DBL) | 0, (value / TWO_PWR_32_DBL) | 0, unsigned);\r\n}\r\n\r\n/**\r\n * Returns a Long representing the given value, provided that it is a finite number. Otherwise, zero is returned.\r\n * @function\r\n * @param {number} value The number in question\r\n * @param {boolean=} unsigned Whether unsigned or not, defaults to signed\r\n * @returns {!Long} The corresponding Long value\r\n */\r\nLong.fromNumber = fromNumber;\r\n\r\n/**\r\n * @param {number} lowBits\r\n * @param {number} highBits\r\n * @param {boolean=} unsigned\r\n * @returns {!Long}\r\n * @inner\r\n */\r\nfunction fromBits(lowBits, highBits, unsigned) {\r\n    return new Long(lowBits, highBits, unsigned);\r\n}\r\n\r\n/**\r\n * Returns a Long representing the 64 bit integer that comes by concatenating the given low and high bits. Each is\r\n *  assumed to use 32 bits.\r\n * @function\r\n * @param {number} lowBits The low 32 bits\r\n * @param {number} highBits The high 32 bits\r\n * @param {boolean=} unsigned Whether unsigned or not, defaults to signed\r\n * @returns {!Long} The corresponding Long value\r\n */\r\nLong.fromBits = fromBits;\r\n\r\n/**\r\n * @function\r\n * @param {number} base\r\n * @param {number} exponent\r\n * @returns {number}\r\n * @inner\r\n */\r\nvar pow_dbl = Math.pow; // Used 4 times (4*8 to 15+4)\r\n\r\n/**\r\n * @param {string} str\r\n * @param {(boolean|number)=} unsigned\r\n * @param {number=} radix\r\n * @returns {!Long}\r\n * @inner\r\n */\r\nfunction fromString(str, unsigned, radix) {\r\n    if (str.length === 0)\r\n        throw Error('empty string');\r\n    if (str === \"NaN\" || str === \"Infinity\" || str === \"+Infinity\" || str === \"-Infinity\")\r\n        return ZERO;\r\n    if (typeof unsigned === 'number') {\r\n        // For goog.math.long compatibility\r\n        radix = unsigned,\r\n        unsigned = false;\r\n    } else {\r\n        unsigned = !! unsigned;\r\n    }\r\n    radix = radix || 10;\r\n    if (radix < 2 || 36 < radix)\r\n        throw RangeError('radix');\r\n\r\n    var p;\r\n    if ((p = str.indexOf('-')) > 0)\r\n        throw Error('interior hyphen');\r\n    else if (p === 0) {\r\n        return fromString(str.substring(1), unsigned, radix).neg();\r\n    }\r\n\r\n    // Do several (8) digits each time through the loop, so as to\r\n    // minimize the calls to the very expensive emulated div.\r\n    var radixToPower = fromNumber(pow_dbl(radix, 8));\r\n\r\n    var result = ZERO;\r\n    for (var i = 0; i < str.length; i += 8) {\r\n        var size = Math.min(8, str.length - i),\r\n            value = parseInt(str.substring(i, i + size), radix);\r\n        if (size < 8) {\r\n            var power = fromNumber(pow_dbl(radix, size));\r\n            result = result.mul(power).add(fromNumber(value));\r\n        } else {\r\n            result = result.mul(radixToPower);\r\n            result = result.add(fromNumber(value));\r\n        }\r\n    }\r\n    result.unsigned = unsigned;\r\n    return result;\r\n}\r\n\r\n/**\r\n * Returns a Long representation of the given string, written using the specified radix.\r\n * @function\r\n * @param {string} str The textual representation of the Long\r\n * @param {(boolean|number)=} unsigned Whether unsigned or not, defaults to signed\r\n * @param {number=} radix The radix in which the text is written (2-36), defaults to 10\r\n * @returns {!Long} The corresponding Long value\r\n */\r\nLong.fromString = fromString;\r\n\r\n/**\r\n * @function\r\n * @param {!Long|number|string|!{low: number, high: number, unsigned: boolean}} val\r\n * @param {boolean=} unsigned\r\n * @returns {!Long}\r\n * @inner\r\n */\r\nfunction fromValue(val, unsigned) {\r\n    if (typeof val === 'number')\r\n        return fromNumber(val, unsigned);\r\n    if (typeof val === 'string')\r\n        return fromString(val, unsigned);\r\n    // Throws for non-objects, converts non-instanceof Long:\r\n    return fromBits(val.low, val.high, typeof unsigned === 'boolean' ? unsigned : val.unsigned);\r\n}\r\n\r\n/**\r\n * Converts the specified value to a Long using the appropriate from* function for its type.\r\n * @function\r\n * @param {!Long|number|string|!{low: number, high: number, unsigned: boolean}} val Value\r\n * @param {boolean=} unsigned Whether unsigned or not, defaults to signed\r\n * @returns {!Long}\r\n */\r\nLong.fromValue = fromValue;\r\n\r\n// NOTE: the compiler should inline these constant values below and then remove these variables, so there should be\r\n// no runtime penalty for these.\r\n\r\n/**\r\n * @type {number}\r\n * @const\r\n * @inner\r\n */\r\nvar TWO_PWR_16_DBL = 1 << 16;\r\n\r\n/**\r\n * @type {number}\r\n * @const\r\n * @inner\r\n */\r\nvar TWO_PWR_24_DBL = 1 << 24;\r\n\r\n/**\r\n * @type {number}\r\n * @const\r\n * @inner\r\n */\r\nvar TWO_PWR_32_DBL = TWO_PWR_16_DBL * TWO_PWR_16_DBL;\r\n\r\n/**\r\n * @type {number}\r\n * @const\r\n * @inner\r\n */\r\nvar TWO_PWR_64_DBL = TWO_PWR_32_DBL * TWO_PWR_32_DBL;\r\n\r\n/**\r\n * @type {number}\r\n * @const\r\n * @inner\r\n */\r\nvar TWO_PWR_63_DBL = TWO_PWR_64_DBL / 2;\r\n\r\n/**\r\n * @type {!Long}\r\n * @const\r\n * @inner\r\n */\r\nvar TWO_PWR_24 = fromInt(TWO_PWR_24_DBL);\r\n\r\n/**\r\n * @type {!Long}\r\n * @inner\r\n */\r\nvar ZERO = fromInt(0);\r\n\r\n/**\r\n * Signed zero.\r\n * @type {!Long}\r\n */\r\nLong.ZERO = ZERO;\r\n\r\n/**\r\n * @type {!Long}\r\n * @inner\r\n */\r\nvar UZERO = fromInt(0, true);\r\n\r\n/**\r\n * Unsigned zero.\r\n * @type {!Long}\r\n */\r\nLong.UZERO = UZERO;\r\n\r\n/**\r\n * @type {!Long}\r\n * @inner\r\n */\r\nvar ONE = fromInt(1);\r\n\r\n/**\r\n * Signed one.\r\n * @type {!Long}\r\n */\r\nLong.ONE = ONE;\r\n\r\n/**\r\n * @type {!Long}\r\n * @inner\r\n */\r\nvar UONE = fromInt(1, true);\r\n\r\n/**\r\n * Unsigned one.\r\n * @type {!Long}\r\n */\r\nLong.UONE = UONE;\r\n\r\n/**\r\n * @type {!Long}\r\n * @inner\r\n */\r\nvar NEG_ONE = fromInt(-1);\r\n\r\n/**\r\n * Signed negative one.\r\n * @type {!Long}\r\n */\r\nLong.NEG_ONE = NEG_ONE;\r\n\r\n/**\r\n * @type {!Long}\r\n * @inner\r\n */\r\nvar MAX_VALUE = fromBits(0xFFFFFFFF|0, 0x7FFFFFFF|0, false);\r\n\r\n/**\r\n * Maximum signed value.\r\n * @type {!Long}\r\n */\r\nLong.MAX_VALUE = MAX_VALUE;\r\n\r\n/**\r\n * @type {!Long}\r\n * @inner\r\n */\r\nvar MAX_UNSIGNED_VALUE = fromBits(0xFFFFFFFF|0, 0xFFFFFFFF|0, true);\r\n\r\n/**\r\n * Maximum unsigned value.\r\n * @type {!Long}\r\n */\r\nLong.MAX_UNSIGNED_VALUE = MAX_UNSIGNED_VALUE;\r\n\r\n/**\r\n * @type {!Long}\r\n * @inner\r\n */\r\nvar MIN_VALUE = fromBits(0, 0x80000000|0, false);\r\n\r\n/**\r\n * Minimum signed value.\r\n * @type {!Long}\r\n */\r\nLong.MIN_VALUE = MIN_VALUE;\r\n\r\n/**\r\n * @alias Long.prototype\r\n * @inner\r\n */\r\nvar LongPrototype = Long.prototype;\r\n\r\n/**\r\n * Converts the Long to a 32 bit integer, assuming it is a 32 bit integer.\r\n * @returns {number}\r\n */\r\nLongPrototype.toInt = function toInt() {\r\n    return this.unsigned ? this.low >>> 0 : this.low;\r\n};\r\n\r\n/**\r\n * Converts the Long to a the nearest floating-point representation of this value (double, 53 bit mantissa).\r\n * @returns {number}\r\n */\r\nLongPrototype.toNumber = function toNumber() {\r\n    if (this.unsigned)\r\n        return ((this.high >>> 0) * TWO_PWR_32_DBL) + (this.low >>> 0);\r\n    return this.high * TWO_PWR_32_DBL + (this.low >>> 0);\r\n};\r\n\r\n/**\r\n * Converts the Long to a string written in the specified radix.\r\n * @param {number=} radix Radix (2-36), defaults to 10\r\n * @returns {string}\r\n * @override\r\n * @throws {RangeError} If `radix` is out of range\r\n */\r\nLongPrototype.toString = function toString(radix) {\r\n    radix = radix || 10;\r\n    if (radix < 2 || 36 < radix)\r\n        throw RangeError('radix');\r\n    if (this.isZero())\r\n        return '0';\r\n    if (this.isNegative()) { // Unsigned Longs are never negative\r\n        if (this.eq(MIN_VALUE)) {\r\n            // We need to change the Long value before it can be negated, so we remove\r\n            // the bottom-most digit in this base and then recurse to do the rest.\r\n            var radixLong = fromNumber(radix),\r\n                div = this.div(radixLong),\r\n                rem1 = div.mul(radixLong).sub(this);\r\n            return div.toString(radix) + rem1.toInt().toString(radix);\r\n        } else\r\n            return '-' + this.neg().toString(radix);\r\n    }\r\n\r\n    // Do several (6) digits each time through the loop, so as to\r\n    // minimize the calls to the very expensive emulated div.\r\n    var radixToPower = fromNumber(pow_dbl(radix, 6), this.unsigned),\r\n        rem = this;\r\n    var result = '';\r\n    while (true) {\r\n        var remDiv = rem.div(radixToPower),\r\n            intval = rem.sub(remDiv.mul(radixToPower)).toInt() >>> 0,\r\n            digits = intval.toString(radix);\r\n        rem = remDiv;\r\n        if (rem.isZero())\r\n            return digits + result;\r\n        else {\r\n            while (digits.length < 6)\r\n                digits = '0' + digits;\r\n            result = '' + digits + result;\r\n        }\r\n    }\r\n};\r\n\r\n/**\r\n * Gets the high 32 bits as a signed integer.\r\n * @returns {number} Signed high bits\r\n */\r\nLongPrototype.getHighBits = function getHighBits() {\r\n    return this.high;\r\n};\r\n\r\n/**\r\n * Gets the high 32 bits as an unsigned integer.\r\n * @returns {number} Unsigned high bits\r\n */\r\nLongPrototype.getHighBitsUnsigned = function getHighBitsUnsigned() {\r\n    return this.high >>> 0;\r\n};\r\n\r\n/**\r\n * Gets the low 32 bits as a signed integer.\r\n * @returns {number} Signed low bits\r\n */\r\nLongPrototype.getLowBits = function getLowBits() {\r\n    return this.low;\r\n};\r\n\r\n/**\r\n * Gets the low 32 bits as an unsigned integer.\r\n * @returns {number} Unsigned low bits\r\n */\r\nLongPrototype.getLowBitsUnsigned = function getLowBitsUnsigned() {\r\n    return this.low >>> 0;\r\n};\r\n\r\n/**\r\n * Gets the number of bits needed to represent the absolute value of this Long.\r\n * @returns {number}\r\n */\r\nLongPrototype.getNumBitsAbs = function getNumBitsAbs() {\r\n    if (this.isNegative()) // Unsigned Longs are never negative\r\n        return this.eq(MIN_VALUE) ? 64 : this.neg().getNumBitsAbs();\r\n    var val = this.high != 0 ? this.high : this.low;\r\n    for (var bit = 31; bit > 0; bit--)\r\n        if ((val & (1 << bit)) != 0)\r\n            break;\r\n    return this.high != 0 ? bit + 33 : bit + 1;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value equals zero.\r\n * @returns {boolean}\r\n */\r\nLongPrototype.isZero = function isZero() {\r\n    return this.high === 0 && this.low === 0;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value equals zero. This is an alias of {@link Long#isZero}.\r\n * @returns {boolean}\r\n */\r\nLongPrototype.eqz = LongPrototype.isZero;\r\n\r\n/**\r\n * Tests if this Long's value is negative.\r\n * @returns {boolean}\r\n */\r\nLongPrototype.isNegative = function isNegative() {\r\n    return !this.unsigned && this.high < 0;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value is positive.\r\n * @returns {boolean}\r\n */\r\nLongPrototype.isPositive = function isPositive() {\r\n    return this.unsigned || this.high >= 0;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value is odd.\r\n * @returns {boolean}\r\n */\r\nLongPrototype.isOdd = function isOdd() {\r\n    return (this.low & 1) === 1;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value is even.\r\n * @returns {boolean}\r\n */\r\nLongPrototype.isEven = function isEven() {\r\n    return (this.low & 1) === 0;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value equals the specified's.\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.equals = function equals(other) {\r\n    if (!isLong(other))\r\n        other = fromValue(other);\r\n    if (this.unsigned !== other.unsigned && (this.high >>> 31) === 1 && (other.high >>> 31) === 1)\r\n        return false;\r\n    return this.high === other.high && this.low === other.low;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value equals the specified's. This is an alias of {@link Long#equals}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.eq = LongPrototype.equals;\r\n\r\n/**\r\n * Tests if this Long's value differs from the specified's.\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.notEquals = function notEquals(other) {\r\n    return !this.eq(/* validates */ other);\r\n};\r\n\r\n/**\r\n * Tests if this Long's value differs from the specified's. This is an alias of {@link Long#notEquals}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.neq = LongPrototype.notEquals;\r\n\r\n/**\r\n * Tests if this Long's value differs from the specified's. This is an alias of {@link Long#notEquals}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.ne = LongPrototype.notEquals;\r\n\r\n/**\r\n * Tests if this Long's value is less than the specified's.\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.lessThan = function lessThan(other) {\r\n    return this.comp(/* validates */ other) < 0;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value is less than the specified's. This is an alias of {@link Long#lessThan}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.lt = LongPrototype.lessThan;\r\n\r\n/**\r\n * Tests if this Long's value is less than or equal the specified's.\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.lessThanOrEqual = function lessThanOrEqual(other) {\r\n    return this.comp(/* validates */ other) <= 0;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value is less than or equal the specified's. This is an alias of {@link Long#lessThanOrEqual}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.lte = LongPrototype.lessThanOrEqual;\r\n\r\n/**\r\n * Tests if this Long's value is less than or equal the specified's. This is an alias of {@link Long#lessThanOrEqual}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.le = LongPrototype.lessThanOrEqual;\r\n\r\n/**\r\n * Tests if this Long's value is greater than the specified's.\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.greaterThan = function greaterThan(other) {\r\n    return this.comp(/* validates */ other) > 0;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value is greater than the specified's. This is an alias of {@link Long#greaterThan}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.gt = LongPrototype.greaterThan;\r\n\r\n/**\r\n * Tests if this Long's value is greater than or equal the specified's.\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.greaterThanOrEqual = function greaterThanOrEqual(other) {\r\n    return this.comp(/* validates */ other) >= 0;\r\n};\r\n\r\n/**\r\n * Tests if this Long's value is greater than or equal the specified's. This is an alias of {@link Long#greaterThanOrEqual}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.gte = LongPrototype.greaterThanOrEqual;\r\n\r\n/**\r\n * Tests if this Long's value is greater than or equal the specified's. This is an alias of {@link Long#greaterThanOrEqual}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {boolean}\r\n */\r\nLongPrototype.ge = LongPrototype.greaterThanOrEqual;\r\n\r\n/**\r\n * Compares this Long's value with the specified's.\r\n * @param {!Long|number|string} other Other value\r\n * @returns {number} 0 if they are the same, 1 if the this is greater and -1\r\n *  if the given one is greater\r\n */\r\nLongPrototype.compare = function compare(other) {\r\n    if (!isLong(other))\r\n        other = fromValue(other);\r\n    if (this.eq(other))\r\n        return 0;\r\n    var thisNeg = this.isNegative(),\r\n        otherNeg = other.isNegative();\r\n    if (thisNeg && !otherNeg)\r\n        return -1;\r\n    if (!thisNeg && otherNeg)\r\n        return 1;\r\n    // At this point the sign bits are the same\r\n    if (!this.unsigned)\r\n        return this.sub(other).isNegative() ? -1 : 1;\r\n    // Both are positive if at least one is unsigned\r\n    return (other.high >>> 0) > (this.high >>> 0) || (other.high === this.high && (other.low >>> 0) > (this.low >>> 0)) ? -1 : 1;\r\n};\r\n\r\n/**\r\n * Compares this Long's value with the specified's. This is an alias of {@link Long#compare}.\r\n * @function\r\n * @param {!Long|number|string} other Other value\r\n * @returns {number} 0 if they are the same, 1 if the this is greater and -1\r\n *  if the given one is greater\r\n */\r\nLongPrototype.comp = LongPrototype.compare;\r\n\r\n/**\r\n * Negates this Long's value.\r\n * @returns {!Long} Negated Long\r\n */\r\nLongPrototype.negate = function negate() {\r\n    if (!this.unsigned && this.eq(MIN_VALUE))\r\n        return MIN_VALUE;\r\n    return this.not().add(ONE);\r\n};\r\n\r\n/**\r\n * Negates this Long's value. This is an alias of {@link Long#negate}.\r\n * @function\r\n * @returns {!Long} Negated Long\r\n */\r\nLongPrototype.neg = LongPrototype.negate;\r\n\r\n/**\r\n * Returns the sum of this and the specified Long.\r\n * @param {!Long|number|string} addend Addend\r\n * @returns {!Long} Sum\r\n */\r\nLongPrototype.add = function add(addend) {\r\n    if (!isLong(addend))\r\n        addend = fromValue(addend);\r\n\r\n    // Divide each number into 4 chunks of 16 bits, and then sum the chunks.\r\n\r\n    var a48 = this.high >>> 16;\r\n    var a32 = this.high & 0xFFFF;\r\n    var a16 = this.low >>> 16;\r\n    var a00 = this.low & 0xFFFF;\r\n\r\n    var b48 = addend.high >>> 16;\r\n    var b32 = addend.high & 0xFFFF;\r\n    var b16 = addend.low >>> 16;\r\n    var b00 = addend.low & 0xFFFF;\r\n\r\n    var c48 = 0, c32 = 0, c16 = 0, c00 = 0;\r\n    c00 += a00 + b00;\r\n    c16 += c00 >>> 16;\r\n    c00 &= 0xFFFF;\r\n    c16 += a16 + b16;\r\n    c32 += c16 >>> 16;\r\n    c16 &= 0xFFFF;\r\n    c32 += a32 + b32;\r\n    c48 += c32 >>> 16;\r\n    c32 &= 0xFFFF;\r\n    c48 += a48 + b48;\r\n    c48 &= 0xFFFF;\r\n    return fromBits((c16 << 16) | c00, (c48 << 16) | c32, this.unsigned);\r\n};\r\n\r\n/**\r\n * Returns the difference of this and the specified Long.\r\n * @param {!Long|number|string} subtrahend Subtrahend\r\n * @returns {!Long} Difference\r\n */\r\nLongPrototype.subtract = function subtract(subtrahend) {\r\n    if (!isLong(subtrahend))\r\n        subtrahend = fromValue(subtrahend);\r\n    return this.add(subtrahend.neg());\r\n};\r\n\r\n/**\r\n * Returns the difference of this and the specified Long. This is an alias of {@link Long#subtract}.\r\n * @function\r\n * @param {!Long|number|string} subtrahend Subtrahend\r\n * @returns {!Long} Difference\r\n */\r\nLongPrototype.sub = LongPrototype.subtract;\r\n\r\n/**\r\n * Returns the product of this and the specified Long.\r\n * @param {!Long|number|string} multiplier Multiplier\r\n * @returns {!Long} Product\r\n */\r\nLongPrototype.multiply = function multiply(multiplier) {\r\n    if (this.isZero())\r\n        return ZERO;\r\n    if (!isLong(multiplier))\r\n        multiplier = fromValue(multiplier);\r\n\r\n    // use wasm support if present\r\n    if (wasm) {\r\n        var low = wasm.mul(this.low,\r\n                           this.high,\r\n                           multiplier.low,\r\n                           multiplier.high);\r\n        return fromBits(low, wasm.get_high(), this.unsigned);\r\n    }\r\n\r\n    if (multiplier.isZero())\r\n        return ZERO;\r\n    if (this.eq(MIN_VALUE))\r\n        return multiplier.isOdd() ? MIN_VALUE : ZERO;\r\n    if (multiplier.eq(MIN_VALUE))\r\n        return this.isOdd() ? MIN_VALUE : ZERO;\r\n\r\n    if (this.isNegative()) {\r\n        if (multiplier.isNegative())\r\n            return this.neg().mul(multiplier.neg());\r\n        else\r\n            return this.neg().mul(multiplier).neg();\r\n    } else if (multiplier.isNegative())\r\n        return this.mul(multiplier.neg()).neg();\r\n\r\n    // If both longs are small, use float multiplication\r\n    if (this.lt(TWO_PWR_24) && multiplier.lt(TWO_PWR_24))\r\n        return fromNumber(this.toNumber() * multiplier.toNumber(), this.unsigned);\r\n\r\n    // Divide each long into 4 chunks of 16 bits, and then add up 4x4 products.\r\n    // We can skip products that would overflow.\r\n\r\n    var a48 = this.high >>> 16;\r\n    var a32 = this.high & 0xFFFF;\r\n    var a16 = this.low >>> 16;\r\n    var a00 = this.low & 0xFFFF;\r\n\r\n    var b48 = multiplier.high >>> 16;\r\n    var b32 = multiplier.high & 0xFFFF;\r\n    var b16 = multiplier.low >>> 16;\r\n    var b00 = multiplier.low & 0xFFFF;\r\n\r\n    var c48 = 0, c32 = 0, c16 = 0, c00 = 0;\r\n    c00 += a00 * b00;\r\n    c16 += c00 >>> 16;\r\n    c00 &= 0xFFFF;\r\n    c16 += a16 * b00;\r\n    c32 += c16 >>> 16;\r\n    c16 &= 0xFFFF;\r\n    c16 += a00 * b16;\r\n    c32 += c16 >>> 16;\r\n    c16 &= 0xFFFF;\r\n    c32 += a32 * b00;\r\n    c48 += c32 >>> 16;\r\n    c32 &= 0xFFFF;\r\n    c32 += a16 * b16;\r\n    c48 += c32 >>> 16;\r\n    c32 &= 0xFFFF;\r\n    c32 += a00 * b32;\r\n    c48 += c32 >>> 16;\r\n    c32 &= 0xFFFF;\r\n    c48 += a48 * b00 + a32 * b16 + a16 * b32 + a00 * b48;\r\n    c48 &= 0xFFFF;\r\n    return fromBits((c16 << 16) | c00, (c48 << 16) | c32, this.unsigned);\r\n};\r\n\r\n/**\r\n * Returns the product of this and the specified Long. This is an alias of {@link Long#multiply}.\r\n * @function\r\n * @param {!Long|number|string} multiplier Multiplier\r\n * @returns {!Long} Product\r\n */\r\nLongPrototype.mul = LongPrototype.multiply;\r\n\r\n/**\r\n * Returns this Long divided by the specified. The result is signed if this Long is signed or\r\n *  unsigned if this Long is unsigned.\r\n * @param {!Long|number|string} divisor Divisor\r\n * @returns {!Long} Quotient\r\n */\r\nLongPrototype.divide = function divide(divisor) {\r\n    if (!isLong(divisor))\r\n        divisor = fromValue(divisor);\r\n    if (divisor.isZero())\r\n        throw Error('division by zero');\r\n\r\n    // use wasm support if present\r\n    if (wasm) {\r\n        // guard against signed division overflow: the largest\r\n        // negative number / -1 would be 1 larger than the largest\r\n        // positive number, due to two's complement.\r\n        if (!this.unsigned &&\r\n            this.high === -0x80000000 &&\r\n            divisor.low === -1 && divisor.high === -1) {\r\n            // be consistent with non-wasm code path\r\n            return this;\r\n        }\r\n        var low = (this.unsigned ? wasm.div_u : wasm.div_s)(\r\n            this.low,\r\n            this.high,\r\n            divisor.low,\r\n            divisor.high\r\n        );\r\n        return fromBits(low, wasm.get_high(), this.unsigned);\r\n    }\r\n\r\n    if (this.isZero())\r\n        return this.unsigned ? UZERO : ZERO;\r\n    var approx, rem, res;\r\n    if (!this.unsigned) {\r\n        // This section is only relevant for signed longs and is derived from the\r\n        // closure library as a whole.\r\n        if (this.eq(MIN_VALUE)) {\r\n            if (divisor.eq(ONE) || divisor.eq(NEG_ONE))\r\n                return MIN_VALUE;  // recall that -MIN_VALUE == MIN_VALUE\r\n            else if (divisor.eq(MIN_VALUE))\r\n                return ONE;\r\n            else {\r\n                // At this point, we have |other| >= 2, so |this/other| < |MIN_VALUE|.\r\n                var halfThis = this.shr(1);\r\n                approx = halfThis.div(divisor).shl(1);\r\n                if (approx.eq(ZERO)) {\r\n                    return divisor.isNegative() ? ONE : NEG_ONE;\r\n                } else {\r\n                    rem = this.sub(divisor.mul(approx));\r\n                    res = approx.add(rem.div(divisor));\r\n                    return res;\r\n                }\r\n            }\r\n        } else if (divisor.eq(MIN_VALUE))\r\n            return this.unsigned ? UZERO : ZERO;\r\n        if (this.isNegative()) {\r\n            if (divisor.isNegative())\r\n                return this.neg().div(divisor.neg());\r\n            return this.neg().div(divisor).neg();\r\n        } else if (divisor.isNegative())\r\n            return this.div(divisor.neg()).neg();\r\n        res = ZERO;\r\n    } else {\r\n        // The algorithm below has not been made for unsigned longs. It's therefore\r\n        // required to take special care of the MSB prior to running it.\r\n        if (!divisor.unsigned)\r\n            divisor = divisor.toUnsigned();\r\n        if (divisor.gt(this))\r\n            return UZERO;\r\n        if (divisor.gt(this.shru(1))) // 15 >>> 1 = 7 ; with divisor = 8 ; true\r\n            return UONE;\r\n        res = UZERO;\r\n    }\r\n\r\n    // Repeat the following until the remainder is less than other:  find a\r\n    // floating-point that approximates remainder / other *from below*, add this\r\n    // into the result, and subtract it from the remainder.  It is critical that\r\n    // the approximate value is less than or equal to the real value so that the\r\n    // remainder never becomes negative.\r\n    rem = this;\r\n    while (rem.gte(divisor)) {\r\n        // Approximate the result of division. This may be a little greater or\r\n        // smaller than the actual value.\r\n        approx = Math.max(1, Math.floor(rem.toNumber() / divisor.toNumber()));\r\n\r\n        // We will tweak the approximate result by changing it in the 48-th digit or\r\n        // the smallest non-fractional digit, whichever is larger.\r\n        var log2 = Math.ceil(Math.log(approx) / Math.LN2),\r\n            delta = (log2 <= 48) ? 1 : pow_dbl(2, log2 - 48),\r\n\r\n        // Decrease the approximation until it is smaller than the remainder.  Note\r\n        // that if it is too large, the product overflows and is negative.\r\n            approxRes = fromNumber(approx),\r\n            approxRem = approxRes.mul(divisor);\r\n        while (approxRem.isNegative() || approxRem.gt(rem)) {\r\n            approx -= delta;\r\n            approxRes = fromNumber(approx, this.unsigned);\r\n            approxRem = approxRes.mul(divisor);\r\n        }\r\n\r\n        // We know the answer can't be zero... and actually, zero would cause\r\n        // infinite recursion since we would make no progress.\r\n        if (approxRes.isZero())\r\n            approxRes = ONE;\r\n\r\n        res = res.add(approxRes);\r\n        rem = rem.sub(approxRem);\r\n    }\r\n    return res;\r\n};\r\n\r\n/**\r\n * Returns this Long divided by the specified. This is an alias of {@link Long#divide}.\r\n * @function\r\n * @param {!Long|number|string} divisor Divisor\r\n * @returns {!Long} Quotient\r\n */\r\nLongPrototype.div = LongPrototype.divide;\r\n\r\n/**\r\n * Returns this Long modulo the specified.\r\n * @param {!Long|number|string} divisor Divisor\r\n * @returns {!Long} Remainder\r\n */\r\nLongPrototype.modulo = function modulo(divisor) {\r\n    if (!isLong(divisor))\r\n        divisor = fromValue(divisor);\r\n\r\n    // use wasm support if present\r\n    if (wasm) {\r\n        var low = (this.unsigned ? wasm.rem_u : wasm.rem_s)(\r\n            this.low,\r\n            this.high,\r\n            divisor.low,\r\n            divisor.high\r\n        );\r\n        return fromBits(low, wasm.get_high(), this.unsigned);\r\n    }\r\n\r\n    return this.sub(this.div(divisor).mul(divisor));\r\n};\r\n\r\n/**\r\n * Returns this Long modulo the specified. This is an alias of {@link Long#modulo}.\r\n * @function\r\n * @param {!Long|number|string} divisor Divisor\r\n * @returns {!Long} Remainder\r\n */\r\nLongPrototype.mod = LongPrototype.modulo;\r\n\r\n/**\r\n * Returns this Long modulo the specified. This is an alias of {@link Long#modulo}.\r\n * @function\r\n * @param {!Long|number|string} divisor Divisor\r\n * @returns {!Long} Remainder\r\n */\r\nLongPrototype.rem = LongPrototype.modulo;\r\n\r\n/**\r\n * Returns the bitwise NOT of this Long.\r\n * @returns {!Long}\r\n */\r\nLongPrototype.not = function not() {\r\n    return fromBits(~this.low, ~this.high, this.unsigned);\r\n};\r\n\r\n/**\r\n * Returns the bitwise AND of this Long and the specified.\r\n * @param {!Long|number|string} other Other Long\r\n * @returns {!Long}\r\n */\r\nLongPrototype.and = function and(other) {\r\n    if (!isLong(other))\r\n        other = fromValue(other);\r\n    return fromBits(this.low & other.low, this.high & other.high, this.unsigned);\r\n};\r\n\r\n/**\r\n * Returns the bitwise OR of this Long and the specified.\r\n * @param {!Long|number|string} other Other Long\r\n * @returns {!Long}\r\n */\r\nLongPrototype.or = function or(other) {\r\n    if (!isLong(other))\r\n        other = fromValue(other);\r\n    return fromBits(this.low | other.low, this.high | other.high, this.unsigned);\r\n};\r\n\r\n/**\r\n * Returns the bitwise XOR of this Long and the given one.\r\n * @param {!Long|number|string} other Other Long\r\n * @returns {!Long}\r\n */\r\nLongPrototype.xor = function xor(other) {\r\n    if (!isLong(other))\r\n        other = fromValue(other);\r\n    return fromBits(this.low ^ other.low, this.high ^ other.high, this.unsigned);\r\n};\r\n\r\n/**\r\n * Returns this Long with bits shifted to the left by the given amount.\r\n * @param {number|!Long} numBits Number of bits\r\n * @returns {!Long} Shifted Long\r\n */\r\nLongPrototype.shiftLeft = function shiftLeft(numBits) {\r\n    if (isLong(numBits))\r\n        numBits = numBits.toInt();\r\n    if ((numBits &= 63) === 0)\r\n        return this;\r\n    else if (numBits < 32)\r\n        return fromBits(this.low << numBits, (this.high << numBits) | (this.low >>> (32 - numBits)), this.unsigned);\r\n    else\r\n        return fromBits(0, this.low << (numBits - 32), this.unsigned);\r\n};\r\n\r\n/**\r\n * Returns this Long with bits shifted to the left by the given amount. This is an alias of {@link Long#shiftLeft}.\r\n * @function\r\n * @param {number|!Long} numBits Number of bits\r\n * @returns {!Long} Shifted Long\r\n */\r\nLongPrototype.shl = LongPrototype.shiftLeft;\r\n\r\n/**\r\n * Returns this Long with bits arithmetically shifted to the right by the given amount.\r\n * @param {number|!Long} numBits Number of bits\r\n * @returns {!Long} Shifted Long\r\n */\r\nLongPrototype.shiftRight = function shiftRight(numBits) {\r\n    if (isLong(numBits))\r\n        numBits = numBits.toInt();\r\n    if ((numBits &= 63) === 0)\r\n        return this;\r\n    else if (numBits < 32)\r\n        return fromBits((this.low >>> numBits) | (this.high << (32 - numBits)), this.high >> numBits, this.unsigned);\r\n    else\r\n        return fromBits(this.high >> (numBits - 32), this.high >= 0 ? 0 : -1, this.unsigned);\r\n};\r\n\r\n/**\r\n * Returns this Long with bits arithmetically shifted to the right by the given amount. This is an alias of {@link Long#shiftRight}.\r\n * @function\r\n * @param {number|!Long} numBits Number of bits\r\n * @returns {!Long} Shifted Long\r\n */\r\nLongPrototype.shr = LongPrototype.shiftRight;\r\n\r\n/**\r\n * Returns this Long with bits logically shifted to the right by the given amount.\r\n * @param {number|!Long} numBits Number of bits\r\n * @returns {!Long} Shifted Long\r\n */\r\nLongPrototype.shiftRightUnsigned = function shiftRightUnsigned(numBits) {\r\n    if (isLong(numBits))\r\n        numBits = numBits.toInt();\r\n    numBits &= 63;\r\n    if (numBits === 0)\r\n        return this;\r\n    else {\r\n        var high = this.high;\r\n        if (numBits < 32) {\r\n            var low = this.low;\r\n            return fromBits((low >>> numBits) | (high << (32 - numBits)), high >>> numBits, this.unsigned);\r\n        } else if (numBits === 32)\r\n            return fromBits(high, 0, this.unsigned);\r\n        else\r\n            return fromBits(high >>> (numBits - 32), 0, this.unsigned);\r\n    }\r\n};\r\n\r\n/**\r\n * Returns this Long with bits logically shifted to the right by the given amount. This is an alias of {@link Long#shiftRightUnsigned}.\r\n * @function\r\n * @param {number|!Long} numBits Number of bits\r\n * @returns {!Long} Shifted Long\r\n */\r\nLongPrototype.shru = LongPrototype.shiftRightUnsigned;\r\n\r\n/**\r\n * Returns this Long with bits logically shifted to the right by the given amount. This is an alias of {@link Long#shiftRightUnsigned}.\r\n * @function\r\n * @param {number|!Long} numBits Number of bits\r\n * @returns {!Long} Shifted Long\r\n */\r\nLongPrototype.shr_u = LongPrototype.shiftRightUnsigned;\r\n\r\n/**\r\n * Converts this Long to signed.\r\n * @returns {!Long} Signed long\r\n */\r\nLongPrototype.toSigned = function toSigned() {\r\n    if (!this.unsigned)\r\n        return this;\r\n    return fromBits(this.low, this.high, false);\r\n};\r\n\r\n/**\r\n * Converts this Long to unsigned.\r\n * @returns {!Long} Unsigned long\r\n */\r\nLongPrototype.toUnsigned = function toUnsigned() {\r\n    if (this.unsigned)\r\n        return this;\r\n    return fromBits(this.low, this.high, true);\r\n};\r\n\r\n/**\r\n * Converts this Long to its byte representation.\r\n * @param {boolean=} le Whether little or big endian, defaults to big endian\r\n * @returns {!Array.<number>} Byte representation\r\n */\r\nLongPrototype.toBytes = function toBytes(le) {\r\n    return le ? this.toBytesLE() : this.toBytesBE();\r\n};\r\n\r\n/**\r\n * Converts this Long to its little endian byte representation.\r\n * @returns {!Array.<number>} Little endian byte representation\r\n */\r\nLongPrototype.toBytesLE = function toBytesLE() {\r\n    var hi = this.high,\r\n        lo = this.low;\r\n    return [\r\n        lo        & 0xff,\r\n        lo >>>  8 & 0xff,\r\n        lo >>> 16 & 0xff,\r\n        lo >>> 24       ,\r\n        hi        & 0xff,\r\n        hi >>>  8 & 0xff,\r\n        hi >>> 16 & 0xff,\r\n        hi >>> 24\r\n    ];\r\n};\r\n\r\n/**\r\n * Converts this Long to its big endian byte representation.\r\n * @returns {!Array.<number>} Big endian byte representation\r\n */\r\nLongPrototype.toBytesBE = function toBytesBE() {\r\n    var hi = this.high,\r\n        lo = this.low;\r\n    return [\r\n        hi >>> 24       ,\r\n        hi >>> 16 & 0xff,\r\n        hi >>>  8 & 0xff,\r\n        hi        & 0xff,\r\n        lo >>> 24       ,\r\n        lo >>> 16 & 0xff,\r\n        lo >>>  8 & 0xff,\r\n        lo        & 0xff\r\n    ];\r\n};\r\n\r\n/**\r\n * Creates a Long from its byte representation.\r\n * @param {!Array.<number>} bytes Byte representation\r\n * @param {boolean=} unsigned Whether unsigned or not, defaults to signed\r\n * @param {boolean=} le Whether little or big endian, defaults to big endian\r\n * @returns {Long} The corresponding Long value\r\n */\r\nLong.fromBytes = function fromBytes(bytes, unsigned, le) {\r\n    return le ? Long.fromBytesLE(bytes, unsigned) : Long.fromBytesBE(bytes, unsigned);\r\n};\r\n\r\n/**\r\n * Creates a Long from its little endian byte representation.\r\n * @param {!Array.<number>} bytes Little endian byte representation\r\n * @param {boolean=} unsigned Whether unsigned or not, defaults to signed\r\n * @returns {Long} The corresponding Long value\r\n */\r\nLong.fromBytesLE = function fromBytesLE(bytes, unsigned) {\r\n    return new Long(\r\n        bytes[0]       |\r\n        bytes[1] <<  8 |\r\n        bytes[2] << 16 |\r\n        bytes[3] << 24,\r\n        bytes[4]       |\r\n        bytes[5] <<  8 |\r\n        bytes[6] << 16 |\r\n        bytes[7] << 24,\r\n        unsigned\r\n    );\r\n};\r\n\r\n/**\r\n * Creates a Long from its big endian byte representation.\r\n * @param {!Array.<number>} bytes Big endian byte representation\r\n * @param {boolean=} unsigned Whether unsigned or not, defaults to signed\r\n * @returns {Long} The corresponding Long value\r\n */\r\nLong.fromBytesBE = function fromBytesBE(bytes, unsigned) {\r\n    return new Long(\r\n        bytes[4] << 24 |\r\n        bytes[5] << 16 |\r\n        bytes[6] <<  8 |\r\n        bytes[7],\r\n        bytes[0] << 24 |\r\n        bytes[1] << 16 |\r\n        bytes[2] <<  8 |\r\n        bytes[3],\r\n        unsigned\r\n    );\r\n};\r\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// Workaround for allowing cjs module to be included in bundle created by\n// rollup.\nimport * as LongExports from 'long';\n// tslint:disable-next-line\nconst Long: LongExports.LongConstructor =\n    // tslint:disable-next-line\n    (LongExports as any).default || LongExports;\n\nexport function hexToLong(hex: string): Long {\n  return Long.fromString(hex, true, 16);\n}\n\n// Some primes between 2^63 and 2^64 for various uses.\n// Hex 0xc3a5c85c97cb3127\nconst k0: Long = hexToLong('c3a5c85c97cb3127');\n// Hex 0xb492b66fbe98f273\nconst k1: Long = hexToLong('b492b66fbe98f273');\n// Hex 0x9ae16a3b2f90404f\nconst k2: Long = hexToLong('9ae16a3b2f90404f');\n\nfunction shiftMix(val: Long): Long {\n  return val.xor(val.shru(47));\n}\n\nfunction fetch(s: Uint8Array, offset: number, numBytes: number): Long {\n  const bytes = s.slice(offset, offset + numBytes);\n  return Long.fromBytes(Array.from(bytes), true, true);\n}\n\nfunction fetch64(s: Uint8Array, offset: number): Long {\n  return fetch(s, offset, 8);\n}\n\nfunction fetch32(s: Uint8Array, offset: number): Long {\n  return fetch(s, offset, 4);\n}\n\nfunction rotate64(val: Long, shift: number): Long {\n  // Avoid shifting by 64: doing so yields an undefined result.\n  return shift === 0 ? val : val.shru(shift).or(val.shl(64 - shift));\n}\n\nfunction hashLen16(u: Long, v: Long, mul = hexToLong('9ddfea08eb382d69')) {\n  // Murmur-inspired hashing.\n  let a = u.xor(v).mul(mul);\n  a = a.xor(a.shru(47));\n  let b = v.xor(a).mul(mul);\n  b = b.xor(b.shru(47));\n  b = b.mul(mul);\n  return b;\n}\n\n// Return a 16-byte hash for 48 bytes.  Quick and dirty.\n// Callers do best to use \"random-looking\" values for a and b.\nfunction weakHashLen32WithSeeds(\n    w: Long, x: Long, y: Long, z: Long, a: Long, b: Long) {\n  a = a.add(w);\n  b = rotate64(b.add(a).add(z), 21);\n  const c = a;\n  a = a.add(x);\n  a = a.add(y);\n  b = b.add(rotate64(a, 44));\n  return [a.add(z), b.add(c)];\n}\n\nfunction weakHashLen32WithSeedsStr(\n    s: Uint8Array, offset: number, a: Long, b: Long) {\n  return weakHashLen32WithSeeds(\n      fetch64(s, offset), fetch64(s, offset + 8), fetch64(s, offset + 16),\n      fetch64(s, offset + 24), a, b);\n}\n\nfunction hashLen0to16(s: Uint8Array, len = s.length): Long {\n  if (len >= 8) {\n    const mul = k2.add(len * 2);\n    const a = fetch64(s, 0).add(k2);\n    const b = fetch64(s, len - 8);\n    const c = rotate64(b, 37).mul(mul).add(a);\n    const d = rotate64(a, 25).add(b).mul(mul);\n    return hashLen16(c, d, mul);\n  }\n  if (len >= 4) {\n    const mul = k2.add(len * 2);\n    const a = fetch32(s, 0);\n    return hashLen16(a.shl(3).add(len), fetch32(s, len - 4), mul);\n  }\n  if (len > 0) {\n    const a = s[0];\n    const b = s[len >> 1];\n    const c = s[len - 1];\n    const y = a + (b << 8);\n    const z = len + (c << 2);\n    return shiftMix(k2.mul(y).xor(k0.mul(z))).mul(k2);\n  }\n  return k2;\n}\n\nfunction hashLen17to32(s: Uint8Array, len = s.length): Long {\n  const mul = k2.add(len * 2);\n  const a = fetch64(s, 0).mul(k1);\n  const b = fetch64(s, 8);\n  const c = fetch64(s, len - 8).mul(mul);\n  const d = fetch64(s, len - 16).mul(k2);\n  return hashLen16(\n      rotate64(a.add(b), 43).add(rotate64(c, 30)).add(d),\n      a.add(rotate64(b.add(k2), 18)).add(c), mul);\n}\n\nfunction hashLen33to64(s: Uint8Array, len = s.length): Long {\n  const mul = k2.add(len * 2);\n  const a = fetch64(s, 0).mul(k2);\n  const b = fetch64(s, 8);\n  const c = fetch64(s, len - 8).mul(mul);\n  const d = fetch64(s, len - 16).mul(k2);\n  const y = rotate64(a.add(b), 43).add(rotate64(c, 30)).add(d);\n  const z = hashLen16(y, a.add(rotate64(b.add(k2), 18)).add(c), mul);\n  const e = fetch64(s, 16).mul(mul);\n  const f = fetch64(s, 24);\n  const g = y.add(fetch64(s, len - 32)).mul(mul);\n  const h = z.add(fetch64(s, len - 24)).mul(mul);\n  return hashLen16(\n      rotate64(e.add(f), 43).add(rotate64(g, 30)).add(h),\n      e.add(rotate64(f.add(a), 18)).add(g), mul);\n}\n\nexport function fingerPrint64(s: Uint8Array, len = s.length): Long {\n  const seed: Long = Long.fromNumber(81, true);\n  if (len <= 32) {\n    if (len <= 16) {\n      return hashLen0to16(s, len);\n    } else {\n      return hashLen17to32(s, len);\n    }\n  } else if (len <= 64) {\n    return hashLen33to64(s, len);\n  }\n\n  // For strings over 64 bytes we loop.  Internal state consists of\n  // 56 bytes: v, w, x, y, and z.\n  let x = seed;\n  let y = seed.mul(k1).add(113);\n\n  let z = shiftMix(y.mul(k2).add(113)).mul(k2);\n  let v = [Long.UZERO, Long.UZERO];\n  let w = [Long.UZERO, Long.UZERO];\n  x = x.mul(k2).add(fetch64(s, 0));\n\n  let offset = 0;\n  // Set end so that after the loop we have 1 to 64 bytes left to process.\n  const end = ((len - 1) >> 6) * 64;\n  const last64 = end + ((len - 1) & 63) - 63;\n\n  do {\n    x = rotate64(x.add(y).add(v[0]).add(fetch64(s, offset + 8)), 37).mul(k1);\n    y = rotate64(y.add(v[1]).add(fetch64(s, offset + 48)), 42).mul(k1);\n    x = x.xor(w[1]);\n    y = y.add(v[0]).add(fetch64(s, offset + 40));\n    z = rotate64(z.add(w[0]), 33).mul(k1);\n    v = weakHashLen32WithSeedsStr(s, offset, v[1].mul(k1), x.add(w[0]));\n    w = weakHashLen32WithSeedsStr(\n        s, offset + 32, z.add(w[1]), y.add(fetch64(s, offset + 16)));\n\n    [z, x] = [x, z];\n    offset += 64;\n  } while (offset !== end);\n  const mul = k1.add(z.and(0xff).shl(1));\n  // Point to the last 64 bytes of input.\n  offset = last64;\n\n  w[0] = w[0].add((len - 1) & 63);\n  v[0] = v[0].add(w[0]);\n  w[0] = w[0].add(v[0]);\n\n  x = rotate64(x.add(y).add(v[0]).add(fetch64(s, offset + 8)), 37).mul(mul);\n  y = rotate64(y.add(v[1]).add(fetch64(s, offset + 48)), 42).mul(mul);\n  x = x.xor(w[1].mul(9));\n  y = y.add(v[0].mul(9).add(fetch64(s, offset + 40)));\n  z = rotate64(z.add(w[0]), 33).mul(mul);\n  v = weakHashLen32WithSeedsStr(s, offset, v[1].mul(mul), x.add(w[0]));\n  w = weakHashLen32WithSeedsStr(\n      s, offset + 32, z.add(w[1]), y.add(fetch64(s, offset + 16)));\n\n  [z, x] = [x, z];\n\n  return hashLen16(\n      hashLen16(v[0], w[0], mul).add(shiftMix(y).mul(k0)).add(z),\n      hashLen16(v[1], w[1], mul).add(x), mul);\n}\n","/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {env} from './environment';\nimport {BackendValues, DataType, TensorLike, TypedArray} from './types';\nimport * as base from './util_base';\nexport * from './util_base';\nexport * from './hash_util';\n\n/**\n * Create typed array for scalar value. Used for storing in `DataStorage`.\n */\nexport function createScalarValue(\n    value: DataType, dtype: DataType): BackendValues {\n  if (dtype === 'string') {\n    return encodeString(value);\n  }\n\n  return toTypedArray([value], dtype);\n}\n\nfunction noConversionNeeded(a: TensorLike, dtype: DataType): boolean {\n  return (a instanceof Float32Array && dtype === 'float32') ||\n      (a instanceof Int32Array && dtype === 'int32') ||\n      (a instanceof Uint8Array && dtype === 'bool');\n}\n\nexport function toTypedArray(a: TensorLike, dtype: DataType): TypedArray {\n  if (dtype === 'string') {\n    throw new Error('Cannot convert a string[] to a TypedArray');\n  }\n  if (Array.isArray(a)) {\n    a = base.flatten(a);\n  }\n\n  if (env().getBool('DEBUG')) {\n    base.checkConversionForErrors(a as number[], dtype);\n  }\n  if (noConversionNeeded(a, dtype)) {\n    return a as TypedArray;\n  }\n  if (dtype == null || dtype === 'float32' || dtype === 'complex64') {\n    return new Float32Array(a as number[]);\n  } else if (dtype === 'int32') {\n    return new Int32Array(a as number[]);\n  } else if (dtype === 'bool') {\n    const bool = new Uint8Array((a as number[]).length);\n    for (let i = 0; i < bool.length; ++i) {\n      if (Math.round((a as number[])[i]) !== 0) {\n        bool[i] = 1;\n      }\n    }\n    return bool;\n  } else {\n    throw new Error(`Unknown data type ${dtype}`);\n  }\n}\n\n/**\n * Returns the current high-resolution time in milliseconds relative to an\n * arbitrary time in the past. It works across different platforms (node.js,\n * browsers).\n *\n * ```js\n * console.log(tf.util.now());\n * ```\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function now(): number {\n  return env().platform.now();\n}\n\n/**\n * Returns a platform-specific implementation of\n * [`fetch`](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API).\n *\n * If `fetch` is defined on the global object (`window`, `process`, etc.),\n * `tf.util.fetch` returns that function.\n *\n * If not, `tf.util.fetch` returns a platform-specific solution.\n *\n * ```js\n * const resource = await tf.util.fetch('https://unpkg.com/@tensorflow/tfjs');\n * // handle response\n * ```\n *\n * @doc {heading: 'Util'}\n */\nexport function fetch(\n    path: string, requestInits?: RequestInit): Promise<Response> {\n  return env().platform.fetch(path, requestInits);\n}\n\n/**\n * Encodes the provided string into bytes using the provided encoding scheme.\n *\n * @param s The string to encode.\n * @param encoding The encoding scheme. Defaults to utf-8.\n *\n * @doc {heading: 'Util'}\n */\nexport function encodeString(s: string, encoding = 'utf-8'): Uint8Array {\n  encoding = encoding || 'utf-8';\n  return env().platform.encode(s, encoding);\n}\n\n/**\n * Decodes the provided bytes into a string using the provided encoding scheme.\n * @param bytes The bytes to decode.\n *\n * @param encoding The encoding scheme. Defaults to utf-8.\n *\n * @doc {heading: 'Util'}\n */\nexport function decodeString(bytes: Uint8Array, encoding = 'utf-8'): string {\n  encoding = encoding || 'utf-8';\n  return env().platform.decode(bytes, encoding);\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {BackendTimer, BackendTimingInfo} from './backends/backend';\nimport {env} from './environment';\nimport {Tensor} from './tensor';\nimport {NamedTensorMap} from './tensor_types';\nimport {DataType, DataTypeMap, TypedArray} from './types';\nimport * as util from './util';\n\nexport type KernelProfile = {\n  kernelName: string,\n  outputs: Tensor[],\n  inputs: NamedTensorMap,\n  timeMs: Promise<number|{error: string}>,\n  extraInfo: Promise<string>\n};\n\nexport class Profiler {\n  constructor(private backendTimer: BackendTimer, private logger?: Logger) {\n    if (logger == null) {\n      this.logger = new Logger();\n    }\n  }\n\n  profileKernel(kernelName: string, inputs: NamedTensorMap, f: () => Tensor[]):\n      KernelProfile {\n    let outputs: Tensor[];\n    const holdResultWrapperFn = () => {\n      outputs = f();\n    };\n    let timer: Promise<BackendTimingInfo>;\n    const start = util.now();\n    if (this.backendTimer.timerAvailable()) {\n      timer = this.backendTimer.time(holdResultWrapperFn);\n    } else {\n      holdResultWrapperFn();\n      for (const output of outputs) {\n        output.dataSync();\n      }\n      timer = Promise.resolve({kernelMs: util.now() - start});\n    }\n    if (env().getBool('CHECK_COMPUTATION_FOR_ERRORS')) {\n      for (let i = 0; i < outputs.length; i++) {\n        const output = outputs[i];\n        // Dangling promise here because we don't want to propagate up\n        // asynchronicity.\n        output.data().then(tensorVals => {\n          checkComputationForErrors(tensorVals, output.dtype, kernelName);\n        });\n      }\n    }\n\n    const kernelProfile = {\n      kernelName,\n      outputs,\n      inputs,\n      timeMs: timer.then(timing => timing.kernelMs),\n      extraInfo: timer.then(\n          timing => timing.getExtraProfileInfo != null ?\n              timing.getExtraProfileInfo() :\n              '')\n    };\n    return kernelProfile;\n  }\n\n  logKernelProfile(kernelProfile: KernelProfile): void {\n    const {kernelName, outputs, timeMs, inputs, extraInfo} = kernelProfile;\n\n    outputs.forEach(result => {\n      Promise.all([result.data(), timeMs, extraInfo]).then(valueContainer => {\n        this.logger.logKernelProfile(\n            kernelName, result, valueContainer[0], valueContainer[1], inputs,\n            valueContainer[2]);\n      });\n    });\n  }\n}\n\nexport function checkComputationForErrors<D extends DataType>(\n    vals: DataTypeMap[D], dtype: D, kernelName: string): boolean {\n  if (dtype !== 'float32') {\n    // Only floating point computations will generate NaN values\n    return false;\n  }\n  for (let i = 0; i < vals.length; i++) {\n    const num = vals[i] as number;\n    if (isNaN(num) || !isFinite(num)) {\n      // Throwing custom exception so behavior is testable.\n      console.warn(`Found ${num} in the result of '${kernelName}'`);\n      return true;\n    }\n  }\n  return false;\n}\n\nexport class Logger {\n  logKernelProfile(\n      name: string, result: Tensor, vals: TypedArray,\n      timeMs: number|{error: string}, inputs: NamedTensorMap,\n      extraInfo?: string) {\n    const time = typeof timeMs === 'number' ? util.rightPad(`${timeMs}ms`, 9) :\n                                              timeMs['error'];\n    const paddedName = util.rightPad(name, 25);\n    const rank = result.rank;\n    const size = result.size;\n    const shape = util.rightPad(result.shape.toString(), 14);\n    let inputShapesDescription = '';\n\n    for (const name in inputs) {\n      const input = inputs[name];\n      if (input != null) {\n        // The input might be a non-tensor (e.g HTMLImageElement), in which case\n        // we claim the output shape as input shape.\n        const inputShape = input.shape || result.shape;\n        const inputRank = inputShape.length;\n        inputShapesDescription +=\n            `${name}: ${inputRank}D ${inputRank > 0 ? inputShape : ''} `;\n      }\n    }\n\n    console.log(\n        `%c${paddedName}\\t%c${time}\\t%c${rank}D ${shape}\\t%c${size}\\t%c${\n            inputShapesDescription}\\t%c${extraInfo}`,\n        'font-weight:bold', 'color:red', 'color:blue', 'color: orange',\n        'color: green', 'color: steelblue');\n  }\n}\n","/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Tensor} from './tensor';\nimport {NamedTensorMap} from './tensor_types';\nimport * as util from './util';\n\nexport interface TapeNode {\n  id: number;\n  kernelName: string;\n  outputs: Tensor[];\n  inputs: NamedTensorMap;\n  // Optional params, defined only for ops with gradient impl.\n  gradient?: (dys: Tensor[]) => NamedGradientMap;\n  saved?: Tensor[];\n}\n\nexport type NamedGradientMap = {\n  [inputName: string]: () => Tensor;\n};\n\n/**\n * Computes a list of TapeNodes that connect x to y, filtering everything else\n * out and preserving the order of the original tape elements.\n *\n * @param tape The tape elements to filter.\n * @param xs The input Tensors.\n * @param y The output Tensor.\n */\nexport function getFilteredNodesXToY(\n    tape: TapeNode[], xs: Tensor[], y: Tensor): TapeNode[] {\n  // Forward pass to compute all the nodes and Tensors that are transitively a\n  // function of x.\n  const tensorsFromX: {[tensorId: number]: boolean} = {};\n  const nodesFromX: {[nodeId: number]: boolean} = {};\n  for (let i = 0; i < xs.length; i++) {\n    tensorsFromX[xs[i].id] = true;\n  }\n\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n    const nodeInputs = node.inputs;\n    for (const inputName in nodeInputs) {\n      const input = nodeInputs[inputName];\n\n      let anyInputFromX = false;\n      for (let j = 0; j < xs.length; j++) {\n        if (tensorsFromX[input.id]) {\n          node.outputs.forEach(output => tensorsFromX[output.id] = true);\n          anyInputFromX = true;\n          nodesFromX[node.id] = true;\n          break;\n        }\n      }\n\n      if (anyInputFromX) {\n        break;\n      }\n    }\n  }\n\n  // Backward pass to find all of the nodes and Tensors that lead to y.\n  const tensorsLeadToY: {[tensorId: number]: boolean} = {};\n  tensorsLeadToY[y.id] = true;\n  const nodesToY: {[nodeId: number]: boolean} = {};\n\n  for (let i = tape.length - 1; i >= 0; i--) {\n    const node = tape[i];\n    const nodeInputs = node.inputs;\n\n    // If any of the outputs lead to y, mark all of the inputs as leading to y.\n    for (let j = 0; j < node.outputs.length; j++) {\n      if (tensorsLeadToY[node.outputs[j].id]) {\n        for (const inputName in nodeInputs) {\n          tensorsLeadToY[nodeInputs[inputName].id] = true;\n          nodesToY[node.id] = true;\n        }\n        break;\n      }\n    }\n  }\n\n  // Return the paths that come from x and lead to y.\n  const filteredTape: TapeNode[] = [];\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n\n    if (nodesFromX[node.id] && nodesToY[node.id]) {\n      // Prune the inputs from the node that aren't a function of x.\n      const prunedInputs: {[inputName: string]: Tensor} = {};\n      for (const inputName in node.inputs) {\n        const nodeInput = node.inputs[inputName];\n        if (tensorsFromX[nodeInput.id]) {\n          prunedInputs[inputName] = nodeInput;\n        }\n      }\n\n      // Copy the node and overwrite inputsAndArgs to the pruned version.\n      const prunedNode = Object.assign({}, node);\n      prunedNode.inputs = prunedInputs;\n      prunedNode.outputs = node.outputs;\n\n      filteredTape.push(prunedNode);\n    }\n  }\n\n  return filteredTape;\n}\n\n/**\n * Backpropagate gradients through the filtered TapeNodes.\n *\n * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map\n * is mutated by this method.\n * @param filteredTape The filtered TapeNodes to backprop through.\n */\nexport function backpropagateGradients(\n    tensorAccumulatedGradientMap: {[tensorId: number]: Tensor},\n    filteredTape: TapeNode[], tidy: (f: Function) => Tensor,\n    add: (a: Tensor, b: Tensor) => Tensor) {\n  // Walk the tape backward and keep a map of Tensor to its gradient.\n  for (let i = filteredTape.length - 1; i >= 0; i--) {\n    const node = filteredTape[i];\n\n    const dys: Tensor[] = [];\n    node.outputs.forEach(o => {\n      const gradTensor = tensorAccumulatedGradientMap[o.id];\n      if (gradTensor != null) {\n        dys.push(gradTensor);\n      } else {\n        // This particular output is not in the back-propagation subgraph, so it\n        // does not affect the final output, thus we put null for its dy.\n        dys.push(null);\n      }\n    });\n\n    if (node.gradient == null) {\n      throw new Error(\n          `Cannot compute gradient: gradient function not found ` +\n          `for ${node.kernelName}.`);\n    }\n\n    // Backprop dy through this node and accumulate gradients over the inputs.\n    const inputGradients = node.gradient(dys);\n\n    for (const inputName in node.inputs) {\n      if (!(inputName in inputGradients)) {\n        throw new Error(\n            `Cannot backprop through input ${inputName}. ` +\n            `Available gradients found: ${Object.keys(inputGradients)}.`);\n      }\n\n      // Call the gradient function.\n      const dx = tidy(() => inputGradients[inputName]());\n      if (dx.dtype !== 'float32') {\n        throw new Error(\n            `Error in gradient for op ${\n                node.kernelName}. The gradient of input ` +\n            `${inputName} must have 'float32' dtype, but has '${dx.dtype}'`);\n      }\n      const x = node.inputs[inputName];\n      if (!util.arraysEqual(dx.shape, x.shape)) {\n        throw new Error(\n            `Error in gradient for op ${\n                node.kernelName}. The gradient of input ` +\n            `'${inputName}' has shape '${dx.shape}', which does not match ` +\n            `the shape of the input '${x.shape}'`);\n      }\n\n      if (tensorAccumulatedGradientMap[x.id] == null) {\n        tensorAccumulatedGradientMap[x.id] = dx;\n      } else {\n        const curGradient = tensorAccumulatedGradientMap[x.id];\n        tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);\n        curGradient.dispose();\n      }\n    }\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType, TypedArray} from './types';\nimport {computeStrides, isString, rightPad, sizeFromShape} from './util';\n\n// Maximum number of values before we decide to show ellipsis.\nconst FORMAT_LIMIT_NUM_VALS = 20;\n// Number of first and last values to show when displaying a, b,...,y, z.\nconst FORMAT_NUM_FIRST_LAST_VALS = 3;\n// Number of significant digits to show.\nconst FORMAT_NUM_SIG_DIGITS = 7;\n\nexport function tensorToString(\n    vals: TypedArray|string[], shape: number[], dtype: DataType,\n    verbose: boolean) {\n  const strides = computeStrides(shape);\n  const padPerCol = computeMaxSizePerColumn(vals, shape, dtype, strides);\n  const rank = shape.length;\n  const valsLines = subTensorToString(vals, shape, dtype, strides, padPerCol);\n  const lines = ['Tensor'];\n  if (verbose) {\n    lines.push(`  dtype: ${dtype}`);\n    lines.push(`  rank: ${rank}`);\n    lines.push(`  shape: [${shape}]`);\n    lines.push(`  values:`);\n  }\n  lines.push(valsLines.map(l => '    ' + l).join('\\n'));\n  return lines.join('\\n');\n}\n\nfunction computeMaxSizePerColumn(\n    vals: TypedArray|string[], shape: number[], dtype: DataType,\n    strides: number[]): number[] {\n  const n = sizeFromShape(shape);\n  const numCols = strides[strides.length - 1];\n  const padPerCol = new Array(numCols).fill(0);\n  const rank = shape.length;\n  const valuesOrTuples =\n      dtype === 'complex64' ? createComplexTuples(vals) : vals;\n\n  if (rank > 1) {\n    for (let row = 0; row < n / numCols; row++) {\n      const offset = row * numCols;\n      for (let j = 0; j < numCols; j++) {\n        padPerCol[j] = Math.max(\n            padPerCol[j],\n            valToString(valuesOrTuples[offset + j], 0, dtype).length);\n      }\n    }\n  }\n  return padPerCol;\n}\n\nfunction valToString(\n    val: number|string|[number, number], pad: number, dtype: DataType) {\n  let valStr: string;\n  if (Array.isArray(val)) {\n    valStr = `${parseFloat(val[0].toFixed(FORMAT_NUM_SIG_DIGITS))} + ` +\n        `${parseFloat(val[1].toFixed(FORMAT_NUM_SIG_DIGITS))}j`;\n  } else if (isString(val)) {\n    valStr = `'${val}'`;\n  } else if (dtype === 'bool') {\n    valStr = boolNumToString(val);\n  } else {\n    valStr = parseFloat(val.toFixed(FORMAT_NUM_SIG_DIGITS)).toString();\n  }\n\n  return rightPad(valStr, pad);\n}\n\nfunction boolNumToString(v: number): string {\n  return v === 0 ? 'false' : 'true';\n}\n\nfunction subTensorToString(\n    vals: TypedArray|string[], shape: number[], dtype: DataType,\n    strides: number[], padPerCol: number[], isLast = true): string[] {\n  const storagePerElement = dtype === 'complex64' ? 2 : 1;\n\n  const size = shape[0];\n  const rank = shape.length;\n  if (rank === 0) {\n    if (dtype === 'complex64') {\n      const complexTuple = createComplexTuples(vals);\n      return [valToString(complexTuple[0], 0, dtype)];\n    }\n    if (dtype === 'bool') {\n      return [boolNumToString(vals[0] as number)];\n    }\n    return [vals[0].toString()];\n  }\n\n  if (rank === 1) {\n    if (size > FORMAT_LIMIT_NUM_VALS) {\n      const firstValsSize = FORMAT_NUM_FIRST_LAST_VALS * storagePerElement;\n\n      let firstVals = Array.from<number|string|[number, number]>(\n          vals.slice(0, firstValsSize));\n      let lastVals = Array.from<number|string|[number, number]>(vals.slice(\n          (size - FORMAT_NUM_FIRST_LAST_VALS) * storagePerElement,\n          size * storagePerElement));\n      if (dtype === 'complex64') {\n        firstVals = createComplexTuples(firstVals);\n        lastVals = createComplexTuples(lastVals);\n      }\n      return [\n        '[' +\n        firstVals.map((x, i) => valToString(x, padPerCol[i], dtype))\n            .join(', ') +\n        ', ..., ' +\n        lastVals\n            .map(\n                (x, i) => valToString(\n                    x, padPerCol[size - FORMAT_NUM_FIRST_LAST_VALS + i], dtype))\n            .join(', ') +\n        ']'\n      ];\n    }\n    const displayVals: Array<number|string|[number, number]> =\n        dtype === 'complex64' ? createComplexTuples(vals) :\n                                Array.from<number|string>(vals);\n\n    return [\n      '[' +\n      displayVals.map((x, i) => valToString(x, padPerCol[i], dtype))\n          .join(', ') +\n      ']'\n    ];\n  }\n\n  // The array is rank 2 or more.\n  const subshape = shape.slice(1);\n  const substrides = strides.slice(1);\n  const stride = strides[0] * storagePerElement;\n  const lines: string[] = [];\n  if (size > FORMAT_LIMIT_NUM_VALS) {\n    for (let i = 0; i < FORMAT_NUM_FIRST_LAST_VALS; i++) {\n      const start = i * stride;\n      const end = start + stride;\n      lines.push(...subTensorToString(\n          vals.slice(start, end), subshape, dtype, substrides, padPerCol,\n          false /* isLast */));\n    }\n    lines.push('...');\n    for (let i = size - FORMAT_NUM_FIRST_LAST_VALS; i < size; i++) {\n      const start = i * stride;\n      const end = start + stride;\n      lines.push(...subTensorToString(\n          vals.slice(start, end), subshape, dtype, substrides, padPerCol,\n          i === size - 1 /* isLast */));\n    }\n  } else {\n    for (let i = 0; i < size; i++) {\n      const start = i * stride;\n      const end = start + stride;\n      lines.push(...subTensorToString(\n          vals.slice(start, end), subshape, dtype, substrides, padPerCol,\n          i === size - 1 /* isLast */));\n    }\n  }\n  const sep = rank === 2 ? ',' : '';\n  lines[0] = '[' + lines[0] + sep;\n  for (let i = 1; i < lines.length - 1; i++) {\n    lines[i] = ' ' + lines[i] + sep;\n  }\n  let newLineSep = ',\\n';\n  for (let i = 2; i < rank; i++) {\n    newLineSep += '\\n';\n  }\n  lines[lines.length - 1] =\n      ' ' + lines[lines.length - 1] + ']' + (isLast ? '' : newLineSep);\n  return lines;\n}\n\nfunction createComplexTuples(vals: Array<{}>|\n                             TypedArray): Array<[number, number]> {\n  const complexTuples: Array<[number, number]> = [];\n  for (let i = 0; i < vals.length; i += 2) {\n    complexTuples.push([vals[i], vals[i + 1]] as [number, number]);\n  }\n  return complexTuples;\n}\n","/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// Workaround for: https://github.com/bazelbuild/rules_nodejs/issues/1265\n/// <reference types=\"@webgpu/types/dist\" />\n\nimport {getGlobal} from './global_util';\nimport {tensorToString} from './tensor_format';\nimport {ArrayMap, BackendValues, DataType, DataTypeMap, DataValues, NumericDataType, Rank, ShapeMap, SingleValueMap, TypedArray} from './types';\nimport * as util from './util';\nimport {computeStrides, toNestedArray} from './util';\n\nexport interface TensorData<D extends DataType> {\n  dataId?: DataId;\n  values?: DataTypeMap[D];\n}\n\n// This interface mimics KernelBackend (in backend.ts), which would create a\n// circular dependency if imported.\nexport interface Backend {}\n\n/**\n * A mutable object, similar to `tf.Tensor`, that allows users to set values\n * at locations before converting to an immutable `tf.Tensor`.\n *\n * See `tf.buffer` for creating a tensor buffer.\n *\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\nexport class TensorBuffer<R extends Rank, D extends DataType = 'float32'> {\n  size: number;\n  shape: ShapeMap[R];\n  strides: number[];\n  values: DataTypeMap[D];\n\n  constructor(shape: ShapeMap[R], public dtype: D, values?: DataTypeMap[D]) {\n    this.shape = shape.slice() as ShapeMap[R];\n    this.size = util.sizeFromShape(shape);\n\n    if (values != null) {\n      const n = values.length;\n      util.assert(\n          n === this.size,\n          () => `Length of values '${n}' does not match the size ` +\n              `inferred by the shape '${this.size}'.`);\n    }\n    if (dtype === 'complex64') {\n      throw new Error(\n          `complex64 dtype TensorBuffers are not supported. Please create ` +\n          `a TensorBuffer for the real and imaginary parts separately and ` +\n          `call tf.complex(real, imag).`);\n    }\n    this.values = values || util.getArrayFromDType(dtype, this.size);\n    this.strides = computeStrides(shape);\n  }\n\n  /**\n   * Sets a value in the buffer at a given location.\n   *\n   * @param value The value to set.\n   * @param locs  The location indices.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Creation'}\n   */\n  set(value: SingleValueMap[D], ...locs: number[]): void {\n    if (locs.length === 0) {\n      locs = [0];\n    }\n    util.assert(\n        locs.length === this.rank,\n        () => `The number of provided coordinates (${locs.length}) must ` +\n            `match the rank (${this.rank})`);\n\n    const index = this.locToIndex(locs);\n    this.values[index] = value as number;\n  }\n\n  /**\n   * Returns the value in the buffer at the provided location.\n   *\n   * @param locs The location indices.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Creation'}\n   */\n  get(...locs: number[]): SingleValueMap[D] {\n    if (locs.length === 0) {\n      locs = [0];\n    }\n    let i = 0;\n    for (const loc of locs) {\n      if (loc < 0 || loc >= this.shape[i]) {\n        const msg = `Requested out of range element at ${locs}. ` +\n            `  Buffer shape=${this.shape}`;\n        throw new Error(msg);\n      }\n      i++;\n    }\n    let index = locs[locs.length - 1];\n    for (let i = 0; i < locs.length - 1; ++i) {\n      index += this.strides[i] * locs[i];\n    }\n    return this.values[index] as SingleValueMap[D];\n  }\n\n  locToIndex(locs: number[]): number {\n    if (this.rank === 0) {\n      return 0;\n    } else if (this.rank === 1) {\n      return locs[0];\n    }\n    let index = locs[locs.length - 1];\n    for (let i = 0; i < locs.length - 1; ++i) {\n      index += this.strides[i] * locs[i];\n    }\n    return index;\n  }\n\n  indexToLoc(index: number): number[] {\n    if (this.rank === 0) {\n      return [];\n    } else if (this.rank === 1) {\n      return [index];\n    }\n    const locs: number[] = new Array(this.shape.length);\n    for (let i = 0; i < locs.length - 1; ++i) {\n      locs[i] = Math.floor(index / this.strides[i]);\n      index -= locs[i] * this.strides[i];\n    }\n    locs[locs.length - 1] = index;\n    return locs;\n  }\n\n  get rank() {\n    return this.shape.length;\n  }\n\n  /**\n   * Creates an immutable `tf.Tensor` object from the buffer.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Creation'}\n   */\n  toTensor(): Tensor<R> {\n    return trackerFn().makeTensor(this.values, this.shape, this.dtype) as\n        Tensor<R>;\n  }\n}\n\nexport interface DataToGPUWebGLOption {\n  customTexShape?: [number, number];\n}\n\nexport type DataToGPUOptions = DataToGPUWebGLOption;\n\nexport interface GPUData {\n  tensorRef: Tensor;\n  texture?: WebGLTexture;\n  buffer?: GPUBuffer;\n  texShape?: [number, number];\n  bufSize?: number;\n}\n\nexport interface TensorTracker {\n  makeTensor(\n      values: DataValues, shape: number[], dtype: DataType,\n      backend?: Backend): Tensor;\n  makeVariable(\n      initialValue: Tensor, trainable?: boolean, name?: string,\n      dtype?: DataType): Variable;\n  incRef(a: Tensor, backend: Backend): void;\n  disposeTensor(t: Tensor): void;\n  disposeVariable(v: Variable): void;\n  read(dataId: DataId): Promise<BackendValues>;\n  readSync(dataId: DataId): BackendValues;\n  readToGPU(dataId: DataId, options?: DataToGPUOptions): GPUData;\n}\n\n/**\n * The Tensor class calls into this handler to delegate chaining operations.\n */\nexport interface OpHandler {\n  cast<T extends Tensor>(x: T, dtype: DataType): T;\n  buffer<R extends Rank, D extends DataType>(\n      shape: ShapeMap[R], dtype: D,\n      values?: DataTypeMap[D]): TensorBuffer<R, D>;\n  print<T extends Tensor>(x: T, verbose: boolean): void;\n  clone<T extends Tensor>(x: T): T;\n  // TODO(yassogba) bring reshape back?\n}\n\n// For tracking tensor creation and disposal.\nlet trackerFn: () => TensorTracker = null;\n// Used by chaining methods to call into ops.\nlet opHandler: OpHandler = null;\n// Used to warn about deprecated methods.\nlet deprecationWarningFn: (msg: string) => void = null;\n// This here so that we can use this method on dev branches and keep the\n// functionality at master.\n// tslint:disable-next-line:no-unused-expression\n[deprecationWarningFn];\n\n/**\n * An external consumer can register itself as the tensor tracker. This way\n * the Tensor class can notify the tracker for every tensor created and\n * disposed.\n */\nexport function setTensorTracker(fn: () => TensorTracker) {\n  trackerFn = fn;\n}\n\n/**\n * An external consumer can register itself as the op handler. This way the\n * Tensor class can have chaining methods that call into ops via the op\n * handler.\n */\nexport function setOpHandler(handler: OpHandler) {\n  opHandler = handler;\n}\n\n/**\n * Sets the deprecation warning function to be used by this file. This way the\n * Tensor class can be a leaf but still use the environment.\n */\nexport function setDeprecationWarningFn(fn: (msg: string) => void) {\n  deprecationWarningFn = fn;\n}\n\n/**\n * We wrap data id since we use weak map to avoid memory leaks.\n * Since we have our own memory management, we have a reference counter\n * mapping a tensor to its data, so there is always a pointer (even if that\n * data is otherwise garbage collectable).\n * See https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/\n * Global_Objects/WeakMap\n */\nexport type DataId = object;  // object instead of {} to force non-primitive.\n\n// Declare this namespace to make Tensor class augmentation work in google3.\nexport declare namespace Tensor {}\n/**\n * A `tf.Tensor` object represents an immutable, multidimensional array of\n * numbers that has a shape and a data type.\n *\n * For performance reasons, functions that create tensors do not necessarily\n * perform a copy of the data passed to them (e.g. if the data is passed as a\n * `Float32Array`), and changes to the data will change the tensor. This is not\n * a feature and is not supported. To avoid this behavior, use the tensor before\n * changing the input data or create a copy with `copy = tf.add(yourTensor, 0)`.\n *\n * See `tf.tensor` for details on how to create a `tf.Tensor`.\n *\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\nexport class Tensor<R extends Rank = Rank> {\n  /** Unique id of this tensor. */\n  readonly id: number;\n  /**\n   * Id of the bucket holding the data for this tensor. Multiple arrays can\n   * point to the same bucket (e.g. when calling array.reshape()).\n   */\n  dataId: DataId;\n  /** The shape of the tensor. */\n  readonly shape: ShapeMap[R];\n  /** Number of elements in the tensor. */\n  readonly size: number;\n  /** The data type for the array. */\n  readonly dtype: DataType;\n  /** The rank type for the array (see `Rank` enum). */\n  readonly rankType: R;\n\n  /** Whether this tensor has been globally kept. */\n  kept = false;\n  /** The id of the scope this tensor is being tracked in. */\n  scopeId: number;\n\n  /**\n   * Number of elements to skip in each dimension when indexing. See\n   * https://docs.scipy.org/doc/numpy/reference/generated/\\\n   * numpy.ndarray.strides.html\n   */\n  readonly strides: number[];\n\n  constructor(shape: ShapeMap[R], dtype: DataType, dataId: DataId, id: number) {\n    this.shape = shape.slice() as ShapeMap[R];\n    this.dtype = dtype || 'float32';\n    this.size = util.sizeFromShape(shape);\n    this.strides = computeStrides(shape);\n    this.dataId = dataId;\n    this.id = id;\n    this.rankType = (this.rank < 5 ? this.rank.toString() : 'higher') as R;\n  }\n\n  get rank(): number {\n    return this.shape.length;\n  }\n\n  /**\n   * Returns a promise of `tf.TensorBuffer` that holds the underlying data.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  async buffer<D extends DataType = 'float32'>(): Promise<TensorBuffer<R, D>> {\n    const vals = await this.data<D>();\n    return opHandler.buffer(this.shape, this.dtype as D, vals);\n  }\n\n  /**\n   * Returns a `tf.TensorBuffer` that holds the underlying data.\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  bufferSync<D extends DataType = 'float32'>(): TensorBuffer<R, D> {\n    return opHandler.buffer(this.shape, this.dtype as D, this.dataSync());\n  }\n\n  /**\n   * Returns the tensor data as a nested array. The transfer of data is done\n   * asynchronously.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  async array(): Promise<ArrayMap[R]> {\n    const vals = await this.data();\n    return toNestedArray(this.shape, vals, this.dtype === 'complex64') as\n        ArrayMap[R];\n  }\n\n  /**\n   * Returns the tensor data as a nested array. The transfer of data is done\n   * synchronously.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  arraySync(): ArrayMap[R] {\n    return toNestedArray(\n               this.shape, this.dataSync(), this.dtype === 'complex64') as\n        ArrayMap[R];\n  }\n\n  /**\n   * Asynchronously downloads the values from the `tf.Tensor`. Returns a\n   * promise of `TypedArray` that resolves when the computation has finished.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  async data<D extends DataType = NumericDataType>(): Promise<DataTypeMap[D]> {\n    this.throwIfDisposed();\n    const data = trackerFn().read(this.dataId);\n    if (this.dtype === 'string') {\n      const bytes = await data as Uint8Array[];\n      try {\n        return bytes.map(b => util.decodeString(b)) as DataTypeMap[D];\n      } catch {\n        throw new Error(\n            'Failed to decode the string bytes into utf-8. ' +\n            'To get the original bytes, call tensor.bytes().');\n      }\n    }\n    return data as Promise<DataTypeMap[D]>;\n  }\n\n  /**\n   * Copy the tensor's data to a new GPU resource. Comparing to the `dataSync()`\n   * and `data()`, this method prevents data from being downloaded to CPU.\n   *\n   * For WebGL backend, the data will be stored on a densely packed texture.\n   * This means that the texture will use the RGBA channels to store value.\n   *\n   * For WebGPU backend, the data will be stored on a buffer. There is no\n   * parameter, so can not use a user-defined size to create the buffer.\n   *\n   * @param options:\n   *     For WebGL,\n   *         - customTexShape: Optional. If set, will use the user defined\n   *     texture shape to create the texture.\n   *\n   * @returns For WebGL backend, a GPUData contains the new texture and\n   *     its information.\n   *     {\n   *        tensorRef: The tensor that is associated with this texture,\n   *        texture: WebGLTexture,\n   *        texShape: [number, number] // [height, width]\n   *     }\n   *\n   *     For WebGPU backend, a GPUData contains the new buffer and\n   *     its information.\n   *     {\n   *        tensorRef: The tensor that is associated with this buffer,\n   *        buffer: GPUBuffer,\n   *        bufSize: number\n   *     }\n   *\n   *     Remember to dispose the GPUData after it is used by\n   *     `res.tensorRef.dispose()`.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  dataToGPU(options?: DataToGPUOptions): GPUData {\n    this.throwIfDisposed();\n    return trackerFn().readToGPU(this.dataId, options);\n  }\n\n  /**\n   * Synchronously downloads the values from the `tf.Tensor`. This blocks the\n   * UI thread until the values are ready, which can cause performance issues.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  dataSync<D extends DataType = NumericDataType>(): DataTypeMap[D] {\n    this.throwIfDisposed();\n    const data = trackerFn().readSync(this.dataId);\n    if (this.dtype === 'string') {\n      try {\n        return (data as Uint8Array[]).map(b => util.decodeString(b)) as\n            DataTypeMap[D];\n      } catch {\n        throw new Error(\n            'Failed to decode the string bytes into utf-8. ' +\n            'To get the original bytes, call tensor.bytes().');\n      }\n    }\n    return data as DataTypeMap[D];\n  }\n\n  /** Returns the underlying bytes of the tensor's data. */\n  async bytes(): Promise<Uint8Array[]|Uint8Array> {\n    this.throwIfDisposed();\n    const data = await trackerFn().read(this.dataId);\n    if (this.dtype === 'string') {\n      return data as Uint8Array[];\n    } else {\n      return new Uint8Array((data as TypedArray).buffer);\n    }\n  }\n\n  /**\n   * Disposes `tf.Tensor` from memory.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  dispose(): void {\n    if (this.isDisposed) {\n      return;\n    }\n    trackerFn().disposeTensor(this);\n    this.isDisposedInternal = true;\n  }\n\n  protected isDisposedInternal = false;\n  get isDisposed(): boolean {\n    return this.isDisposedInternal;\n  }\n\n  throwIfDisposed() {\n    if (this.isDisposed) {\n      throw new Error(`Tensor is disposed.`);\n    }\n  }\n\n  /**\n   * Prints the `tf.Tensor`. See `tf.print` for details.\n   *\n   * @param verbose Whether to print verbose information about the tensor,\n   *    including dtype and size.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  print(verbose = false): void {\n    return opHandler.print(this, verbose);\n  }\n\n  /**\n   * Returns a copy of the tensor. See `tf.clone` for details.\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  clone<T extends Tensor>(this: T): T {\n    this.throwIfDisposed();\n    return opHandler.clone(this);\n  }\n\n  /**\n   * Returns a human-readable description of the tensor. Useful for logging.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  toString(verbose = false): string {\n    const vals = this.dataSync();\n    return tensorToString(vals, this.shape, this.dtype, verbose);\n  }\n\n  cast<T extends this>(dtype: DataType): T {\n    this.throwIfDisposed();\n    return opHandler.cast(this as T, dtype);\n  }\n  variable(trainable = true, name?: string, dtype?: DataType): Variable<R> {\n    this.throwIfDisposed();\n    return trackerFn().makeVariable(this, trainable, name, dtype) as\n        Variable<R>;\n  }\n}\n\nObject.defineProperty(Tensor, Symbol.hasInstance, {\n  value: (instance: Tensor) => {\n    // Implementation note: we should use properties of the object that will be\n    // defined before the constructor body has finished executing (methods).\n    // This is because when this code is transpiled by babel, babel will call\n    // classCallCheck before the constructor body is run.\n    // See https://github.com/tensorflow/tfjs/issues/3384 for backstory.\n    return !!instance && instance.data != null && instance.dataSync != null &&\n        instance.throwIfDisposed != null;\n  }\n});\n\nexport function getGlobalTensorClass() {\n  // Use getGlobal so that we can augment the Tensor class across package\n  // boundaries becase the node resolution alg may result in different modules\n  // being returned for this file depending on the path they are loaded from.\n  return getGlobal('Tensor', () => {\n    return Tensor;\n  });\n}\n\n// Global side effect. Cache global reference to Tensor class\ngetGlobalTensorClass();\n\nexport interface NumericTensor<R extends Rank = Rank> extends Tensor<R> {\n  dtype: NumericDataType;\n  dataSync<D extends DataType = NumericDataType>(): DataTypeMap[D];\n  data<D extends DataType = NumericDataType>(): Promise<DataTypeMap[D]>;\n  dataToGPU(options?: DataToGPUOptions): GPUData;\n}\n\nexport interface StringTensor<R extends Rank = Rank> extends Tensor<R> {\n  dtype: 'string';\n  dataSync<D extends DataType = 'string'>(): DataTypeMap[D];\n  data<D extends DataType = 'string'>(): Promise<DataTypeMap[D]>;\n}\n\n/** @doclink Tensor */\nexport type Scalar = Tensor<Rank.R0>;\n/** @doclink Tensor */\nexport type Tensor1D = Tensor<Rank.R1>;\n/** @doclink Tensor */\nexport type Tensor2D = Tensor<Rank.R2>;\n/** @doclink Tensor */\nexport type Tensor3D = Tensor<Rank.R3>;\n/** @doclink Tensor */\nexport type Tensor4D = Tensor<Rank.R4>;\n/** @doclink Tensor */\nexport type Tensor5D = Tensor<Rank.R5>;\n/** @doclink Tensor */\nexport type Tensor6D = Tensor<Rank.R6>;\n\n/**\n * A mutable `tf.Tensor`, useful for persisting state, e.g. for training.\n *\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\nexport class Variable<R extends Rank = Rank> extends Tensor<R> {\n  name: string;\n\n  constructor(\n      initialValue: Tensor<R>, public trainable: boolean, name: string,\n      tensorId: number) {\n    super(\n        initialValue.shape, initialValue.dtype, initialValue.dataId, tensorId);\n    this.name = name;\n  }\n\n  /**\n   * Assign a new `tf.Tensor` to this variable. The new `tf.Tensor` must have\n   * the same shape and dtype as the old `tf.Tensor`.\n   *\n   * @param newValue New tensor to be assigned to this variable.\n   *\n   * @doc {heading: 'Tensors', subheading: 'Classes'}\n   */\n  assign(newValue: Tensor<R>): void {\n    if (newValue.dtype !== this.dtype) {\n      throw new Error(\n          `dtype of the new value (${newValue.dtype}) and ` +\n          `previous value (${this.dtype}) must match`);\n    }\n    if (!util.arraysEqual(newValue.shape, this.shape)) {\n      throw new Error(\n          `shape of the new value (${newValue.shape}) and ` +\n          `previous value (${this.shape}) must match`);\n    }\n    trackerFn().disposeTensor(this);\n    this.dataId = newValue.dataId;\n    trackerFn().incRef(this, null /* backend */);\n  }\n\n  dispose(): void {\n    trackerFn().disposeVariable(this);\n    this.isDisposedInternal = true;\n  }\n}\n\nObject.defineProperty(Variable, Symbol.hasInstance, {\n  value: (instance: Variable) => {\n    return instance instanceof Tensor && instance.assign != null &&\n        instance.assign instanceof Function;\n  }\n});\n","/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n/** @docalias number[] */\nexport interface ShapeMap {\n  R0: number[];\n  R1: [number];\n  R2: [number, number];\n  R3: [number, number, number];\n  R4: [number, number, number, number];\n  R5: [number, number, number, number, number];\n  R6: [number, number, number, number, number, number];\n}\n\n/** @docalias number[] */\nexport interface ArrayMap {\n  R0: number;\n  R1: number[];\n  R2: number[][];\n  R3: number[][][];\n  R4: number[][][][];\n  R5: number[][][][][];\n  R6: number[][][][][][];\n}\n\nexport interface DataTypeMap {\n  float32: Float32Array;\n  int32: Int32Array;\n  bool: Uint8Array;\n  complex64: Float32Array;\n  string: string[];\n}\n\nexport interface SingleValueMap {\n  bool: boolean;\n  int32: number;\n  float32: number;\n  complex64: number;\n  string: string;\n}\n\n/** @docalias 'float32'|'int32'|'bool'|'complex64'|'string' */\nexport type DataType = keyof DataTypeMap;\nexport type NumericDataType = 'float32'|'int32'|'bool'|'complex64';\nexport type TypedArray = Float32Array|Int32Array|Uint8Array;\n/** Tensor data used in tensor creation and user-facing API. */\nexport type DataValues = DataTypeMap[DataType];\n/** The underlying tensor data that gets stored in a backend. */\nexport type BackendValues = Float32Array|Int32Array|Uint8Array|Uint8Array[];\n\nexport enum Rank {\n  R0 = 'R0',\n  R1 = 'R1',\n  R2 = 'R2',\n  R3 = 'R3',\n  R4 = 'R4',\n  R5 = 'R5',\n  R6 = 'R6'\n}\n\nexport type FlatVector = boolean[]|number[]|TypedArray;\nexport type RegularArray<T> =\n    T[]|T[][]|T[][][]|T[][][][]|T[][][][][]|T[][][][][][];\n\n// tslint:disable-next-line:no-any\nexport interface RecursiveArray<T extends any> {\n  [index: number]: T|RecursiveArray<T>;\n}\n\n// Looks for upcasting types. Used, for example, in operations with mixed dtype\n// inputs.\nenum UpcastInt32AndMap {\n  'float32' = 'float32',\n  'int32' = 'int32',\n  'bool' = 'int32',\n  'complex64' = 'complex64'\n}\n\nenum UpcastBoolAndMap {\n  'float32' = 'float32',\n  'int32' = 'int32',\n  'bool' = 'bool',\n  'complex64' = 'complex64'\n}\n\nenum UpcastFloat32AndMap {\n  'float32' = 'float32',\n  'int32' = 'float32',\n  'bool' = 'float32',\n  'complex64' = 'complex64'\n}\n\nenum UpcastComplex64AndMap {\n  'float32' = 'complex64',\n  'int32' = 'complex64',\n  'bool' = 'complex64',\n  'complex64' = 'complex64'\n}\n\nconst upcastTypeMap = {\n  'float32': UpcastFloat32AndMap,\n  'int32': UpcastInt32AndMap,\n  'bool': UpcastBoolAndMap,\n  'complex64': UpcastComplex64AndMap\n};\n\nexport function upcastType(typeA: DataType, typeB: DataType): DataType {\n  if (typeA === 'string' || typeB === 'string') {\n    if (typeA === 'string' && typeB === 'string') {\n      return 'string';\n    }\n    throw new Error(`Can not upcast ${typeA} with ${typeB}`);\n  }\n  return upcastTypeMap[typeA][typeB];\n}\n\n/** Returns the output type after summation. */\nexport function sumOutType(type: DataType): DataType {\n  return upcastType(type, 'int32');\n}\n\n/** @docalias TypedArray|Array */\nexport type TensorLike =\n    TypedArray|number|boolean|string|RecursiveArray<number|number[]|TypedArray>|\n    RecursiveArray<boolean>|RecursiveArray<string>|Uint8Array[];\nexport type ScalarLike = number|boolean|string|Uint8Array;\n/** @docalias TypedArray|Array */\nexport type TensorLike1D = TypedArray|number[]|boolean[]|string[]|Uint8Array[];\n/** @docalias TypedArray|Array */\nexport type TensorLike2D = TypedArray|number[]|number[][]|boolean[]|boolean[][]|\n    string[]|string[][]|Uint8Array[]|Uint8Array[][];\n/** @docalias TypedArray|Array */\nexport type TensorLike3D = TypedArray|number[]|number[][][]|boolean[]|\n    boolean[][][]|string[]|string[][][]|Uint8Array[]|Uint8Array[][][];\n/** @docalias TypedArray|Array */\nexport type TensorLike4D = TypedArray|number[]|number[][][][]|boolean[]|\n    boolean[][][][]|string[]|string[][][][]|Uint8Array[]|Uint8Array[][][][];\n/** @docalias TypedArray|Array */\nexport type TensorLike5D =\n    TypedArray|number[]|number[][][][][]|boolean[]|boolean[][][][][]|string[]|\n    string[][][][][]|Uint8Array[]|Uint8Array[][][][][];\n/** @docalias TypedArray|Array */\nexport type TensorLike6D =\n    TypedArray|number[]|number[][][][][][]|boolean[]|boolean[][][][][][]|\n    string[]|string[][][][][][]|Uint8Array[]|Uint8Array[][][][][];\n\n/** Type for representing image data in Uint8Array type. */\nexport interface PixelData {\n  width: number;\n  height: number;\n  data: Uint8Array;\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Tensor} from './tensor';\nimport {TensorContainer, TensorContainerArray} from './tensor_types';\nimport {upcastType} from './types';\nimport {assert} from './util';\n\nexport function makeTypesMatch<T extends Tensor>(a: T, b: T): [T, T] {\n  if (a.dtype === b.dtype) {\n    return [a, b];\n  }\n  const dtype = upcastType(a.dtype, b.dtype);\n  return [a.cast(dtype), b.cast(dtype)];\n}\n\nexport function assertTypesMatch(a: Tensor, b: Tensor): void {\n  assert(\n      a.dtype === b.dtype,\n      () => `The dtypes of the first(${a.dtype}) and` +\n          ` second(${b.dtype}) input must match`);\n}\n\nexport function isTensorInList(tensor: Tensor, tensorList: Tensor[]): boolean {\n  return tensorList.some(x => x.id === tensor.id);\n}\n\n/**\n * Extracts any `Tensor`s found within the provided object.\n *\n * @param container an object that may be a `Tensor` or may directly contain\n *   `Tensor`s, such as a `Tensor[]` or `{key: Tensor, ...}`. In general it\n *   is safe to pass any object here, except that `Promise`s are not\n *   supported.\n * @returns An array of `Tensors` found within the passed object. If the\n *   argument is simply a `Tensor', a list containing that `Tensor` is\n *   returned. If the object is not a `Tensor` or does not\n *   contain `Tensors`, an empty list is returned.\n */\nexport function getTensorsInContainer(result: TensorContainer): Tensor[] {\n  const list: Tensor[] = [];\n  const seen = new Set<{}|void>();\n  walkTensorContainer(result, list, seen);\n  return list;\n}\n\nfunction walkTensorContainer(\n    container: TensorContainer, list: Tensor[], seen: Set<{}|void>): void {\n  if (container == null) {\n    return;\n  }\n  if (container instanceof Tensor) {\n    list.push(container);\n    return;\n  }\n  if (!isIterable(container)) {\n    return;\n  }\n  // Iteration over keys works also for arrays.\n  const iterable = container as TensorContainerArray;\n  for (const k in iterable) {\n    const val = iterable[k];\n    if (!seen.has(val)) {\n      seen.add(val);\n      walkTensorContainer(val, list, seen);\n    }\n  }\n}\n\n// tslint:disable-next-line:no-any\nfunction isIterable(obj: any): boolean {\n  return Array.isArray(obj) || typeof obj === 'object';\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {BackendTimingInfo, DataMover, KernelBackend} from './backends/backend';\nimport {Environment, setEnvironmentGlobal} from './environment';\nimport {getGlobalNamespace} from './global_util';\nimport {Add, Cast, Identity} from './kernel_names';\nimport {getGradient, getKernel, getKernelsForBackend, GradFunc, NamedAttrMap, TensorInfo} from './kernel_registry';\nimport * as log from './log';\nimport {KernelProfile, Profiler} from './profiler';\nimport {backpropagateGradients, getFilteredNodesXToY, TapeNode} from './tape';\nimport {DataId, DataToGPUOptions, GPUData, setTensorTracker, Tensor, TensorTracker, Variable} from './tensor';\nimport {GradSaveFunc, NamedTensorMap, NamedVariableMap, TensorContainer} from './tensor_types';\nimport {getTensorsInContainer} from './tensor_util';\nimport {BackendValues, DataType, DataValues} from './types';\nimport * as util from './util';\nimport {bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape} from './util';\n\n/**\n * A function that computes an output. The save function is for saving tensors\n * computed in the forward pass, that we need in the backward pass.\n */\nexport type ForwardFunc<T> = (backend: KernelBackend, save?: GradSaveFunc) => T;\n\n/**\n * @docalias (a: Tensor, b: Tensor,..., save?: Function) => {\n *   value: Tensor,\n *   gradFunc: (dy: Tensor, saved?: NamedTensorMap) => Tensor | Tensor[]\n * }\n */\nexport type CustomGradientFunc<T extends Tensor> =\n    (...inputs: Array<Tensor|GradSaveFunc>) => {\n      value: T;\n      gradFunc: (dy: T, saved: Tensor[]) => Tensor | Tensor[];\n    };\n\nexport type MemoryInfo = {\n  numTensors: number; numDataBuffers: number; numBytes: number;\n  unreliable?: boolean; reasons: string[];\n};\n\ntype KernelInfo = {\n  name: string; bytesAdded: number; totalBytesSnapshot: number;\n  tensorsAdded: number;\n  totalTensorsSnapshot: number;\n  inputShapes: number[][];\n  outputShapes: number[][];\n  kernelTimeMs: number | {error: string} | Promise<number|{error: string}>;\n  extraInfo: string | Promise<string>;\n};\n\nexport type ProfileInfo = {\n  newBytes: number; newTensors: number; peakBytes: number;\n  kernels: KernelInfo[];\n  result: TensorContainer;\n  kernelNames: string[];\n};\n\nexport interface TimingInfo extends BackendTimingInfo {\n  wallMs: number;\n}\n\n/** @docalias Function */\nexport type ScopeFn<T extends TensorContainer> = () => T;\n\ninterface ScopeState {\n  track: Tensor[];\n  name: string;\n  id: number;\n}\n\ninterface RegisteredKernelInvocation<I extends NamedTensorMap> {\n  kernelName: string;\n  inputs: I;\n  attrs?: NamedAttrMap;\n}\n\ninterface CustomGradKernelInvocation<T extends Tensor|Tensor[],\n                                               I extends NamedTensorMap> {\n  forwardFunc: ForwardFunc<T>;\n  backwardsFunc: (dy: T, saved: Tensor[]) => {\n    [P in keyof I]: () => I[P]\n  };\n  inputs: I;\n  attrs?: NamedAttrMap;\n}\n\nfunction isRegisteredKernelInvocation<T extends Tensor|Tensor[],\n                                                I extends NamedTensorMap>(\n    kernelInvocation: RegisteredKernelInvocation<I>|\n    CustomGradKernelInvocation<T, I>):\n    kernelInvocation is RegisteredKernelInvocation<I> {\n  return (kernelInvocation as RegisteredKernelInvocation<I>).kernelName != null;\n}\n\nclass EngineState {\n  // Public since optimizers will use it.\n  registeredVariables: NamedVariableMap = {};\n\n  nextTapeNodeId = 0;\n  numBytes = 0;\n  numTensors = 0;\n  numStringTensors = 0;\n  numDataBuffers = 0;\n\n  activeTape: TapeNode[];\n  // Number of nested tf.grad() statements when computing higher-order\n  // gradients. E.g. `1` for first-order gradients and `2` for second-order\n  // gradients. Used to track if the tape should be removed after a backprop.\n  gradientDepth = 0;\n  // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n  // off the tape.\n  kernelDepth = 0;\n\n  // Keep Tensors that parallel the tapes.\n  activeScope: ScopeState;\n  scopeStack: ScopeState[] = [];\n  /**\n   * Keeps track of the number of data moves during a kernel execution. We\n   * maintain a stack since kernels can call other kernels, recursively.\n   */\n  numDataMovesStack: number[] = [];\n  nextScopeId = 0;\n\n  tensorInfo = new WeakMap<DataId, {\n    backend: KernelBackend,\n    bytes: number,\n    dtype: DataType,\n    shape: number[]\n  }>();\n\n  profiling = false;\n  activeProfile: ProfileInfo = {\n    newBytes: 0,\n    newTensors: 0,\n    peakBytes: 0,\n    kernels: [],\n    result: null,\n    get kernelNames():\n        string[] {\n          return Array.from(new Set(this.kernels.map(k => k.name)));\n        }\n  };\n\n  dispose() {\n    for (const variableName in this.registeredVariables) {\n      this.registeredVariables[variableName].dispose();\n    }\n  }\n}\n\nexport class Engine implements TensorTracker, DataMover {\n  state: EngineState;\n  backendName: string;\n  registry: {[id: string]: KernelBackend} = {};\n  registryFactory: {\n    [id: string]: {\n      factory: () => KernelBackend | Promise<KernelBackend>,\n      priority: number\n    }\n  } = {};\n\n  private profiler: Profiler;\n  private backendInstance: KernelBackend;\n  private pendingBackendInit: Promise<boolean>;\n  private pendingBackendInitId = 0;\n\n  constructor(public ENV: Environment) {\n    this.state = new EngineState();\n  }\n\n  async ready(): Promise<void> {\n    if (this.pendingBackendInit != null) {\n      return this.pendingBackendInit.then(() => {});\n    }\n    if (this.backendInstance != null) {\n      return;\n    }\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const success = await this.initializeBackend(backendName).success;\n      if (success) {\n        await this.setBackend(backendName);\n        return;\n      }\n    }\n\n    throw new Error(\n        `Could not initialize any backends, all backend initializations ` +\n        `failed.`);\n  }\n\n  get backend(): KernelBackend {\n    if (this.pendingBackendInit != null) {\n      throw new Error(\n          `Backend '${this.backendName}' has not yet been initialized. Make ` +\n          `sure to await tf.ready() or await tf.setBackend() before calling ` +\n          `other methods`);\n    }\n    if (this.backendInstance == null) {\n      const {name, asyncInit} = this.initializeBackendsAndReturnBest();\n      if (asyncInit) {\n        throw new Error(\n            `The highest priority backend '${name}' has not yet been ` +\n            `initialized. Make sure to await tf.ready() or ` +\n            `await tf.setBackend() before calling other methods`);\n      }\n      this.setBackend(name);\n    }\n    return this.backendInstance;\n  }\n\n  backendNames(): string[] {\n    return Object.keys(this.registryFactory);\n  }\n\n  findBackend(backendName: string): KernelBackend {\n    if (!(backendName in this.registry)) {\n      // If the backend hasn't been initialized but we have a registry entry for\n      // it, initialize it and return it.\n      if (backendName in this.registryFactory) {\n        const {asyncInit} = this.initializeBackend(backendName);\n        if (asyncInit) {\n          // Backend is not ready yet.\n          return null;\n        }\n      } else {\n        return null;\n      }\n    }\n    return this.registry[backendName];\n  }\n\n  findBackendFactory(backendName: string):\n      () => KernelBackend | Promise<KernelBackend> {\n    if (!(backendName in this.registryFactory)) {\n      return null;\n    }\n    return this.registryFactory[backendName].factory;\n  }\n\n  registerBackend(\n      backendName: string,\n      factory: () => KernelBackend | Promise<KernelBackend>,\n      priority = 1): boolean {\n    if (backendName in this.registryFactory) {\n      log.warn(\n          `${backendName} backend was already registered. ` +\n          `Reusing existing backend factory.`);\n      return false;\n    }\n    this.registryFactory[backendName] = {factory, priority};\n    return true;\n  }\n\n  async setBackend(backendName: string): Promise<boolean> {\n    if (this.registryFactory[backendName] == null) {\n      throw new Error(`Backend name '${backendName}' not found in registry`);\n    }\n    this.backendName = backendName;\n    if (this.registry[backendName] == null) {\n      this.backendInstance = null;\n      const {success, asyncInit} = this.initializeBackend(backendName);\n      const result = asyncInit ? await success : success;\n      if (!result) {\n        return false;\n      }\n    }\n    this.backendInstance = this.registry[backendName];\n    this.setupRegisteredKernels();\n    // Reset the profiler.\n    this.profiler = new Profiler(this.backendInstance);\n\n    return true;\n  }\n\n  private setupRegisteredKernels(): void {\n    const kernels = getKernelsForBackend(this.backendName);\n    kernels.forEach(kernel => {\n      if (kernel.setupFunc != null) {\n        kernel.setupFunc(this.backendInstance);\n      }\n    });\n  }\n\n  private disposeRegisteredKernels(backendName: string): void {\n    const kernels = getKernelsForBackend(backendName);\n    kernels.forEach(kernel => {\n      if (kernel.disposeFunc != null) {\n        kernel.disposeFunc(this.registry[backendName]);\n      }\n    });\n  }\n\n  /**\n   * Initializes a backend by looking up the backend name in the factory\n   * registry and calling the factory method. Returns a boolean representing\n   * whether the initialization of the backend suceeded. Throws an error if\n   * there is no backend in the factory registry.\n   */\n  private initializeBackend(backendName: string):\n      {success: boolean|Promise<boolean>, asyncInit: boolean} {\n    const registryFactoryEntry = this.registryFactory[backendName];\n    if (registryFactoryEntry == null) {\n      throw new Error(\n          `Cannot initialize backend ${backendName}, no registration found.`);\n    }\n\n    try {\n      const backend = registryFactoryEntry.factory();\n      /* Test if the factory returns a promise.\n      Done in a more liberal way than\n      previous 'Promise.resolve(backend)===backend'\n      as we needed to account for custom Promise\n      implementations (e.g. Angular) */\n      if (backend && !(backend instanceof KernelBackend) &&\n          typeof backend.then === 'function') {\n        const promiseId = ++this.pendingBackendInitId;\n        const success =\n            backend\n                .then(backendInstance => {\n                  // Outdated promise. Another backend was set in the meantime.\n                  if (promiseId < this.pendingBackendInitId) {\n                    return false;\n                  }\n                  this.registry[backendName] = backendInstance;\n                  this.pendingBackendInit = null;\n                  return true;\n                })\n                .catch(err => {\n                  // Outdated promise. Another backend was set in the meantime.\n                  if (promiseId < this.pendingBackendInitId) {\n                    return false;\n                  }\n                  this.pendingBackendInit = null;\n                  log.warn(`Initialization of backend ${backendName} failed`);\n                  log.warn(err.stack || err.message);\n                  return false;\n                });\n        this.pendingBackendInit = success;\n        return {success, asyncInit: true};\n      } else {\n        this.registry[backendName] = backend as KernelBackend;\n        return {success: true, asyncInit: false};\n      }\n    } catch (err) {\n      log.warn(`Initialization of backend ${backendName} failed`);\n      log.warn(err.stack || err.message);\n      return {success: false, asyncInit: false};\n    }\n  }\n\n  removeBackend(backendName: string): void {\n    if (!(backendName in this.registryFactory)) {\n      throw new Error(`${backendName} backend not found in registry`);\n    }\n    if (this.backendName === backendName && this.pendingBackendInit != null) {\n      // There is a pending promise of the backend we want to remove. Make it\n      // obsolete.\n      this.pendingBackendInitId++;\n    }\n\n    if (backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n\n    delete this.registryFactory[backendName];\n\n    // Unset the backend if it is active.\n    if (this.backendName === backendName) {\n      this.pendingBackendInit = null;\n      this.backendName = null;\n      this.backendInstance = null;\n    }\n  }\n\n  private getSortedBackends(): string[] {\n    if (Object.keys(this.registryFactory).length === 0) {\n      throw new Error('No backend found in registry.');\n    }\n    return Object.keys(this.registryFactory).sort((a: string, b: string) => {\n      // Highest priority comes first.\n      return this.registryFactory[b].priority -\n          this.registryFactory[a].priority;\n    });\n  }\n\n  private initializeBackendsAndReturnBest():\n      {name: string, asyncInit: boolean} {\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const {success, asyncInit} = this.initializeBackend(backendName);\n      if (asyncInit || success) {\n        return {name: backendName, asyncInit};\n      }\n    }\n    throw new Error(\n        `Could not initialize any backends, all backend initializations ` +\n        `failed.`);\n  }\n\n  moveData(backend: KernelBackend, dataId: DataId) {\n    const info = this.state.tensorInfo.get(dataId);\n    const srcBackend = info.backend;\n    const values = this.readSync(dataId);\n    const refCount = srcBackend.refCount(dataId);\n    // Delete the tensor from the old backend and move it to the new\n    // backend.\n    srcBackend.disposeData(dataId, true);\n    info.backend = backend;\n    backend.move(dataId, values, info.shape, info.dtype, refCount);\n    if (this.shouldCheckForMemLeaks()) {\n      // Track the number of moves during a kernel execution to correctly\n      // detect memory leaks.\n      this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n    }\n  }\n\n  tidy<T extends TensorContainer>(nameOrFn: string|ScopeFn<T>, fn?: ScopeFn<T>):\n      T {\n    let name: string = null;\n    if (fn == null) {\n      // Called with only 1 argument.\n      if (typeof nameOrFn !== 'function') {\n        throw new Error('Please provide a function to tidy()');\n      }\n      fn = nameOrFn;\n    } else {\n      // Called with 2 arguments.\n      if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n        throw new Error(\n            'When calling with two arguments, the first argument ' +\n            'to tidy() must be a string');\n      }\n      if (typeof fn !== 'function') {\n        throw new Error(\n            'When calling with two arguments, the 2nd argument ' +\n            'to tidy() must be a function');\n      }\n      name = nameOrFn as string;\n      // TODO(nsthorat,smilkov): Do operation logging and performance\n      // profiling.\n    }\n    let result: T;\n    return this.scopedRun(\n        () => this.startScope(name), () => this.endScope(result), () => {\n          result = fn();\n          if (result instanceof Promise) {\n            console.error('Cannot return a Promise inside of tidy.');\n          }\n          return result;\n        });\n  }\n\n  private scopedRun<T>(start: () => void, end: () => void, f: () => T): T {\n    start();\n    try {\n      const res = f();\n      end();\n      return res;\n    } catch (ex) {\n      end();\n      throw ex;\n    }\n  }\n\n  private static nextTensorId = 0;\n  private nextTensorId(): number {\n    return Engine.nextTensorId++;\n  }\n\n  private static nextVariableId = 0;\n  private nextVariableId(): number {\n    return Engine.nextVariableId++;\n  }\n\n  /**\n   * This method is called instead of the public-facing tensor.clone() when\n   * saving a tensor for backwards pass. It makes sure to add the clone\n   * operation to the tape regardless of being called inside a kernel\n   * execution.\n   */\n  private clone(x: Tensor): Tensor {\n    const y: Tensor = ENGINE.runKernel(Identity, {x} as {} as NamedTensorMap);\n    const inputs = {x};\n    const grad = (dy: Tensor) => ({\n      x: () => {\n        const dtype = 'float32';\n        const gradInputs = {x: dy};\n        const attrs = {dtype};\n\n        return ENGINE.runKernel(\n                   Cast, gradInputs as {} as NamedTensorMap,\n                   // tslint:disable-next-line: no-unnecessary-type-assertion\n                   attrs as {} as NamedAttrMap) as Tensor;\n      }\n    });\n    const saved: Tensor[] = [];\n    this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n    return y;\n  }\n\n  /**\n   * Execute a kernel with the given name and return the output tensor.\n   *\n   * @param kernelName The name of the kernel to execute.\n   * @param inputs A map of input names to tensors.\n   * @param attrs A map of attribute names to their values. An attribute is a\n   *     primitive (non-tensor) input to the kernel.\n   * @param inputsToSave A list of tensors, inputs to save for the backprop\n   *     computation.\n   * @param outputsToSave A list of booleans, specifying which output to save\n   *     for the backprop computation. These are booleans since the output\n   * tensors are not visible to the user.\n   */\n  runKernel<T extends Tensor|Tensor[]>(\n      kernelName: string, inputs: NamedTensorMap, attrs?: NamedAttrMap): T {\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n    const hasKernel = getKernel(kernelName, this.backendName) != null;\n    if (!hasKernel) {\n      throw new Error(`Kernel '${kernelName}' not registered for backend '${\n          this.backendName}'`);\n    }\n    return this.runKernelFunc({kernelName, inputs, attrs});\n  }\n\n  private shouldCheckForMemLeaks(): boolean {\n    return this.ENV.getBool('IS_TEST');\n  }\n\n  private checkKernelForMemLeak(\n      kernelName: string, numDataIdsBefore: number,\n      outInfos: TensorInfo[]): void {\n    const numDataIdsAfter = this.backend.numDataIds();\n\n    // Count the number of data ids associated with the result of the kernel.\n    let numOutputDataIds = 0;\n    outInfos.forEach(info => {\n      // Complex numbers allocate 3 data ids, one for 'real', one for\n      // 'imaginary', and one for the container that holds the former two.\n      numOutputDataIds += (info.dtype === 'complex64' ? 3 : 1);\n    });\n\n    // Account for the number of moves during kernel execution. A \"data move\"\n    // can happen in the middle of a kernel execution, placing a new (key,value)\n    // pair in the data storage. Since data moves have net zero effect (we\n    // always remove the data from the old backend), we have to cancel them out\n    // when detecting memory leaks.\n    const numMoves =\n        this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n    const dataIdsLeaked =\n        numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n    if (dataIdsLeaked > 0) {\n      throw new Error(\n          `Backend '${this.backendName}' has an internal memory leak ` +\n          `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\n    }\n  }\n\n  /**\n   * Internal helper method to execute a kernel Func\n   *\n   * Use `runKernel` to execute kernels from outside of engine.\n   */\n  private runKernelFunc<T extends Tensor|Tensor[], I extends NamedTensorMap>(\n      kernelParams: RegisteredKernelInvocation<I>|\n      CustomGradKernelInvocation<T, I>): T {\n    let outputs: Tensor[];\n    let saved: Tensor[] = [];\n    const isTapeOn = this.isTapeOn();\n\n    const startingBytecount = this.state.numBytes;\n    const startingNumTensors = this.state.numTensors;\n\n    if (this.shouldCheckForMemLeaks()) {\n      this.state.numDataMovesStack.push(0);\n    }\n\n    let kernelFunc: () => Tensor[];\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n\n    let out: TensorInfo|TensorInfo[];\n\n    const kernelOrScopeName = isRegisteredKernelInvocation(kernelParams) ?\n        kernelParams.kernelName :\n        this.state.activeScope != null ? this.state.activeScope.name : '';\n\n    // Create the kernelFunc from either a registered kernel OR passed in\n    // forward/backward functions (used by custom grad). In this context a\n    // kernelFunc wraps a kernel implementation with some bookkeeping.\n\n    if (isRegisteredKernelInvocation(kernelParams)) {\n      const {kernelName, inputs, attrs} = kernelParams;\n      if (this.backendName == null) {\n        // backend has not been initialized yet (backend initialization is lazy\n        // can be deferred until an op/ kernel is run).\n        // The below getter has side effects that will try to initialize the\n        // backend and set properties like this.backendName\n        // tslint:disable-next-line: no-unused-expression\n        this.backend;\n      }\n      const kernel = getKernel(kernelName, this.backendName);\n      util.assert(\n          kernel != null,\n          () => `Cannot find registered kernel '${kernelName}' for backend '${\n              this.backendName}'`);\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = kernel.kernelFunc({inputs, attrs, backend: this.backend});\n        const outInfos = Array.isArray(out) ? out : [out];\n        if (this.shouldCheckForMemLeaks()) {\n          this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n        }\n\n        const outTensors = outInfos.map((outInfo: TensorInfo|Tensor) => {\n          // todo (yassogba) remove this option (Tensor) when node backend\n          // methods have been modularized and they all return tensorInfo.\n          // TensorInfos do not have a rank attribute.\n          if ((outInfo as Tensor).rank != null) {\n            return outInfo as Tensor;\n          }\n          return this.makeTensorFromTensorInfo(outInfo);\n        });\n\n        // Save any required inputs and outputs.\n\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since there would be no backprop for these tensors\n        // (which would otherwise dispose them).\n        if (isTapeOn) {\n          const tensorsToSave =\n              this.getTensorsForGradient(kernelName, inputs, outTensors);\n          saved = this.saveTensorsForBackwardMode(tensorsToSave);\n        }\n        return outTensors;\n      };\n    } else {\n      const {forwardFunc} = kernelParams;\n      // Running a customGrad op.\n      const saveFunc: GradSaveFunc = (tensors) => {\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since we would never run backprop, which disposes\n        // the kept tensors.\n        if (!isTapeOn) {\n          return;\n        }\n        saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n      };\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = this.tidy(() => forwardFunc(this.backend, saveFunc));\n        const outs = (Array.isArray(out) ? out : [out]) as Tensor[];\n        if (this.shouldCheckForMemLeaks()) {\n          // Scope name is used to print a more helpful error message if needed.\n          this.checkKernelForMemLeak(kernelOrScopeName, numDataIdsBefore, outs);\n        }\n        return outs;\n      };\n    }\n\n    //\n    // Run the kernelFunc. Optionally profiling it.\n    //\n    const {inputs, attrs} = kernelParams;\n    const backwardsFunc = isRegisteredKernelInvocation(kernelParams) ?\n        null :\n        kernelParams.backwardsFunc;\n\n    let kernelProfile: KernelProfile;\n    this.scopedRun(\n        // Stop recording to a tape when running a kernel.\n        () => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n          if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\n            outputs = kernelFunc();\n          } else {\n            kernelProfile = this.profiler.profileKernel(\n                kernelOrScopeName, inputs, () => kernelFunc());\n            if (this.ENV.getBool('DEBUG')) {\n              this.profiler.logKernelProfile(kernelProfile);\n            }\n            outputs = kernelProfile.outputs;\n          }\n        });\n\n    if (isTapeOn) {\n      this.addTapeNode(\n          kernelOrScopeName, inputs, outputs, backwardsFunc, saved, attrs);\n    }\n\n    if (this.state.profiling) {\n      this.state.activeProfile.kernels.push({\n        name: kernelOrScopeName,\n        bytesAdded: this.state.numBytes - startingBytecount,\n        totalBytesSnapshot: this.state.numBytes,\n        tensorsAdded: this.state.numTensors - startingNumTensors,\n        totalTensorsSnapshot: this.state.numTensors,\n        inputShapes: Object.keys(inputs).map(\n            key => inputs[key] != null ? inputs[key].shape : null),\n        outputShapes: outputs.map(item => item.shape),\n        kernelTimeMs: kernelProfile.timeMs,\n        extraInfo: kernelProfile.extraInfo\n      });\n    }\n    return (Array.isArray(out) ? outputs : outputs[0]) as T;\n  }\n\n  /**\n   * Saves tensors used in forward mode for use in backward mode.\n   *\n   * @param tensors the list of tensors to save.\n   */\n  private saveTensorsForBackwardMode(tensors: Tensor[]): Tensor[] {\n    const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n    return saved;\n  }\n\n  /**\n   * Returns a list of tensors to save for a given gradient calculation.\n   *\n   * @param kernelName name of kernel to look up gradient for.\n   * @param inputs a map of input tensors.\n   * @param outputs an array of output tensors from forward mode of kernel.\n   */\n  private getTensorsForGradient(\n      kernelName: string, inputs: NamedTensorMap,\n      outputs: Tensor[]): Tensor[]|null {\n    const gradConfig = getGradient(kernelName);\n    if (gradConfig != null) {\n      const inputsToSave: string[] = gradConfig.inputsToSave || [];\n      const outputsToSave: boolean[] = gradConfig.outputsToSave || [];\n\n      // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n      // specified in inputsToSave will be saved.\n      let inputTensorsToSave: Tensor[];\n      if (gradConfig.saveAllInputs) {\n        util.assert(\n            Array.isArray(inputs),\n            () => 'saveAllInputs is true, expected inputs to be an array.');\n\n        inputTensorsToSave = Object.keys(inputs).map((key) => inputs[key]);\n      } else {\n        inputTensorsToSave = inputsToSave.map((inputName) => inputs[inputName]);\n      }\n\n      const outputTensorsToSave: Tensor[] =\n          outputs.filter((_, i) => outputsToSave[i]);\n\n      return inputTensorsToSave.concat(outputTensorsToSave);\n    }\n    // We return an empty list rather than throw an error because the kernel we\n    // are looking up may not actually be relevant to backproping through the\n    // overall function\n    //\n    // See 'does not error if irrelevant (pruned) ops are missing grads' test\n    // in gradients_test.ts for an example.\n    return [];\n  }\n\n  /**\n   * Internal method used by public APIs for tensor creation. Makes a new\n   * tensor with the provided shape, dtype and values. It always\n   * creates a new data id and writes the values to the underlying backend.\n   */\n  makeTensor(\n      values: DataValues, shape: number[], dtype: DataType,\n      backend?: KernelBackend): Tensor {\n    if (values == null) {\n      throw new Error('Values passed to engine.makeTensor() are null');\n    }\n    dtype = dtype || 'float32';\n    backend = backend || this.backend;\n    let backendVals = values as BackendValues;\n    if (dtype === 'string' && util.isString(values[0])) {\n      backendVals = (values as string[]).map(d => util.encodeString(d));\n    }\n    const dataId = backend.write(backendVals, shape, dtype);\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend);\n\n    // Count bytes for string tensors.\n    if (dtype === 'string') {\n      const info = this.state.tensorInfo.get(dataId);\n      const newBytes = bytesFromStringArray(backendVals as Uint8Array[]);\n      this.state.numBytes += newBytes - info.bytes;\n      info.bytes = newBytes;\n    }\n    return t;\n  }\n\n  /**\n   * Internal method used by backends. Makes a new tensor\n   * that is a wrapper around an existing data id. It doesn't create\n   * a new data id, only increments the ref count used in memory tracking.\n   * @deprecated\n   */\n  makeTensorFromDataId(\n    dataId: DataId, shape: number[], dtype: DataType,\n    backend?: KernelBackend): Tensor {\n    dtype = dtype || 'float32';\n    const tensorInfo: TensorInfo = {dataId, shape, dtype};\n    return this.makeTensorFromTensorInfo(tensorInfo, backend);\n  }\n\n  /**\n   * Internal method used by backends. Makes a new tensor that is a wrapper\n   * around an existing data id in TensorInfo. It doesn't create a new data id,\n   * only increments the ref count used in memory tracking.\n   */\n  makeTensorFromTensorInfo(tensorInfo: TensorInfo, backend?: KernelBackend):\n      Tensor {\n    const {dataId, shape, dtype} = tensorInfo;\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend);\n    return t;\n  }\n\n  makeVariable(\n      initialValue: Tensor, trainable = true, name?: string,\n      dtype?: DataType): Variable {\n    name = name || this.nextVariableId().toString();\n    if (dtype != null && dtype !== initialValue.dtype) {\n      initialValue = initialValue.cast(dtype);\n    }\n    const v = new Variable(initialValue, trainable, name, this.nextTensorId());\n    if (this.state.registeredVariables[v.name] != null) {\n      throw new Error(`Variable with name ${v.name} was already registered`);\n    }\n    this.state.registeredVariables[v.name] = v;\n    this.incRef(v, this.backend);\n    return v;\n  }\n\n  trackTensor(a: Tensor, backend: KernelBackend): void {\n    this.state.numTensors++;\n    if (a.dtype === 'string') {\n      this.state.numStringTensors++;\n    }\n    // Bytes for complex numbers are counted by their components. Bytes for\n    // string tensors are counted when writing values.\n    let bytes = 0;\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      bytes = a.size * util.bytesPerElement(a.dtype);\n    }\n    this.state.numBytes += bytes;\n\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      this.state.numDataBuffers++;\n      this.state.tensorInfo.set(a.dataId, {\n        backend: backend || this.backend,\n        dtype: a.dtype,\n        shape: a.shape,\n        bytes\n      });\n    }\n\n    if (!(a instanceof Variable)) {\n      this.track(a);\n    }\n  }\n\n  // Track the tensor by dataId and increase the refCount for the dataId in the\n  // backend.\n  // TODO(pyu10055): This is currently used by makeVariable method, to increase\n  // refCount on the backend for the dataId. It can potentially be replaced with\n  // Identity op indead of calling backend directly.\n  incRef(a: Tensor, backend: KernelBackend): void {\n    this.trackTensor(a, backend);\n    this.backend.incRef(a.dataId);\n  }\n\n  removeDataId(dataId: DataId, backend: KernelBackend) {\n    if (this.state.tensorInfo.has(dataId) &&\n        this.state.tensorInfo.get(dataId).backend === backend) {\n      this.state.tensorInfo.delete(dataId);\n      this.state.numDataBuffers--;\n    }\n  }\n  disposeTensor(a: Tensor): void {\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      return;\n    }\n    const info = this.state.tensorInfo.get(a.dataId);\n\n    this.state.numTensors--;\n    if (a.dtype === 'string') {\n      this.state.numStringTensors--;\n      this.state.numBytes -= info.bytes;\n    }\n    // Don't count bytes for complex numbers as they are counted by their\n    // components.\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      const bytes = a.size * util.bytesPerElement(a.dtype);\n      this.state.numBytes -= bytes;\n    }\n\n    // Remove the reference to dataId if backend dispose the data successfully\n    if (info.backend.disposeData(a.dataId)) {\n      this.removeDataId(a.dataId, info.backend);\n    }\n\n    // TODO(nsthorat): Construct an error and save the stack trace for\n    // debugging when in debug mode. Creating a stack trace is too expensive\n    // to do unconditionally.\n  }\n\n  disposeVariables(): void {\n    for (const varName in this.state.registeredVariables) {\n      const v = this.state.registeredVariables[varName];\n      this.disposeVariable(v);\n    }\n  }\n\n  disposeVariable(v: Variable): void {\n    this.disposeTensor(v);\n    if (this.state.registeredVariables[v.name] != null) {\n      delete this.state.registeredVariables[v.name];\n    }\n  }\n\n  memory(): MemoryInfo {\n    const info = this.backend.memory() as MemoryInfo;\n    info.numTensors = this.state.numTensors;\n    info.numDataBuffers = this.state.numDataBuffers;\n    info.numBytes = this.state.numBytes;\n    if (this.state.numStringTensors > 0) {\n      info.unreliable = true;\n      if (info.reasons == null) {\n        info.reasons = [];\n      }\n      info.reasons.push(\n          'Memory usage by string tensors is approximate ' +\n          '(2 bytes per character)');\n    }\n    return info;\n  }\n\n  async profile(query: () => (TensorContainer | Promise<TensorContainer>)):\n      Promise<ProfileInfo> {\n    this.state.profiling = true;\n\n    const startBytes = this.state.numBytes;\n    const startNumTensors = this.state.numTensors;\n\n    this.state.activeProfile.kernels = [];\n    this.state.activeProfile.result = await query();\n\n    this.state.profiling = false;\n\n    this.state.activeProfile.peakBytes = Math.max(\n        ...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\n    this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n    this.state.activeProfile.newTensors =\n        this.state.numTensors - startNumTensors;\n    for (const kernel of this.state.activeProfile.kernels) {\n      kernel.kernelTimeMs = await kernel.kernelTimeMs;\n      kernel.extraInfo = await kernel.extraInfo;\n    }\n    return this.state.activeProfile;\n  }\n\n  isTapeOn(): boolean {\n    return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n  }\n\n  private addTapeNode(\n      kernelName: string, inputs: NamedTensorMap, outputs: Tensor[],\n      gradientsFunc: GradFunc, saved: Tensor[], attrs: NamedAttrMap): void {\n    const tapeNode: TapeNode =\n        {id: this.state.nextTapeNodeId++, kernelName, inputs, outputs, saved};\n\n    const gradConfig = getGradient(kernelName);\n    if (gradConfig != null) {\n      gradientsFunc = gradConfig.gradFunc;\n    }\n    if (gradientsFunc != null) {\n      tapeNode.gradient = (dys: Tensor[]) => {\n        // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n        // the backprop graph to the user as null instead of zeros\n        dys = dys.map((dy, i) => {\n          if (dy == null) {\n            const output = outputs[i];\n            const vals = util.makeZerosTypedArray(output.size, output.dtype);\n            return this.makeTensor(vals, output.shape, output.dtype);\n          }\n          return dy;\n        });\n        // Grad functions of ops with single outputs expect a dy, while ops\n        // with multiple outputs expect dys (array of dy).\n        return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n      };\n    }\n    this.state.activeTape.push(tapeNode);\n  }\n\n  keep<T extends Tensor>(result: T): T {\n    result.kept = true;\n    return result;\n  }\n\n  private startTape() {\n    if (this.state.gradientDepth === 0) {\n      this.state.activeTape = [];\n    }\n    this.state.gradientDepth++;\n  }\n\n  private endTape() {\n    this.state.gradientDepth--;\n  }\n\n  /**\n   * Start a scope. Use this with endScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n  startScope(name?: string) {\n    const scopeInfo: ScopeState = {\n      track: [],\n      name: 'unnamed scope',\n      id: this.state.nextScopeId++\n    };\n    if (name) {\n      scopeInfo.name = name;\n    }\n    this.state.scopeStack.push(scopeInfo);\n    this.state.activeScope = scopeInfo;\n  }\n\n  /**\n   * End a scope. Use this with startScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n  endScope(result?: TensorContainer) {\n    const tensorsToTrackInParent = getTensorsInContainer(result);\n    const tensorsToTrackInParentSet =\n        new Set(tensorsToTrackInParent.map(t => t.id));\n\n    // Dispose the arrays tracked in this scope.\n    for (let i = 0; i < this.state.activeScope.track.length; i++) {\n      const tensor = this.state.activeScope.track[i];\n      if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n        tensor.dispose();\n      }\n    }\n\n    const oldScope = this.state.scopeStack.pop();\n    this.state.activeScope = this.state.scopeStack.length === 0 ?\n        null :\n        this.state.scopeStack[this.state.scopeStack.length - 1];\n\n    // Track the current result in the parent scope.\n    tensorsToTrackInParent.forEach(tensor => {\n      // Only track the tensor if was allocated in the inner scope and is not\n      // globally kept.\n      if (!tensor.kept && tensor.scopeId === oldScope.id) {\n        this.track(tensor);\n      }\n    });\n  }\n\n  /**\n   * Returns gradients of `f` with respect to each of the `xs`. The gradients\n   * returned are of the same length as `xs`, but some might be null if `f`\n   * was not a function of that `x`. It also takes optional dy to multiply the\n   * gradient, which defaults to `1`.\n   */\n  gradients<T extends Tensor>(\n      f: () => T, xs: Tensor[], dy?: T,\n      allowNoGradients = false): {value: T, grads: Tensor[]} {\n    util.assert(\n        xs.length > 0, () => 'gradients() received an empty list of xs.');\n    if (dy != null && dy.dtype !== 'float32') {\n      throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\n    }\n\n    const y = this.scopedRun(\n        () => this.startTape(), () => this.endTape(),\n        () => this.tidy('forward', f));\n\n    util.assert(\n        y instanceof Tensor,\n        () => 'The result y returned by f() must be a tensor.');\n    // Filter out the nodes that don't connect x => y.\n    const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n    if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n      throw new Error(\n          'Cannot compute gradient of y=f(x) with respect to x. Make sure ' +\n          'that the f you passed encloses all operations that lead from x ' +\n          'to y.');\n    }\n\n    return this.tidy('backward', () => {\n      const accumulatedGradientMap: {[tensorId: number]: Tensor} = {};\n      accumulatedGradientMap[y.id] = (dy == null) ? ones(y.shape) : dy;\n\n      // Backprop gradients through the filtered nodes.\n      backpropagateGradients(\n          accumulatedGradientMap, filteredTape,\n          // Pass the tidy function to avoid circular dep with `tape.ts`.\n          f => this.tidy(f as ScopeFn<Tensor>),\n          // Pass an add function to avoide a circular dep with `tape.ts`.\n          add);\n      const grads = xs.map(x => accumulatedGradientMap[x.id]);\n\n      if (this.state.gradientDepth === 0) {\n        // This means that we are not computing higher-order gradients\n        // and can clean up the tape.\n        this.state.activeTape.forEach(node => {\n          for (const tensor of node.saved) {\n            tensor.dispose();\n          }\n        });\n        this.state.activeTape = null;\n      }\n      return {value: y, grads};\n    });\n  }\n\n  customGrad<T extends Tensor>(f: CustomGradientFunc<T>):\n      (...args: Array<Tensor|GradSaveFunc>) => T {\n    util.assert(\n        util.isFunction(f),\n        () => 'The f passed in customGrad(f) must be a function.');\n    return (...inputs: Tensor[]): T => {\n      util.assert(\n          inputs.every(t => t instanceof Tensor),\n          () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' +\n              'tensors');\n\n      let res: {\n        value: T,\n        gradFunc: (dy: T, saved: Tensor[]) => Tensor | Tensor[],\n      };\n      const inputMap: NamedTensorMap = {};\n      inputs.forEach((input, i) => {\n        inputMap[i] = input;\n      });\n\n      const forwardFunc: ForwardFunc<T> = (_, save) => {\n        res = f(...[...inputs, save]);\n        util.assert(\n            res.value instanceof Tensor,\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.value` is a tensor');\n        util.assert(\n            util.isFunction(res.gradFunc),\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.gradFunc` is a function.');\n        return res.value;\n      };\n\n      const backwardsFunc = (dy: T, saved: Tensor[]) => {\n        const gradRes = res.gradFunc(dy, saved);\n        const grads: Tensor[] = Array.isArray(gradRes) ? gradRes : [gradRes];\n        util.assert(\n            grads.length === inputs.length,\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.gradFunc` is a function that returns ' +\n                'the same number of tensors as inputs passed to f(...).');\n        util.assert(\n            grads.every(t => t instanceof Tensor),\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.gradFunc` is a function that returns ' +\n                'a list of only tensors.');\n        const gradMap: {[key: string]: () => Tensor} = {};\n        grads.forEach((grad, i) => {\n          gradMap[i] = () => grad;\n        });\n        return gradMap;\n      };\n\n      return this.runKernelFunc({\n        forwardFunc,\n        backwardsFunc,\n        inputs: inputMap,\n      });\n    };\n  }\n\n  readSync(dataId: DataId): BackendValues {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readSync(dataId);\n  }\n  read(dataId: DataId): Promise<BackendValues> {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.read(dataId);\n  }\n\n  readToGPU(dataId: DataId, options?: DataToGPUOptions): GPUData {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readToGPU(dataId, options);\n  }\n\n  async time(query: () => void): Promise<TimingInfo> {\n    const start = now();\n    const timingInfo = await this.backend.time(query) as TimingInfo;\n    timingInfo.wallMs = now() - start;\n    return timingInfo;\n  }\n\n  /**\n   * Tracks a Tensor in the current scope to be automatically cleaned up\n   * when the current scope ends, and returns the value.\n   *\n   * @param result The Tensor to track in the current scope.\n   */\n  private track<T extends Tensor>(result: T): T {\n    if (this.state.activeScope != null) {\n      result.scopeId = this.state.activeScope.id;\n      this.state.activeScope.track.push(result);\n    }\n\n    return result;\n  }\n\n  get registeredVariables(): NamedVariableMap {\n    return this.state.registeredVariables;\n  }\n\n  /**\n   * Resets the engine state. Removes all backends but does not remove\n   * registered backend factories.\n   */\n  reset(): void {\n    // Make any pending promise obsolete.\n    this.pendingBackendInitId++;\n\n    this.state.dispose();\n    this.ENV.reset();\n    this.state = new EngineState();\n\n    for (const backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n    this.backendName = null;\n    this.backendInstance = null;\n    this.pendingBackendInit = null;\n  }\n}\n\nfunction ones(shape: number[]): Tensor {\n  const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n  return ENGINE.makeTensor(values, shape, 'float32');\n}\n\nexport function getOrMakeEngine(): Engine {\n  const ns = getGlobalNamespace() as {} as {_tfengine: Engine};\n  if (ns._tfengine == null) {\n    const environment = new Environment(ns);\n    ns._tfengine = new Engine(environment);\n  }\n  setEnvironmentGlobal(ns._tfengine.ENV);\n\n  // Tell the current tensor interface that the global engine is responsible\n  // for tracking.\n  setTensorTracker(() => ns._tfengine);\n  return ns._tfengine;\n}\n\nexport const ENGINE = getOrMakeEngine();\n\n/**\n * A implementation of the add op for use within engine and tape.\n *\n * This allows us to avoid a circular dependency between add.ts and engine.\n * It is exported to be available in tape tests.\n */\nexport function add(a: Tensor, b: Tensor): Tensor {\n  // We duplicate Add here to avoid a circular dependency with add.ts.\n  const inputs = {a, b};\n  return ENGINE.runKernel(Add, inputs as {} as NamedTensorMap);\n}\n","/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// tslint:disable-next-line:no-any\nfunction _isNavigatorDefined(): boolean {\n  return typeof navigator !== 'undefined' && navigator != null;\n}\n\nlet isMobileMockValue: boolean|undefined;\n\nexport function mockIsMobile(value: boolean|undefined) {\n  isMobileMockValue = value;\n}\n\nexport function isMobile(nav?: Navigator): boolean {\n  if (isMobileMockValue !== undefined) {\n    return isMobileMockValue;\n  }\n  if (nav || _isNavigatorDefined()) {\n    if (!nav) {\n      nav = navigator;\n    }\n    if (nav.product === 'ReactNative') {\n      return true;\n    }\n\n    const a = nav.userAgent || nav.vendor ||\n        // tslint:disable-next-line:no-any\n        (typeof window !== 'undefined' ? (window as any).opera : '');\n    // Use `navigator.userAgentData.mobile` as fallback.\n    if (!a) {\n      // tslint:disable-next-line:no-any\n      const navAny = nav as any;\n      return navAny.userAgentData && navAny.userAgentData.mobile;\n    }\n    // tslint:disable-next-line:max-line-length\n    return /(android|bb\\d+|meego).+mobile|avantgo|bada\\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\\.(browser|link)|vodafone|wap|windows ce|xda|xiino/i\n               .test(a) ||\n        // tslint:disable-next-line:max-line-length\n        /1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\\-(n|u)|c55\\/|capi|ccwa|cdm\\-|cell|chtm|cldc|cmd\\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\\-s|devi|dica|dmob|do(c|p)o|ds(12|\\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\\-|_)|g1 u|g560|gene|gf\\-5|g\\-mo|go(\\.w|od)|gr(ad|un)|haie|hcit|hd\\-(m|p|t)|hei\\-|hi(pt|ta)|hp( i|ip)|hs\\-c|ht(c(\\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\\-(20|go|ma)|i230|iac( |\\-|\\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\\/)|klon|kpt |kwc\\-|kyo(c|k)|le(no|xi)|lg( g|\\/(k|l|u)|50|54|\\-[a-w])|libw|lynx|m1\\-w|m3ga|m50\\/|ma(te|ui|xo)|mc(01|21|ca)|m\\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\\-2|po(ck|rt|se)|prox|psio|pt\\-g|qa\\-a|qc(07|12|21|32|60|\\-[2-7]|i\\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\\-|oo|p\\-)|sdk\\/|se(c(\\-|0|1)|47|mc|nd|ri)|sgh\\-|shar|sie(\\-|m)|sk\\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\\-|v\\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\\-|tdg\\-|tel(i|m)|tim\\-|t\\-mo|to(pl|sh)|ts(70|m\\-|m3|m5)|tx\\-9|up(\\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\\-|your|zeto|zte\\-/i\n            .test(a.substr(0, 4));\n  }\n  return false;\n}\n\nexport function isBrowser(): boolean {\n  return (typeof window !== 'undefined' && window.document != null) ||\n      //@ts-ignore\n      (typeof WorkerGlobalScope !== 'undefined');\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport './engine';\n\nimport * as device_util from './device_util';\nimport {env} from './environment';\n\nconst ENV = env();\n\n/**\n * This file contains environment-related flag registrations.\n */\n\n/** Whether to enable debug mode. */\nENV.registerFlag('DEBUG', () => false, debugValue => {\n  if (debugValue) {\n    console.warn(\n        'Debugging mode is ON. The output of every math call will ' +\n        'be downloaded to CPU and checked for NaNs. ' +\n        'This significantly impacts performance.');\n  }\n});\n\n/** Whether we are in a browser (as versus, say, node.js) environment. */\nENV.registerFlag('IS_BROWSER', () => device_util.isBrowser());\n\n/** Whether we are in a browser (as versus, say, node.js) environment. */\nENV.registerFlag(\n    'IS_NODE',\n    () => (typeof process !== 'undefined') &&\n        (typeof process.versions !== 'undefined') &&\n        (typeof process.versions.node !== 'undefined'));\n\n/** Whether this browser is Chrome. */\nENV.registerFlag(\n    'IS_CHROME',\n    () => typeof navigator !== 'undefined' && navigator != null &&\n        navigator.userAgent != null && /Chrome/.test(navigator.userAgent) &&\n        /Google Inc/.test(navigator.vendor));\n\n/**\n * True when the environment is \"production\" where we disable safety checks\n * to gain performance.\n */\nENV.registerFlag('PROD', () => false);\n\n/**\n * Whether to do sanity checks when inferring a shape from user-provided\n * values, used when creating a new tensor.\n */\nENV.registerFlag(\n    'TENSORLIKE_CHECK_SHAPE_CONSISTENCY', () => ENV.getBool('DEBUG'));\n\n/** Whether deprecation warnings are enabled. */\nENV.registerFlag('DEPRECATION_WARNINGS_ENABLED', () => true);\n\n/** True if running unit tests. */\nENV.registerFlag('IS_TEST', () => false);\n\n/** Whether to check computation result for errors. */\nENV.registerFlag('CHECK_COMPUTATION_FOR_ERRORS', () => true);\n\n/** Whether the backend needs to wrap input to imageBitmap. */\nENV.registerFlag('WRAP_TO_IMAGEBITMAP', () => false);\n\n/** Experimental flag, whether enter compile only phase. */\nENV.registerFlag('ENGINE_COMPILE_ONLY', () => false);\n\n/** Whether to enable canvas2d willReadFrequently for GPU backends */\nENV.registerFlag('CANVAS2D_WILL_READ_FREQUENTLY_FOR_GPU', () => false);\n\n/** Whether to use setTimeoutCustom */\nENV.registerFlag('USE_SETTIMEOUTCUSTOM', () => false);\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from './engine';\nimport {env} from './environment';\nimport {Tensor} from './tensor';\nimport {DataType, TensorLike} from './types';\nimport {assert, flatten, inferDtype, isTypedArray, toTypedArray} from './util';\n\nexport function inferShape(val: TensorLike, dtype?: DataType): number[] {\n  let firstElem: typeof val = val;\n\n  if (isTypedArray(val)) {\n    return dtype === 'string' ? [] : [val.length];\n  }\n  if (!Array.isArray(val)) {\n    return [];  // Scalar.\n  }\n  const shape: number[] = [];\n\n  while (Array.isArray(firstElem) ||\n         isTypedArray(firstElem) && dtype !== 'string') {\n    shape.push(firstElem.length);\n    firstElem = firstElem[0];\n  }\n  if (Array.isArray(val) &&\n      env().getBool('TENSORLIKE_CHECK_SHAPE_CONSISTENCY')) {\n    deepAssertShapeConsistency(val, shape, []);\n  }\n\n  return shape;\n}\n\nfunction deepAssertShapeConsistency(\n    val: TensorLike, shape: number[], indices: number[]) {\n  indices = indices || [];\n  if (!(Array.isArray(val)) && !isTypedArray(val)) {\n    assert(\n        shape.length === 0,\n        () => `Element arr[${indices.join('][')}] is a primitive, ` +\n            `but should be an array/TypedArray of ${shape[0]} elements`);\n    return;\n  }\n  assert(\n      shape.length > 0,\n      () => `Element arr[${indices.join('][')}] should be a primitive, ` +\n          `but is an array of ${val.length} elements`);\n  assert(\n      val.length === shape[0],\n      () => `Element arr[${indices.join('][')}] should have ${shape[0]} ` +\n          `elements, but has ${val.length} elements`);\n  const subShape = shape.slice(1);\n  for (let i = 0; i < val.length; ++i) {\n    deepAssertShapeConsistency(val[i], subShape, indices.concat(i));\n  }\n}\n\nfunction assertDtype(\n    expectedDtype: DataType|'numeric'|'string_or_numeric',\n    actualDType: DataType, argName: string, functionName: string) {\n  if (expectedDtype === 'string_or_numeric') {\n    return;\n  }\n  if (expectedDtype == null) {\n    throw new Error(`Expected dtype cannot be null.`);\n  }\n  if (expectedDtype !== 'numeric' && expectedDtype !== actualDType ||\n      expectedDtype === 'numeric' && actualDType === 'string') {\n    throw new Error(\n        `Argument '${argName}' passed to '${functionName}' must ` +\n        `be ${expectedDtype} tensor, but got ${actualDType} tensor`);\n  }\n}\n\nexport function convertToTensor<T extends Tensor>(\n    x: T|TensorLike, argName: string, functionName: string,\n    parseAsDtype: DataType|'numeric'|'string_or_numeric' = 'numeric'): T {\n  if (x instanceof Tensor) {\n    assertDtype(parseAsDtype, x.dtype, argName, functionName);\n    return x;\n  }\n  let inferredDtype = inferDtype(x);\n  // If the user expects a bool/int/float, use that info to update the\n  // inferredDtype when it is not a string.\n  if (inferredDtype !== 'string' &&\n      ['bool', 'int32', 'float32'].indexOf(parseAsDtype) >= 0) {\n    inferredDtype = parseAsDtype as DataType;\n  }\n  assertDtype(parseAsDtype, inferredDtype, argName, functionName);\n\n  if ((x == null) ||\n      (!isTypedArray(x) && !Array.isArray(x) && typeof x !== 'number' &&\n       typeof x !== 'boolean' && typeof x !== 'string')) {\n    const type = x == null ? 'null' : (x as {}).constructor.name;\n    throw new Error(\n        `Argument '${argName}' passed to '${functionName}' must be a ` +\n        `Tensor or TensorLike, but got '${type}'`);\n  }\n  const inferredShape = inferShape(x, inferredDtype);\n  if (!isTypedArray(x) && !Array.isArray(x)) {\n    x = [x] as number[];\n  }\n  const skipTypedArray = true;\n  const values = inferredDtype !== 'string' ?\n      toTypedArray(x, inferredDtype as DataType) :\n      flatten(x as string[], [], skipTypedArray) as string[];\n  return ENGINE.makeTensor(values, inferredShape, inferredDtype) as T;\n}\n\nexport function convertToTensorArray<T extends Tensor>(\n    arg: Array<T|TensorLike>, argName: string, functionName: string,\n    parseAsDtype: DataType|'numeric'|'string_or_numeric' = 'numeric'): T[] {\n  if (!Array.isArray(arg)) {\n    throw new Error(\n        `Argument ${argName} passed to ${functionName} must be a ` +\n        '`Tensor[]` or `TensorLike[]`');\n  }\n  const tensors = arg as T[];\n  return tensors.map(\n      (t, i) =>\n          convertToTensor(t, `${argName}[${i}]`, functionName, parseAsDtype));\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {ENGINE} from '../engine';\nimport {isPromise} from '../util';\n\nexport const OP_SCOPE_SUFFIX = '__op';\n\n/**\n * Used for wrapping functions that perform math operations on\n * Tensors. The function will be wrapped in a named scope that cleans all\n * memory usage after the function is done.\n */\nexport function op<T extends Function>(f: {[name: string]: T}): T {\n  const keys = Object.keys(f);\n  if (keys.length !== 1) {\n    throw new Error(\n        `Please provide an object with a single key ` +\n        `(operation name) mapping to a function. Got an object with ` +\n        `${keys.length} keys.`);\n  }\n\n  let opName = keys[0];\n  const fn = f[opName];\n\n  // Strip the underscore from the end of the function name.\n  if (opName.endsWith('_')) {\n    opName = opName.substring(0, opName.length - 1);\n  }\n\n  // add an __op suffix to distinguish ops from kernels in tf.profile\n  opName = opName + OP_SCOPE_SUFFIX;\n\n  // tslint:disable-next-line:no-any\n  const f2 = (...args: any[]) => {\n    ENGINE.startScope(opName);\n    try {\n      const result = fn(...args);\n      if (isPromise(result)) {\n        console.error('Cannot return a Promise inside of tidy.');\n      }\n      ENGINE.endScope(result);\n      return result;\n    } catch (ex) {\n      ENGINE.endScope(null);\n      throw ex;\n    }\n  };\n  Object.defineProperty(f2, 'name', {value: opName, configurable: true});\n\n  // tslint:disable-next-line:no-any\n  return f2 as any as T;\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {ENGINE} from '../engine';\nimport {Complex, ComplexInputs} from '../kernel_names';\nimport {Tensor} from '../tensor';\nimport {NamedTensorMap} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {TensorLike} from '../types';\nimport * as util from '../util';\n\nimport {op} from './operation';\n\n/**\n * Converts two real numbers to a complex number.\n *\n * Given a tensor `real` representing the real part of a complex number, and a\n * tensor `imag` representing the imaginary part of a complex number, this\n * operation returns complex numbers elementwise of the form [r0, i0, r1, i1],\n * where r represents the real part and i represents the imag part.\n *\n * The input tensors real and imag must have the same shape.\n *\n * ```js\n * const real = tf.tensor1d([2.25, 3.25]);\n * const imag = tf.tensor1d([4.75, 5.75]);\n * const complex = tf.complex(real, imag);\n *\n * complex.print();\n * ```\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nfunction complex_<T extends Tensor>(real: T|TensorLike, imag: T|TensorLike): T {\n  const $real = convertToTensor(real, 'real', 'complex');\n  const $imag = convertToTensor(imag, 'imag', 'complex');\n  util.assertShapesMatch(\n      $real.shape, $imag.shape,\n      `real and imag shapes, ${$real.shape} and ${$imag.shape}, ` +\n          `must match in call to tf.complex().`);\n\n  const inputs: ComplexInputs = {real: $real, imag: $imag};\n  return ENGINE.runKernel(Complex, inputs as {} as NamedTensorMap);\n}\n\nexport const complex = op({complex_});\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {Tensor} from '../tensor';\nimport {TensorLike, TypedArray} from '../types';\nimport {DataType} from '../types';\nimport {assert, assertNonNegativeIntegerDimensions, flatten, inferDtype, isTypedArray, sizeFromShape, toTypedArray} from '../util';\n\n/** This is shared code across all tensor creation methods. */\nexport function makeTensor(\n    values: TensorLike, shape: number[], inferredShape: number[],\n    dtype?: DataType): Tensor {\n  if (dtype == null) {\n    dtype = inferDtype(values);\n  }\n  if (dtype === 'complex64') {\n    throw new Error(\n        `Cannot construct a complex64 tensor directly. ` +\n        `Please use tf.complex(real, imag).`);\n  }\n  if (!isTypedArray(values) && !Array.isArray(values) &&\n      typeof values !== 'number' && typeof values !== 'boolean' &&\n      typeof values !== 'string') {\n    throw new Error(\n        'values passed to tensor(values) must be a number/boolean/string or ' +\n        'an array of numbers/booleans/strings, or a TypedArray');\n  }\n  if (shape != null) {\n    assertNonNegativeIntegerDimensions(shape);\n\n    const providedSize = sizeFromShape(shape);\n    const inferredSize = sizeFromShape(inferredShape);\n    assert(\n        providedSize === inferredSize,\n        () =>\n            `Based on the provided shape, [${shape}], the tensor should have ` +\n            `${providedSize} values but has ${inferredSize}`);\n\n    for (let i = 0; i < inferredShape.length; ++i) {\n      const inferred = inferredShape[i];\n      const flatDimsDontMatch = i === inferredShape.length - 1 ?\n          inferred !== sizeFromShape(shape.slice(i)) :\n          true;\n      assert(\n          inferredShape[i] === shape[i] || !flatDimsDontMatch,\n          () => `Error creating a new Tensor. Inferred shape ` +\n              `(${inferredShape}) does not match the provided ` +\n              `shape (${shape}). `);\n    }\n  }\n\n  if (!isTypedArray(values) && !Array.isArray(values)) {\n    values = [values] as number[];\n  }\n\n  shape = shape || inferredShape;\n  values = dtype !== 'string' ?\n      toTypedArray(values, dtype) :\n      flatten(values as string[], [], true) as string[];\n  return ENGINE.makeTensor(values as TypedArray, shape, dtype);\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Tensor} from '../tensor';\nimport {inferShape} from '../tensor_util_env';\nimport {TensorLike} from '../types';\nimport {DataType, Rank, ShapeMap} from '../types';\n\nimport {makeTensor} from './tensor_ops_util';\n\n/**\n * Creates a `tf.Tensor` with the provided values, shape and dtype.\n *\n * ```js\n * // Pass an array of values to create a vector.\n * tf.tensor([1, 2, 3, 4]).print();\n * ```\n *\n * ```js\n * // Pass a nested array of values to make a matrix or a higher\n * // dimensional tensor.\n * tf.tensor([[1, 2], [3, 4]]).print();\n * ```\n *\n * ```js\n * // Pass a flat array and specify a shape yourself.\n * tf.tensor([1, 2, 3, 4], [2, 2]).print();\n * ```\n *\n * @param values The values of the tensor. Can be nested array of numbers,\n *     or a flat array, or a `TypedArray`. If the values are strings,\n *     they will be encoded as utf-8 and kept as `Uint8Array[]`.\n * @param shape The shape of the tensor. Optional. If not provided,\n *   it is inferred from `values`.\n * @param dtype The data type.\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nexport function tensor<R extends Rank>(\n    values: TensorLike, shape?: ShapeMap[R], dtype?: DataType): Tensor<R> {\n  const inferredShape = inferShape(values, dtype);\n  return makeTensor(values, shape, inferredShape, dtype) as Tensor<R>;\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n/* Type definitions for exporting and importing of models. */\n\n/**\n * A map from Tensor dtype to number of bytes per element of the Tensor.\n */\nexport const DTYPE_VALUE_SIZE_MAP: {[dtype: string]: number} = {\n  'float32': 4,\n  'float16': 2,\n  'int32': 4,\n  'uint16': 2,\n  'uint8': 1,\n  'bool': 1,\n  'complex64': 8\n};\n\n/**\n * A weight manifest.\n *\n * The weight manifest consists of an ordered list of weight-manifest groups.\n * Each weight-manifest group (\"group\" for short hereafter) consists of a\n * number of weight values stored in a number of paths.\n * See the documentation of `WeightManifestGroupConfig` below for more details.\n */\nexport declare type WeightsManifestConfig = WeightsManifestGroupConfig[];\n\n/**\n * A weight-manifest group.\n *\n * Consists of an ordered list of weight values encoded in binary format,\n * stored in an ordered list of paths.\n */\nexport declare interface WeightsManifestGroupConfig {\n  /**\n   * An ordered list of paths.\n   *\n   * Paths are intentionally abstract in order to be general. For example, they\n   * can be relative URL paths or relative paths on the file system.\n   */\n  paths: string[];\n\n  /**\n   * Specifications of the weights stored in the paths.\n   */\n  weights: WeightsManifestEntry[];\n}\n\n/**\n * Group to which the weight belongs.\n *\n * - 'optimizer': Weight from a stateful optimizer.\n */\nexport type WeightGroup = 'model'|'optimizer';\n\n/**\n * An entry in the weight manifest.\n *\n * The entry contains specification of a weight.\n */\nexport declare interface WeightsManifestEntry {\n  /**\n   * Name of the weight, e.g., 'Dense_1/bias'\n   */\n  name: string;\n\n  /**\n   * Shape of the weight.\n   */\n  shape: number[];\n\n  /**\n   * Data type of the weight.\n   */\n  dtype: 'float32'|'int32'|'bool'|'string'|'complex64';\n\n  /**\n   * Type of the weight.\n   *\n   * Optional.\n   *\n   * The value 'optimizer' indicates the weight belongs to an optimizer\n   * (i.e., used only during model training and not during inference).\n   */\n  group?: WeightGroup;\n\n  /**\n   * Information for dequantization of the weight.\n   */\n  quantization?: {\n    scale?: number,  // The scaling constant to multiply by.\n    min?: number,    // The (possibly nudged) minimum weight to add.\n       dtype: 'uint16'|'uint8'|'float16'  // The dtype of the quantized weights.\n  };\n}\n\n/**\n * Options for saving a model.\n * @innamespace io\n */\nexport interface SaveConfig {\n  /**\n   * Whether to save only the trainable weights of the model, ignoring the\n   * non-trainable ones.\n   */\n  trainableOnly?: boolean;\n\n  /**\n   * Whether the optimizer will be saved (if exists).\n   *\n   * Default: `false`.\n   */\n  includeOptimizer?: boolean;\n}\n\n/**\n * Result of a saving operation.\n */\nexport interface SaveResult {\n  /**\n   * Information about the model artifacts saved.\n   */\n  modelArtifactsInfo: ModelArtifactsInfo;\n\n  /**\n   * HTTP responses from the server that handled the model-saving request (if\n   * any). This is applicable only to server-based saving routes.\n   */\n  responses?: Response[];\n\n  /**\n   * Error messages and related data (if any).\n   */\n  errors?: Array<{}|string>;\n}\n\nexport declare interface ModelArtifactsInfo {\n  /**\n   * Timestamp for when the model is saved.\n   */\n  dateSaved: Date;\n\n  /**\n   * TODO (cais,yassogba) consider removing GraphDef as GraphDefs now\n   * come in a JSON format and none of our IOHandlers support a non json\n   * format. We could conder replacing this with 'Binary' if we want to\n   * allow future handlers to save to non json formats (though they will\n   * probably want more information than 'Binary').\n   * Type of the model topology\n   *\n   * Type of the model topology\n   *\n   * Possible values:\n   *   - JSON: JSON config (human-readable, e.g., Keras JSON).\n   *   - GraphDef: TensorFlow\n   *     [GraphDef](https://www.tensorflow.org/extend/tool_developers/#graphdef)\n   *     protocol buffer (binary).\n   */\n  modelTopologyType: 'JSON'|'GraphDef';\n\n  /**\n   * Size of model topology (Keras JSON or GraphDef), in bytes.\n   */\n  modelTopologyBytes?: number;\n\n  /**\n   * Size of weight specification or manifest, in bytes.\n   */\n  weightSpecsBytes?: number;\n\n  /**\n   * Size of weight value data, in bytes.\n   */\n  weightDataBytes?: number;\n}\n\n/** Model training configuration. */\nexport declare interface TrainingConfig {\n  // TODO(cais): Tighten the typing once keras spec is available to tfjs-core.\n  // See\n  // tslint:disable-next-line:max-line-length\n  // https://github.com/tensorflow/tfjs-layers/blob/master/src/keras_format/training_config.ts\n  /** Optimizer used for the model training. */\n  optimizer_config: {};\n\n  // TODO(cais): Tighten the typing once keras spec is available to tfjs-core.\n  /** Loss function(s) for the model's output(s). */\n  loss: string|string[]|{[key: string]: string};\n\n  // TODO(cais): Tighten the typing once keras spec is available to tfjs-core.\n  /** Metric function(s) for the model's output(s). */\n  metrics?: string[]|{[key: string]: string};\n\n  // TODO(cais): Tighten the typing once keras spec is available to tfjs-core.\n  weighted_metrics?: string[];\n\n  // TODO(cais): Tighten the typing once keras spec is available to tfjs-core.\n  sample_weight_mode?: string;\n\n  loss_weights?: number[]|{[key: string]: number};\n}\n\n/**\n * The serialized artifacts of a model, including topology and weights.\n *\n * The `modelTopology`, `trainingConfig`, `weightSpecs` and `weightData` fields\n * of this interface are optional, in order to support topology- or weights-only\n * saving and loading.\n *\n * Note this interface is used internally in IOHandlers.  For the file format\n * written to disk as `model.json`, see `ModelJSON`.\n */\nexport declare interface ModelArtifacts {\n  /**\n   * Model topology.\n   *\n   * For Keras-style `tf.Model`s, this is a JSON object.\n   * For TensorFlow-style models (e.g., `SavedModel`), this is the JSON\n   * encoding of the `GraphDef` protocol buffer.\n   */\n  modelTopology?: {}|ArrayBuffer;\n\n  /**\n   * Serialized configuration for the model's training.\n   */\n  trainingConfig?: TrainingConfig;\n\n  /**\n   * Weight specifications.\n   *\n   * This corresponds to the weightsData below.\n   */\n  weightSpecs?: WeightsManifestEntry[];\n\n  /**\n   * Binary buffer for all weight values concatenated in the order specified\n   * by `weightSpecs`.\n   */\n  weightData?: ArrayBuffer;\n\n  /**\n   * Hard-coded format name for models saved from TensorFlow.js or converted\n   * by TensorFlow.js Converter.\n   */\n  format?: string;\n\n  /**\n   * What library is responsible for originally generating this artifact.\n   *\n   * Used for debugging purposes. E.g., 'TensorFlow.js v1.0.0'.\n   */\n  generatedBy?: string;\n\n  /**\n   * What library or tool is responsible for converting the original model\n   * to this format, applicable only if the model is output by a converter.\n   *\n   * Used for debugging purposes.  E.g., 'TensorFlow.js Converter v1.0.0'.\n   *\n   * A value of `null` means the model artifacts are generated without any\n   * conversion process (e.g., saved directly from a TensorFlow.js\n   * `tf.LayersModel` instance.)\n   */\n  convertedBy?: string|null;\n\n  /**\n   * Inputs and outputs signature for saved model.\n   */\n  signature?: {};\n\n  /**\n   * User-defined metadata about the model.\n   */\n  userDefinedMetadata?: {[key: string]: {}};\n\n  /**\n   * Initializer for the model.\n   */\n  modelInitializer?: {};\n}\n\n/**\n * The on-disk format of the `model.json` file.\n *\n * TF.js 1.0 always populates the optional fields when writing model.json.\n * Prior versions did not provide those fields.\n */\nexport declare interface ModelJSON {\n  /**\n   * Model topology.\n   *\n   * For Keras-style `tf.Model`s, this is a JSON object.\n   * For TensorFlow-style models (e.g., `SavedModel`), this is the JSON\n   * encoding of the `GraphDef` protocol buffer.\n   */\n  modelTopology: {};\n\n  /** Model training configuration. */\n  trainingConfig?: TrainingConfig;\n\n  /**\n   * Weights manifest.\n   *\n   * The weights manifest consists of an ordered list of weight-manifest\n   * groups. Each weight-manifest group consists of a number of weight values\n   * stored in a number of paths. See the documentation of\n   * `WeightsManifestConfig` for more details.\n   */\n  weightsManifest: WeightsManifestConfig;\n\n  /**\n   * Hard-coded format name for models saved from TensorFlow.js or converted\n   * by TensorFlow.js Converter.\n   */\n  format?: string;\n\n  /**\n   * What library is responsible for originally generating this artifact.\n   *\n   * Used for debugging purposes. E.g., 'TensorFlow.js v1.0.0'.\n   */\n  generatedBy?: string;\n\n  /**\n   * What library or tool is responsible for converting the original model\n   * to this format, applicable only if the model is output by a converter.\n   *\n   * Used for debugging purposes.  E.g., 'TensorFlow.js Converter v1.0.0'.\n   *\n   * A value of `null` means the model artifacts are generated without any\n   * conversion process (e.g., saved directly from a TensorFlow.js\n   * `tf.LayersModel` instance.)\n   */\n  convertedBy?: string|null;\n\n  /**\n   * Inputs and outputs signature for saved model.\n   */\n  signature?: {};\n\n  /**\n   * User-defined metadata about the model.\n   */\n  userDefinedMetadata?: {[key: string]: {}};\n\n  /**\n   * Initializer for the model.\n   */\n  modelInitializer?: {};\n}\n\n/**\n * Type definition for handlers of loading operations.\n */\nexport type LoadHandler = () => Promise<ModelArtifacts>;\n\n/**\n * Type definition for handlers of saving operations.\n */\nexport type SaveHandler = (modelArtifact: ModelArtifacts) =>\n    Promise<SaveResult>;\n\n/**\n * Interface for a model import/export handler.\n *\n * The `save` and `load` handlers are both optional, in order to allow handlers\n * that support only saving or loading.\n */\n// tslint:disable-next-line:interface-name\nexport interface IOHandler {\n  save?: SaveHandler;\n  load?: LoadHandler;\n}\n\n/**\n * Type definition for handlers of synchronous loading operations.\n */\nexport type LoadHandlerSync = () => ModelArtifacts;\n\n/**\n * Type definition for handlers of synchronous saving operations.\n */\nexport type SaveHandlerSync = (modelArtifact: ModelArtifacts) => SaveResult;\n\n/**\n * Interface for a synchronous model import/export handler.\n *\n * The `save` and `load` handlers are both optional, in order to allow handlers\n * that support only saving or loading.\n */\n// tslint:disable-next-line:interface-name\nexport type IOHandlerSync = {\n  save?: SaveHandlerSync;\n  load?: LoadHandlerSync;\n};\n\n/**\n * An interface for the manager of a model store.\n *\n * A model store is defined as a storage medium on which multiple models can\n * be stored. Each stored model has a unique `path` as its identifier.\n * A `ModelStoreManager` for the store allows actions including\n *\n * - Listing the models stored in the store.\n * - Deleting a model from the store.\n */\nexport interface ModelStoreManager {\n  /**\n   * List all models in the model store.\n   *\n   * @returns A dictionary mapping paths of existing models to their\n   *   model artifacts info. Model artifacts info include type of the model's\n   *   topology, byte sizes of the topology, weights, etc.\n   */\n  listModels(): Promise<{[path: string]: ModelArtifactsInfo}>;\n\n  /**\n   * Remove a model specified by `path`.\n   *\n   * @param path\n   * @returns ModelArtifactsInfo of the deleted model (if and only if deletion\n   *   is successful).\n   * @throws Error if deletion fails, e.g., if no model exists at `path`.\n   */\n  removeModel(path: string): Promise<ModelArtifactsInfo>;\n}\n\n/**\n * Callback for the progress of a long-running action such as an HTTP\n * request for a large binary object.\n *\n * `fraction` should be a number in the [0, 1] interval, indicating how\n * much of the action has completed.\n */\nexport type OnProgressCallback = (fraction: number) => void;\n\n/** @innamespace io */\nexport interface LoadOptions {\n  /**\n   * RequestInit (options) for HTTP requests.\n   *\n   * For detailed information on the supported fields, see\n   * [https://developer.mozilla.org/en-US/docs/Web/API/Request/Request](\n   *     https://developer.mozilla.org/en-US/docs/Web/API/Request/Request)\n   */\n  requestInit?: RequestInit;\n\n  /**\n   * Progress callback.\n   */\n  onProgress?: OnProgressCallback;\n\n  /**\n   * A function used to override the `window.fetch` function.\n   */\n  fetchFunc?: Function;\n\n  /**\n   * Strict loading model: whether extraneous weights or missing\n   * weights should trigger an `Error`.\n   *\n   * If `true`, require that the provided weights exactly match those\n   * required by the layers. `false` means that both extra weights\n   * and missing weights will be silently ignored.\n   *\n   * Default: `true`.\n   */\n  strict?: boolean;\n\n  /**\n   * Path prefix for weight files, by default this is calculated from the\n   * path of the model JSON file.\n   *\n   * For instance, if the path to the model JSON file is\n   * `http://localhost/foo/model.json`, then the default path prefix will be\n   * `http://localhost/foo/`. If a weight file has the path value\n   * `group1-shard1of2` in the weight manifest, then the weight file will be\n   * loaded from `http://localhost/foo/group1-shard1of2` by default. However,\n   * if you provide a `weightPathPrefix` value of\n   * `http://localhost/foo/alt-weights`, then the weight file will be loaded\n   * from the path `http://localhost/foo/alt-weights/group1-shard1of2` instead.\n   */\n  weightPathPrefix?: string;\n\n  /**\n   * Whether the module or model is to be loaded from TF Hub.\n   *\n   * Setting this to `true` allows passing a TF-Hub module URL, omitting the\n   * standard model file name and the query parameters.\n   *\n   * Default: `false`.\n   */\n  fromTFHub?: boolean;\n\n  /**\n   * An async function to convert weight file name to URL. The weight file\n   * names are stored in model.json's weightsManifest.paths field. By default we\n   * consider weight files are colocated with the model.json file. For example:\n   *     model.json URL: https://www.google.com/models/1/model.json\n   *     group1-shard1of1.bin url:\n   *        https://www.google.com/models/1/group1-shard1of1.bin\n   *\n   * With this func you can convert the weight file name to any URL.\n   */\n  weightUrlConverter?: (weightFileName: string) => Promise<string>;\n}\n\n/**\n * Additional options for Platform.fetch\n */\nexport interface RequestDetails {\n  /**\n   * Is this request for a binary file (as opposed to a json file)\n   */\n  isBinary?: boolean;\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {complex} from '../ops/complex';\nimport {tensor} from '../ops/tensor';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\nimport {TypedArray} from '../types';\nimport {sizeFromShape} from '../util';\n\nimport {DTYPE_VALUE_SIZE_MAP, ModelArtifacts, ModelArtifactsInfo, ModelJSON, WeightGroup, WeightsManifestConfig, WeightsManifestEntry} from './types';\n\n/** Number of bytes reserved for the length of the string. (32bit integer). */\nconst NUM_BYTES_STRING_LENGTH = 4;\n\n/**\n * Encode a map from names to weight values as an ArrayBuffer, along with an\n * `Array` of `WeightsManifestEntry` as specification of the encoded weights.\n *\n * This function does not perform sharding.\n *\n * This function is the reverse of `decodeWeights`.\n *\n * @param tensors A map (\"dict\") from names to tensors.\n * @param group Group to which the weights belong (optional).\n * @returns A `Promise` of\n *   - A flat `ArrayBuffer` with all the binary values of the `Tensor`s\n *     concatenated.\n *   - An `Array` of `WeightManifestEntry`s, carrying information including\n *     tensor names, `dtype`s and shapes.\n * @throws Error: on unsupported tensor `dtype`.\n */\nexport async function encodeWeights(\n    tensors: NamedTensorMap|NamedTensor[], group?: WeightGroup):\n    Promise<{data: ArrayBuffer, specs: WeightsManifestEntry[]}> {\n  // TODO(adarob, cais): Support quantization.\n  const specs: WeightsManifestEntry[] = [];\n  const dataPromises: Array<Promise<TypedArray>> = [];\n\n  const names: string[] = Array.isArray(tensors) ?\n      tensors.map(tensor => tensor.name) :\n      Object.keys(tensors);\n\n  for (let i = 0; i < names.length; ++i) {\n    const name = names[i];\n    const t = Array.isArray(tensors) ? tensors[i].tensor : tensors[name];\n    if (t.dtype !== 'float32' && t.dtype !== 'int32' && t.dtype !== 'bool' &&\n        t.dtype !== 'string' && t.dtype !== 'complex64') {\n      throw new Error(`Unsupported dtype in weight '${name}': ${t.dtype}`);\n    }\n    const spec: WeightsManifestEntry = {name, shape: t.shape, dtype: t.dtype};\n    if (t.dtype === 'string') {\n      const utf8bytes = new Promise<TypedArray>(async resolve => {\n        const vals = await t.bytes() as Uint8Array[];\n        const totalNumBytes = vals.reduce((p, c) => p + c.length, 0) +\n            NUM_BYTES_STRING_LENGTH * vals.length;\n        const bytes = new Uint8Array(totalNumBytes);\n        let offset = 0;\n        for (let i = 0; i < vals.length; i++) {\n          const val = vals[i];\n          const bytesOfLength =\n              new Uint8Array(new Uint32Array([val.length]).buffer);\n          bytes.set(bytesOfLength, offset);\n          offset += NUM_BYTES_STRING_LENGTH;\n          bytes.set(val, offset);\n          offset += val.length;\n        }\n        resolve(bytes);\n      });\n      dataPromises.push(utf8bytes);\n    } else {\n      dataPromises.push(t.data());\n    }\n    if (group != null) {\n      spec.group = group;\n    }\n    specs.push(spec);\n  }\n\n  const tensorValues = await Promise.all(dataPromises);\n  return {data: concatenateTypedArrays(tensorValues), specs};\n}\n\n/**\n * Decode flat ArrayBuffer as weights.\n *\n * This function does not handle sharding.\n *\n * This function is the reverse of `encodeWeights`.\n *\n * @param buffer A flat ArrayBuffer carrying the binary values of the tensors\n *   concatenated in the order specified in `specs`.\n * @param specs Specifications of the names, dtypes and shapes of the tensors\n *   whose value are encoded by `buffer`.\n * @return A map from tensor name to tensor value, with the names corresponding\n *   to names in `specs`.\n * @throws Error, if any of the tensors has unsupported dtype.\n */\nexport function decodeWeights(\n    buffer: ArrayBuffer, specs: WeightsManifestEntry[]): NamedTensorMap {\n  // TODO(adarob, cais): Support quantization.\n  const out: NamedTensorMap = {};\n  let float16Decode: (buffer: Uint16Array) => Float32Array | undefined;\n  let offset = 0;\n  for (const spec of specs) {\n    const name = spec.name;\n    const dtype = spec.dtype;\n    const shape = spec.shape;\n    const size = sizeFromShape(shape);\n    let values: TypedArray|string[]|Uint8Array[];\n\n    if ('quantization' in spec) {\n      const quantization = spec.quantization;\n      if (quantization.dtype === 'uint8' || quantization.dtype === 'uint16') {\n        if (!('min' in quantization && 'scale' in quantization)) {\n          throw new Error(\n              `Weight ${spec.name} with quantization ${quantization.dtype} ` +\n              `doesn't have corresponding metadata min and scale.`);\n        }\n      } else if (quantization.dtype === 'float16') {\n        if (dtype !== 'float32') {\n          throw new Error(\n              `Weight ${spec.name} is quantized with ${quantization.dtype} ` +\n              `which only supports weights of type float32 not ${dtype}.`);\n        }\n      } else {\n        throw new Error(\n            `Weight ${spec.name} has unknown ` +\n            `quantization dtype ${quantization.dtype}. ` +\n            `Supported quantization dtypes are: ` +\n            `'uint8', 'uint16', and 'float16'.`);\n      }\n      const quantizationSizeFactor = DTYPE_VALUE_SIZE_MAP[quantization.dtype];\n      const byteBuffer =\n          buffer.slice(offset, offset + size * quantizationSizeFactor);\n      const quantizedArray = (quantization.dtype === 'uint8') ?\n          new Uint8Array(byteBuffer) :\n          new Uint16Array(byteBuffer);\n      if (dtype === 'float32') {\n        if (quantization.dtype === 'uint8' || quantization.dtype === 'uint16') {\n          values = new Float32Array(quantizedArray.length);\n          for (let i = 0; i < quantizedArray.length; i++) {\n            const v = quantizedArray[i];\n            values[i] = v * quantization.scale + quantization.min;\n          }\n        } else if (quantization.dtype === 'float16') {\n          if (float16Decode === undefined) {\n            float16Decode = getFloat16Decoder();\n          }\n          values = float16Decode(quantizedArray as Uint16Array);\n        } else {\n          throw new Error(\n              `Unsupported quantization type ${quantization.dtype} ` +\n              `for weight type float32.`);\n        }\n      } else if (dtype === 'int32') {\n        if (quantization.dtype !== 'uint8' && quantization.dtype !== 'uint16') {\n          throw new Error(\n              `Unsupported quantization type ${quantization.dtype} ` +\n              `for weight type int32.`);\n        }\n        values = new Int32Array(quantizedArray.length);\n        for (let i = 0; i < quantizedArray.length; i++) {\n          const v = quantizedArray[i];\n          values[i] = Math.round(v * quantization.scale + quantization.min);\n        }\n      } else {\n        throw new Error(`Unsupported dtype in weight '${name}': ${dtype}`);\n      }\n      offset += size * quantizationSizeFactor;\n    } else if (dtype === 'string') {\n      const size = sizeFromShape(spec.shape);\n      values = [];\n      for (let i = 0; i < size; i++) {\n        const byteLength = new Uint32Array(\n            buffer.slice(offset, offset + NUM_BYTES_STRING_LENGTH))[0];\n        offset += NUM_BYTES_STRING_LENGTH;\n        const bytes = new Uint8Array(buffer.slice(offset, offset + byteLength));\n        (values as Uint8Array[]).push(bytes);\n        offset += byteLength;\n      }\n    } else {\n      const dtypeFactor = DTYPE_VALUE_SIZE_MAP[dtype];\n      const byteBuffer = buffer.slice(offset, offset + size * dtypeFactor);\n\n      if (dtype === 'float32') {\n        values = new Float32Array(byteBuffer);\n      } else if (dtype === 'int32') {\n        values = new Int32Array(byteBuffer);\n      } else if (dtype === 'bool') {\n        values = new Uint8Array(byteBuffer);\n      } else if (dtype === 'complex64') {\n        values = new Float32Array(byteBuffer);\n        const real = new Float32Array(values.length / 2);\n        const image = new Float32Array(values.length / 2);\n        for (let i = 0; i < real.length; i++) {\n          real[i] = values[i * 2];\n          image[i] = values[i * 2 + 1];\n        }\n        const realTensor = tensor(real, shape, 'float32');\n        const imageTensor = tensor(image, shape, 'float32');\n        out[name] = complex(realTensor, imageTensor);\n        realTensor.dispose();\n        imageTensor.dispose();\n      } else {\n        throw new Error(`Unsupported dtype in weight '${name}': ${dtype}`);\n      }\n      offset += size * dtypeFactor;\n    }\n    if (dtype !== 'complex64') {\n      out[name] = tensor(values, shape, dtype);\n    }\n  }\n  return out;\n}\n\n/**\n * Concatenate TypedArrays into an ArrayBuffer.\n */\nexport function concatenateTypedArrays(xs: TypedArray[]): ArrayBuffer {\n  // TODO(adarob, cais): Support quantization.\n  if (xs === null) {\n    throw new Error(`Invalid input value: ${JSON.stringify(xs)}`);\n  }\n\n  let totalByteLength = 0;\n\n  // `normalizedXs` is here for this reason: a `TypedArray`'s `buffer'\n  // can have a different byte length from that of the `TypedArray` itself,\n  // for example, when the `TypedArray` is created from an offset in an\n  // `ArrayBuffer`. `normliazedXs` holds `TypedArray`s whose `buffer`s match\n  // the `TypedArray` in byte length. If an element of `xs` does not show\n  // this property, a new `TypedArray` that satisfy this property will be\n  // constructed and pushed into `normalizedXs`.\n  const normalizedXs: TypedArray[] = [];\n  xs.forEach((x: TypedArray) => {\n    totalByteLength += x.byteLength;\n    // tslint:disable:no-any\n    normalizedXs.push(\n        x.byteLength === x.buffer.byteLength ? x :\n                                               new (x.constructor as any)(x));\n    if (!(x as any instanceof Float32Array || x as any instanceof Int32Array ||\n          x as any instanceof Uint8Array)) {\n      throw new Error(`Unsupported TypedArray subtype: ${x.constructor.name}`);\n    }\n    // tslint:enable:no-any\n  });\n\n  const y = new Uint8Array(totalByteLength);\n  let offset = 0;\n  normalizedXs.forEach((x: TypedArray) => {\n    y.set(new Uint8Array(x.buffer), offset);\n    offset += x.byteLength;\n  });\n\n  return y.buffer;\n}\n\n// Use Buffer on Node.js instead of Blob/atob/btoa\nconst useNodeBuffer = typeof Buffer !== 'undefined' &&\n    (typeof Blob === 'undefined' || typeof atob === 'undefined' ||\n     typeof btoa === 'undefined');\n\n/**\n * Calculate the byte length of a JavaScript string.\n *\n * Note that a JavaScript string can contain wide characters, therefore the\n * length of the string is not necessarily equal to the byte length.\n *\n * @param str Input string.\n * @returns Byte length.\n */\nexport function stringByteLength(str: string): number {\n  if (useNodeBuffer) {\n    return Buffer.byteLength(str);\n  }\n  return new Blob([str]).size;\n}\n\n/**\n * Encode an ArrayBuffer as a base64 encoded string.\n *\n * @param buffer `ArrayBuffer` to be converted.\n * @returns A string that base64-encodes `buffer`.\n */\nexport function arrayBufferToBase64String(buffer: ArrayBuffer): string {\n  if (useNodeBuffer) {\n    return Buffer.from(buffer).toString('base64');\n  }\n  const buf = new Uint8Array(buffer);\n  let s = '';\n  for (let i = 0, l = buf.length; i < l; i++) {\n    s += String.fromCharCode(buf[i]);\n  }\n  return btoa(s);\n}\n\n/**\n * Decode a base64 string as an ArrayBuffer.\n *\n * @param str Base64 string.\n * @returns Decoded `ArrayBuffer`.\n */\nexport function base64StringToArrayBuffer(str: string): ArrayBuffer {\n  if (useNodeBuffer) {\n    const buf = Buffer.from(str, 'base64');\n    return buf.buffer.slice(buf.byteOffset, buf.byteOffset + buf.byteLength);\n  }\n  const s = atob(str);\n  const buffer = new Uint8Array(s.length);\n  for (let i = 0; i < s.length; ++i) {\n    buffer.set([s.charCodeAt(i)], i);\n  }\n  return buffer.buffer;\n}\n\n/**\n * Concatenate a number of ArrayBuffers into one.\n *\n * @param buffers A number of array buffers to concatenate.\n * @returns Result of concatenating `buffers` in order.\n */\nexport function concatenateArrayBuffers(buffers: ArrayBuffer[]): ArrayBuffer {\n  if (buffers.length === 1) {\n    return buffers[0];\n  }\n\n  let totalByteLength = 0;\n  buffers.forEach((buffer: ArrayBuffer) => {\n    totalByteLength += buffer.byteLength;\n  });\n\n  const temp = new Uint8Array(totalByteLength);\n  let offset = 0;\n  buffers.forEach((buffer: ArrayBuffer) => {\n    temp.set(new Uint8Array(buffer), offset);\n    offset += buffer.byteLength;\n  });\n  return temp.buffer;\n}\n\n/**\n * Get the basename of a path.\n *\n * Behaves in a way analogous to Linux's basename command.\n *\n * @param path\n */\nexport function basename(path: string): string {\n  const SEPARATOR = '/';\n  path = path.trim();\n  while (path.endsWith(SEPARATOR)) {\n    path = path.slice(0, path.length - 1);\n  }\n  const items = path.split(SEPARATOR);\n  return items[items.length - 1];\n}\n\n/**\n * Create `ModelJSON` from `ModelArtifacts`.\n *\n * @param artifacts Model artifacts, describing the model and its weights.\n * @param manifest Weight manifest, describing where the weights of the\n *     `ModelArtifacts` are stored, and some metadata about them.\n * @returns Object representing the `model.json` file describing the model\n *     artifacts and weights\n */\nexport function getModelJSONForModelArtifacts(\n    artifacts: ModelArtifacts, manifest: WeightsManifestConfig): ModelJSON {\n  const result: ModelJSON = {\n    modelTopology: artifacts.modelTopology,\n    format: artifacts.format,\n    generatedBy: artifacts.generatedBy,\n    convertedBy: artifacts.convertedBy,\n    weightsManifest: manifest\n  };\n  if (artifacts.signature != null) {\n    result.signature = artifacts.signature;\n  }\n  if (artifacts.userDefinedMetadata != null) {\n    result.userDefinedMetadata = artifacts.userDefinedMetadata;\n  }\n  if (artifacts.modelInitializer != null) {\n    result.modelInitializer = artifacts.modelInitializer;\n  }\n  if (artifacts.trainingConfig != null) {\n    result.trainingConfig = artifacts.trainingConfig;\n  }\n  return result;\n}\n\n/**\n * Create `ModelArtifacts` from a JSON file and weights.\n *\n * @param modelJSON Object containing the parsed JSON of `model.json`\n * @param weightSpecs The list of WeightsManifestEntry for the model. Must be\n *     passed if the modelJSON has a weightsManifest.\n * @param weightData An ArrayBuffer of weight data for the model corresponding\n *     to the weights in weightSpecs. Must be passed if the modelJSON has a\n *     weightsManifest.\n * @returns A Promise of the `ModelArtifacts`, as described by the JSON file.\n */\nexport function getModelArtifactsForJSONSync(\n    modelJSON: ModelJSON, weightSpecs?: WeightsManifestEntry[],\n    weightData?: ArrayBuffer): ModelArtifacts {\n\n  const modelArtifacts: ModelArtifacts = {\n    modelTopology: modelJSON.modelTopology,\n    format: modelJSON.format,\n    generatedBy: modelJSON.generatedBy,\n    convertedBy: modelJSON.convertedBy\n  };\n\n  if (modelJSON.trainingConfig != null) {\n    modelArtifacts.trainingConfig = modelJSON.trainingConfig;\n  }\n  if (modelJSON.weightsManifest != null) {\n    if (!weightSpecs) {\n      throw new Error('modelJSON has weightsManifest but weightSpecs is null');\n    }\n    if (!weightData) {\n      throw new Error('modelJSON has weightsManifest but weightData is null');\n    }\n    modelArtifacts.weightSpecs = weightSpecs;\n    modelArtifacts.weightData = weightData;\n  }\n  if (modelJSON.signature != null) {\n    modelArtifacts.signature = modelJSON.signature;\n  }\n  if (modelJSON.userDefinedMetadata != null) {\n    modelArtifacts.userDefinedMetadata = modelJSON.userDefinedMetadata;\n  }\n  if (modelJSON.modelInitializer != null) {\n    modelArtifacts.modelInitializer = modelJSON.modelInitializer;\n  }\n\n  return modelArtifacts;\n}\n\n/**\n * Create `ModelArtifacts` from a JSON file.\n *\n * @param modelJSON Object containing the parsed JSON of `model.json`\n * @param loadWeights Function that takes the JSON file's weights manifest,\n *     reads weights from the listed path(s), and returns a Promise of the\n *     weight manifest entries along with the weights data.\n * @returns A Promise of the `ModelArtifacts`, as described by the JSON file.\n */\nexport async function getModelArtifactsForJSON(\n    modelJSON: ModelJSON,\n    loadWeights: (weightsManifest: WeightsManifestConfig) => Promise<[\n      /* weightSpecs */ WeightsManifestEntry[], /* weightData */ ArrayBuffer\n    ]>): Promise<ModelArtifacts> {\n  let weightSpecs: WeightsManifestEntry[] | undefined;\n  let weightData: ArrayBuffer | undefined;\n\n  if (modelJSON.weightsManifest != null) {\n    [weightSpecs, weightData] = await loadWeights(modelJSON.weightsManifest);\n  }\n\n  return getModelArtifactsForJSONSync(modelJSON, weightSpecs, weightData);\n}\n\n/**\n * Populate ModelArtifactsInfo fields for a model with JSON topology.\n * @param modelArtifacts\n * @returns A ModelArtifactsInfo object.\n */\nexport function getModelArtifactsInfoForJSON(modelArtifacts: ModelArtifacts):\n    ModelArtifactsInfo {\n  